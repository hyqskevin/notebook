## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
自动驾驶技术的计算机视觉：问题，数据和前沿技术

### 8. Motion & Pose Estimation 运动与姿势估测
- 8.1. 2D Motion Estimation – Optical Flow  2D运动估测 - 光流
- Optical flow is defined as the two-dimensional motion of brightness patterns between two images. This definition only represents motion of intensities in the image plane but not the 3D motion of the objects in the scene. Recovering the 3D motion itself is the goal in Scene Flow discussed in Section 8.2.Figure 23 shows the synthetic Yosemite sequence with the optical flow ground truth generated by texture mapping aerial images of Yosemite valley on depth maps of the valley. Optical flow provides important information about the scene and serves as input for several tasks such as ego-motion estimation (Section 8.3), structure-from-motion and tracking (Section 9). The research on this problem started several decades ago with the variational formulation by Horn & Schunck (1981) assuming the brightness of a pixel to be constant over time. Optical flow is an inverse problem in which insufficient information is given to fully specify the solution. The brightness at a pixel provides only one constraint while the unknown motion vector has two components. This is known as the aperture problem and can only be solved by introducing an additional constraint which is usually a smoothness assumption encouraging similar motion vectors between neighboring pixel. Despite the long history of the optical flow problem, occlusions, large displacement and fine details are still challenging for modern methods. A fundamental problem with the optical flow definition is that besides the actual motion of interest, illumination changes, reflections and transparency can also cause intensity changes besides the motion.
- 光流被定义为两个图像之间的亮度图案的二维运动。该定义仅表示图像平面中的强度运动，而不表示场景中物体的3D运动。恢复3D运动本身是第8.2节中讨论的场景流程中的目标。图23显示了由优胜美地山谷的纹理映射生成的合成优胜美地序列，其深谷图的深度图。光流提供关于场景的重要信息，并且用作诸如自主运动估计（8.3节），运动结构和跟踪（9）等几项任务的输入。关于这个问题的研究在几十年前就开始，而Horn＆Schunck（1981）的变分公式假定像素的亮度随着时间的推移是恒定的。光流是一个反向问题，其中没有足够的信息来完全指定解决方案。像素处的亮度仅提供一个约束，而未知运动矢量具有两个分量。这被称为孔径问题，并且只能通过引入额外的约束来解决，附加约束通常是鼓励相邻像素之间的相似运动矢量的平滑假设。尽管光流问题的历史悠久，但现代方法仍然存在堵塞，大排量和细微细节。光流定义的一个根本问题是，除了感兴趣的实际运动之外，照明变化，反射和透明度也可能导致运动以外的强度变化。

- **Variational Formulation**: Traditionally, the optical flow problem has been approached with a variational formulation. Variational methods minimize an energy consisting of a data term, assuming little appearance change over time, and a smoothness term, encouraging similarity between spatial neighbors. Horn & Schunck (1981) introduced the brightness constancy assumption which models the intensity value of a pixel as constant over time. Considering one pixel this assumption yields one equation with two unknowns that cannot be solved as such (aperture problem). To estimate the optical flow an additional constraint is necessary. A common way of regularizing variational optical flow estimation is to encourage similarity of spatially neighboring flow vectors. This prior is motivated by the fact that flow fields are often smooth and discontinuities typically occur only at object boundaries. The original formulation by Horn & Schunck (1981) uses a quadratic penalty function in the data and smoothness term. This has the major limitation that violations of the brightness constancy assumption, like varying illumination conditions, can not be handled. One very popular way to alleviate this problem is using a robust penalty function as proposed by Black & Anandan (1993). In addition, several different data terms have been proposed that are less affected by illumination changes. Vogel et al. (2013) systematically evaluates pixel- and patch-based data costs in a unified testbed on the KITTI dataset (Geiger et al. (2012b)). On real data, they found patch-based terms to perform better than pixel-based terms. Another limitation of the original formulation by Horn & Schunck (1981) is that the homogeneous non-robust smoothness term does not allow flow discontinuities. However, in real world scenes different objects often cause optical flow discontinuities at their boundaries thus violating this assumption. Total Variation regularization used in Zach et al. (2007) replaces the quadratic penalization by the L1 norm to preserve discontinuities in the flow field. A remaining disadvantage of this model is that it favors fronto-parallel surfaces which is not a realistic assumption for real-world scenes. Thus, higher-order regularizations like the Total Generalized Variation (TGV) model have been proposed by Bredies et al. (2010). TGV priors can better represent real data as they leverage a piecewise affine motion model. The non-local Total Generalized Variation by Ranftl et al. (2014) is an extension of this model which enforces the piecewise affine assumption in a local neighborhood. They observed that considering only direct neighbors leads to a decrease of performance in regions where the data term is ambiguous. Zimmer et al. (2011) provide a detailed assessment of imageand flow-driven regularizers for the variational formulation and discuss the qualities of different data terms. Besides the model specifications, the choice of the optimization method and its implementation are additional factors which influence the performance of variational optical flow estimation algorithms.  detailed study of optical flow methods is provided by Sun et al. (2014). They uncover the reasons for the success of modern optical flow methods and propose an approach optimizing a classical formulation with modern techniques.
- 变异配方：：传统上，光流问题已经用变分公式进行了解。变数方法将由数据项组成的能量最小化，假定随着时间的推移几乎没有出现变化，并且平滑度项令人鼓励空间邻居之间的相似性。 Horn＆Schunck（1981）引入了亮度恒定性假设，其将像素的强度值随时间推定为恒定。考虑到一个像素，这个假设产生一个方程，其中有两个不能被解决的未知数（孔问题）。为了估计光流量，需要额外的约束。变分光学流量估计的常规方法是鼓励空间相邻流向量的相似性。这一事实是由于流场通常是平滑的，并且不连续通常仅在物体边界处发生。 Horn＆Schunck（1981）的原始方法在数据和平滑度项中使用二次惩罚函数。这样做的主要限制是不能处理亮度恒定假设的违规，如不同的照明条件。减轻这个问题的一个非常受欢迎的方法是使用Black＆Anandan（1993）提出的强大的惩罚函数。此外，已经提出了几个不同的数据项，其受照明变化的影响较小。 Vogel等（2013）系统评估KITTI数据集统一测试平台中的像素和补丁数据成本（Geiger等（2012b））。在实际数据中，他们发现基于片段的术语比基于像素的术语更好。 Horn＆Schunck（1981）的原始方法的另一个限制是，均匀非鲁棒平滑度项不允许流量不连续性。然而，在现实世界的场景中，不同的对象常常会在其边界引起光流不连续，从而违反了这一假设。 Zach等人使用的总变异正则化（2007）用L1范数取代二次惩罚，以保持流场中的不连续性。这种模式的一个缺点是，它有利于前平行表面，这对于现实世界的场景来说不是一个现实的假设。因此，Bredies等人已经提出了诸如总广义变化（TGV）模型的高阶正则化。 （2010年）。 TGV优先级可以更好地代表实际数据，因为它们利用分段仿射运动模型。 Ranftl等人的非局部总广义变异（2014）是这个模式的延伸，它强制在当地社区进行分段仿射假设。他们观察到，只考虑直接邻居导致数据项不明确的地区的业绩下降。 Zimmer等人（2011）提供了对变分公式的图像和流量驱动的正则化器的详细评估，并讨论了不同数据项的质量。除了模型规范，优化方法的选择及其实现还是影响变分光流估计算法性能的附加因素。 Sun等人提供了光学流动方法的详细研究。 （2014）。他们揭示了现代光流方法成功的原因，并提出了一种利用现代技术优化古典配方的方法。

- **Sparse Matches**: One major challenge, in particular for variational methods, is the estimation of large displacements since usually linear approximations are used that only hold in case of pixel motion. This problem is typically addressed with a coarse-to-fine strategy, estimating the flow on a coarser resolution to initialize the estimation on a finer resolution. While this strategy works for large structures of little complexity, fine geometric details are often lost in the process. Besides, textural details important for correspondence estimation are lost at coarse resolutions, hence leading the optimizer to a local minimum. One example for the loss of fine details is illustrated with a fast moving hand in Figure 24. These problems can be alleviated by integrating sparse features into the variational formulation as proposed by Brox & Malik (2011). The feature matches, obtained from nearest neighbor search on a coarse grid, are used as soft constraint in a coarse-to-fine optimization. While in Figure 24 the warping methods fail to recover the optical flow for the hand, the feature matches lead the optimization to the right solution. Another possibility to deal with large displacements is suggested by Revaud et al. (2015). They replace the coarse-tofine strategy with an interpolation of sparse matches to initialize a dense optimization at full resolution. Sparse matches are obtained using DeepMatching, a deep neural network matching approach introduced by Weinzaepfel et al. (2013). In contrast to DeepMatching, Menze et al. (2015a) use approximate nearest neighbor search to generate a set of proposals as candidates to be used in a discrete optimization framework. Inference is made feasible by restricting the number of matches to the most likely ones with non-maxima suppression and exploiting the truncated form of the pairwise potentials. Motivated by the success of Siamese networks in stereo (Zbontar & LeCun (2016)) (see Section 7.1), Guney & Geiger (2016) extend this work to learning features for 2D patch matching. They further investigate the importance of the receptive field size exploiting dilated convolutions as proposed by Yu & Koltun (2016) for semantic segmentation. Chen & Koltun (2016) argue that the heuristic pruning used to make inference feasible destroys the highly regular structure of the space of mappings and propose a discrete optimization over the full space. Min-convolutions are used to reduce the complexity and to effiectively optimize the large label space using a modified version of Tree-Reweighted Message Passing by Kolmogorov (2006). Wulff&Black (2015) present a different approach to obtain dense optical flow from sparse matches. In their approach, the optical flow field is represented as a weighted sum of basis flow fields learned from reference flow fields which have been estimated from Hollywood movies. They estimate the optical flow by finding the weights which minimize the error with respect to the detected sparse feature correspondences. While this results in overly smooth flow fields, the approach is very fast. Besides, a slower layered approach has been approached which better handles flow discontinuities.
- **稀疏匹配**：特别是变分方法的一个主要挑战是大位移的估计，因为通常使用仅在像素运动的情况下使用线性近似。这个问题通常用粗略到精细的策略来解决，估计更粗糙的分辨率上的流量来初始化更精细分辨率的估计。虽然这个策略适用于很小复杂性的大型结构，但是在这个过程中经常会丢失精细的几何细节。此外，对于通信估计重要的纹理细节在粗分辨率下丢失，因此导致优化器达到局部最小值。 Brox＆Malik（2011）提出的将稀疏特征整合到变分公式中可以缓解这些问题。在粗网格上从最近邻搜索获得的特征匹配，在粗到精优化中被用作软约束。而在图24中，翘曲方法无法恢复手的光流，特征匹配将优化导向正确的解决方案。 Revaud等人提出了处理大排量的另一种可能性。 （2015）。它们用粗略匹配的插值来代替粗略到精细策略，以在全分辨率下初始化密集优化。使用DeepMatching（Weinzaepfel等人介绍的深层神经网络匹配方法）获得稀疏匹配。 （2013年）。与DeepMatching相反，Menze等（2015a）使用近似最近邻搜索来生成一组提案作为在离散优化框架中使用的候选。通过将匹配次数限制为最有可能的匹配次数，通过非最大抑制和利用成对电位的截断形式，可以推断推理。由于暹罗网络在立体声（Zbontar＆LeCun（2016））中的成功（见第7.1节），Guney＆Geiger（2016）将此工作扩展到2D补丁匹配的学习功能。他们进一步研究了Yu＆Koltun（2016）提出的扩展卷积的接受场大小对语义分割的重要性。 Chen＆Koltun（2016）认为，用于推理可行性的启发式修剪破坏了映射空间的高度规则结构，并提出了在全部空间上的离散优化。 Min-convolutions用于降低复杂性，并使用Kolmogorov（2006）传递的Tree-Reweighted Message的修改版本来有效优化大标签空间。 Wulff＆Black（2015）提出了一种从稀疏匹配获得密集光流的不同方法。在他们的方法中，光流场被表示为从已经从好莱坞电影估计的参考流场获得的基本流场的加权和。它们通过找到相对于检测到的稀疏特征对应使误差最小化的权重来估计光流。虽然这导致流场过于流畅，但方法非常快。此外，已经采用较慢的分层方法，其更好地处理流动不连续性。

- **High Speed Flow**: With some exceptions (Wulff & Black(2015); Timofte&Gool (2015);Weinzaepfel et al. (2013); Farneback (2003); Zach et al. (2007)) most of the optical flow approaches are very inefficient and can not be applied in real-time which is necessary for applications in autonomous driving. The trade-off between accuracy and speed for different algorithms on KITTI 2012 benchmark Geiger et al. (2012b) is illustrated in Figure 25. The methods based on variational inference yield the best accuracy, however belong to the slowest set of methods for motion estimation. However, the duality based approach for total variation optical flow proposed by Zach et al. (2007) allows an efficient GPU implementation that performs in real-time (30 Hz) on a resolution of 320 x 240. Sparse matching approaches are usually more efficient than variational formulations but often need variational refinement as post processing step to achieve subpixel precision. The recent introduction of deep learning to the optical flow problem yielded several almost real-time approaches (Dosovitskiy et al. (2015); Ranjan & Black (2016)) including Ilg et al. (2016) which achieves state-of-the-art performance on popular datasets. These methods will be discussed below. The approach proposed by Kroeger et al. (2016) allows to trade-off accuracy and computational time. They realize fast patch correspondences with inverse search and obtain a dense flow field with the aggregation of patches along multiple scales. This allows them to estimate optical flow with up to 600 Hz at the cost of accuracy.
- 高速流：除了一些例外（Wulff＆Black（2015）; Timofte＆Gool（2015）; Weinzaepfel等（2013）; Farneback（2003）; Zach et al。（2007））大部分光流方法非常低效，不能实时应用，这对于自主驾驶中的应用是必需的。在KITTI 2012基准测试Geiger等人的不同算法的精度和速度之间的权衡（2012b）如图25所示。基于变分推理的方法产生最佳精度，但属于运动估计方法的最慢组。然而，Zach等人提出的基于双变量的全变量光流的方法（2007）允许在320 x 240的分辨率下实时（30 Hz）执行高效的GPU实现。稀疏匹配方法通常比变分公式更有效，但通常需要变分细化作为后处理步骤来实现子像素精度。最近对光流问题的深入学习引入了几种几乎实时的方法（Dosovitskiy等（2015）; Ranjan＆Black（2016）），其中包括Ilg等。 （2016），其在流行数据集上实现了最先进的性能。这些方法将在下面讨论。 Kroeger等人提出的方法（2016）允许权衡精度和计算时间。它们通过反向搜索实现快速补片对应，并通过多个尺度的斑块聚合获得稠密的流场。这允许他们以高达600Hz的精度估计光流。

- **State-of-the-art**: Currently, Sintel Butler et al. (2012) and KITTI Geiger et al. (2012b, 2013) discussed in Section 2 are the most popular datasets for the evaluation of optical flow algorithms. However, in this survey we focus on the autonomous driving application. Therefore, we will only refer to the KITTI leaderboard when we compare methods. Still, optical flow approaches not specifically designed for autonomous driving have a similar ranking on Sintel. In Table 7 we show the leaderboard for the KITTI 2015 benchmark. The performance of methods is assessed using the percentage of outliers, which are flow vectors with the absolute endpoint error (EPE) exceeding 3 pixel and 5% of its true values. The percentage of outliers is averaged over background (Fl-bg), foreground (Fl-fg) and all regions (Fl-all). In addition, the density of the output flow field and the runtime are provided. The best performing methods either learn optical flow end-to-end (Ilg et al. (2016)) or use semantic segmentation to split the scene into independently moving objects Bai et al. (2016); Sevilla-Lara et al. (2016). The best performing approach FlowNet2 (Ilg et al. (2016)) trains a deep neural network to solve the optical flow problem.
- 最先进的技术：目前，Sintel Butler等人（2012）和KITTI Geiger等人（2012b，2013）在第2节中讨论的是用于评估光流算法的最流行的数据集。然而，在本次调查中，我们专注于自主驾驶应用。因此，当我们比较方法时，我们只会参考KITTI排行榜。尽管如此，未专门为自主驾驶而设计的光流方法在Sintel上具有相似的排名。在表7中，我们显示了KITTI 2015基准测试的排行榜。使用异常值的百分比来评估方法的性能，其中绝对端点误差（EPE）超过3个像素的流向量和其真实值的5％。异常值的百分比在背景（Fl-bg），前景（Fl-fg）和所有区域（Fl-all）上平均。此外，还提供了输出流场的密度和运行时间。最佳表现的方法是学习光流端到端（Ilg et al。（2016）），或者使用语义分割来将场景分解成独立移动的对象Bai et al。 （2016）; Sevilla-Lara等人（2016）。 FlowNet2（Ilg et al。（2016））训练一个深层神经网络来解决光流问题。

- **Epipolar Flow**: In the context of autonomous driving, simplifying assumptions can be used to alleviate the optical flow problem. The assumption of a static scene or the decomposition of a scene into rigidly moving objects allow to treat optical flow as matching problem along epipolar lines radiated from the focus of expansion. Yamaguchi et al. (2013) propose a slanted-plane Markov random field that represents the epipolar flow of each segment with slanted planes. This formulation needs a time consuming optimization and can be avoided with the joint stereo and flow formulation of Yamaguchi et al. (2014). They assume the scene to be static and present a new semi global block matching algorithm using the joint evidence of stereo and video. This formulation allows them to rank third in KITTI 2012 while being 10 times faster than the best performing method. In contrast to these approaches, Bai et al.(2016) use the slanted plane model only for background flow estimation. An instance-segmentation allows them to formulate an independent epipolar flow estimation problem for each moving object. While for KITTI 2012 the advantage of this formulation is not evident because of the static scene, on KITTI 2015 which comprises dynamic scenes they achieve better results (Table 7).
- 对极流：在自主驾驶的背景下，可以使用简化假设来减轻光流问题。将静态场景或将场景分解为刚性移动物体的假设允许将光流作为从扩展焦点辐射的沿线的匹配问题来处理。山口等（2013）提出了一个斜面马尔可夫随机场，其表示具有倾斜平面的每个段的对极流。该配方需要耗时的优化，并且可以通过Yamaguchi等人的联合立体声和流量配方来避免。 （2014）。他们假设场景是静态的，并使用立体声和视频的联合证据呈现新的半全局块匹配算法。这种配方使他们在KITTI 2012中排名第三，而且比最佳性能方法快10倍。与这些方法相反，Bai等（2016）将倾斜平面模型仅用于背景流估计。实例分割允许它们为每个移动对象制定独立的对极流估计问题。而对于KITTI 2012来说，由于静态场景，KITTI 2015的优势并不明显，KITTI 2015包含动态场景，从而获得更好的效果（表7）。

- **Semantic Segmentation**: Scenes in the context of autonomous driving are usually composed of a static background and dynamic moving traffic participants. This observation can be exploited by splitting the scene into independently moving objects. As mentioned above, Bai et al. (2016) extract traffic participants using instance-level segmentation and estimate the optical flow independently for different instances. Sevilla-Lara et al. (2016) use semantic segmentation for optical flow estimation in several ways: on one hand, semantics provide information on object boundaries as well as spatial relationships between objects that are used to reason about depth ordering. On the other hand, the division of the scene allows Sevilla-Lara et al. (2016) to exploit different motion models according to the respective object type, similar to Bai et al. (2016). The motion of planar regions is modeled with homographies, whereas independently moving objects are modeled by affine motions allowing for deviations. Complex objects like vegetation are modeled with a classical spatially varying dense flow field. Finally, the constancy of object identities over time is used to encourage temporal consistency of the optical flow.
- 语义分割：在自主驾驶的上下文中的场景通常由静态背景和动态移动交通参与者组成。这种观察可以通过将场景分解为独立移动的对象来利用。如上所述，Bai et al。 （2016）使用实例级分割提取流量参与者，并针对不同的实例独立地估计光流。 Sevilla-Lara等人（2016）以多种方式使用语义分割进行光流估计：一方面，语义提供关于对象边界的信息以及用于深入排序的对象之间的空间关系。另一方面，场景的划分允许Sevilla-Lara等人（2016）根据各自的对象类型开发不同的运动模型，类似于Bai等。 （2016）。平面区域的运动是用同形图建模的，而独立运动的物体是通过允许偏差的仿射运动进行建模的。复杂的物体，如植被，建立在经典的空间变化密集流场。最后，随着时间的推移，对象身份的一致性被用来促进光流的时间一致性。

- **Confidences**: Considering the remaining challenges in optical flow, a confidence measure to assess the quality of the estimated flow is desirable. Several measures based on spatial and temporal gradients have been proposed (Uras et al. (1988); Anandan (1989); Simoncelli et al. (1991)) that quantify the difficulty to estimate flow for a specific image. In contrast, algorithm-specific measures (Bruhn & Weickert (2006); Kybic & Nieuwenhuis (2011)) have been proposed which give a confidence for an estimation only for a specific group of methods. Learning-based measures like Kondermann et al. (2007, 2008) learn a model that relates flow algorithm success to spatio-temporal image data or the computed flow field. A detailed evaluation of different confidence measures is given by Mac Aodha et al. (2013). In addition, they present another learning based approach which uses multiple feature types such as temporal, texture, distance from images edges, and others, to estimate confidences for the success of a given method.
- 置信度：考虑到光流中存在的其他挑战，需要一种评估估计流量质量的置信度量。已经提出了基于空间和时间梯度的几种措施（Uras等人（1988）; Anandan（1989）; Simoncelli等人（1991）），其量化了针对特定图像估计流动的难度。相比之下，已经提出了算法特异性测量（Bruhn＆Weickert（2006）; Kybic＆Nieuwenhuis（2011）），其对于仅针对特定组的方法进行估计而置信度。基于学习的措施，如Kondermann et al。 （2007,2008）学习了一种将流程算法成功与时空图像数据或计算流场相关联的模型。 Mac Aodha等人给出了不同信心度量的详细评估。 （2013年）。此外，他们提出另一种基于学习的方法，其使用多个特征类型，例如时间，纹理，与图像边缘的距离等，以估计给定方法的成功的置信度。

- **Deep Learning**: Most optical flow approaches do not incorporate any high-level information like semantics which makes it hard to resolve ambiguities. The knowledge about objects and their material property can be used to model reflectance and transparency which would allow to be unaffected by these phenomena. The recent success of convolutional neural networks to learn high-level information have led to the attempt of using them for the optical flow problem. Dosovitskiy et al. (2015) presented FlowNet to learn optical flow end-to-end using a CNN. FlowNet consists of a contracting part which extracts important features and an expanding part which produces the high resolution flow. They propose two different architectures: a simple network stacking the images and a complex network correlating features of the separately processed images. One problem in learning optical flow is the limited amount of training data. KITTI 2012 Geiger et al. (2012b) and KITTI 2015 Menze & Geiger (2015) only provide around 200 training examples each while Sintel Butler et al. (2012) has 1041 training image pairs. Since these datasets are too small to train large CNNs, Dosovitskiy et al. (2015) created the Flying Chairs dataset by rendering 3D chair models on top of images from Flickr. This first attempt to end-to-end optical flow learning demonstrated that it was possible to learn optical flow but could not reach state-of-the art performance on KITTI (Table 7) or Sintel. However, compared to methods performing at almost real-time they were the best performing. In contrast to the contracting and expanding networks of Dosovitskiy et al. (2015), Ranjan & Black (2016) present SpyNet, an architectur  inspired by the coarse-to-fine matching strategy leveraged in traditional optical flow estimation techniques. Each layer of the network represents a different scale and only estimates the residual flow with respect to the warped image. This formulation allowed them to achieve similar performance as FlowNet while being faster. Being 96 % smaller than FlowNet one major contribution was the memory efficiency which makes it attractive for embedded systems. Ilg et al. (2016) present FlowNet2, an improved version of FlowNet, by stacking the architectures and fusing the stacked network with a subnetwork specialized on small motions. Similar to SpyNet, they also input the warped image into the stacked networks. However, each stacked network estimates the flow between the original frames instead of the residual flow as in SpyNet. In contrast to FlowNet and SpyNet, they use the FlyingThings3D dataset (Mayer et al.(2016)) consisting of 22k renderings of static 3D scenes with moving 3D models from ShapeNet dataset (Savva et al. (2015)). FlowNet2 performs on par with state-of-the-art methods on Sintel and outperforms all others on KITTI 2015 (Table 7) while being one of the fastest. They provide different network variants for the spectrum between 8fps and 140fps allowing the trade-off between accuracy and computational resources.
- 深度学习：大多数光流方法不包含任何高级信息，如语义学，这使得难以解决歧义。关于物体及其材料性质的知识可用于建模反射率和透明度，这将不会受到这些现象的影响。卷积神经网络近来在学习高级信息方面的成功导致了将其用于光流问题的尝试。 Dosovitskiy等人（2015）介绍了FlowNet，使用CNN学习光端机端到端。 FlowNet由一个收缩部分组成，它提取重要特征和扩展部分产生高分辨率流。他们提出了两种不同的架构：一个简单的网络堆叠图像和一个复杂的网络相关的单独处理的图像的功能。学习光流的一个问题是训练数据量有限。 KITTI 2012 Geiger et al。 （2012b）和KITTI 2015 Menze＆Geiger（2015）仅提供约200个培训示例，而Sintel Butler等（2012）拥有1041个训练图像对。由于这些数据集太小，无法训练大型CNN，Dosovitskiy et al。 （2015）通过在Flickr的图像之上渲染3D椅子模型创建了飞行椅数据集。这种端到端光流学习的第一次尝试表明，有可能学习光流，但无法达到KITTI（表7）或Sintel的最先进的性能。然而，与几乎实时执行的方法相比，它们表现最好。与Dosovitskiy等人的承包和扩展网络相反。 （2015），Ranjan＆Black（2016）目前是SpyNet，这是一种灵感来自于传统光流估计技术中的粗匹配匹配策略的架构。网络的每个层表示不同的比例，并且仅估计相对于翘曲图像的残余流量。这种配方使得它们在获得与FlowNet相似的性能的同时更快。比FlowNet小96％的主要贡献是内存效率，使嵌入式系统具有吸引力。 Ilg等人（2016）提供FlowNet2，FlowNet的改进版本，通过堆叠架构并将堆叠网络与专门用于小型运动的子网进行融合。与SpyNet类似，他们还将翘曲的图像输入堆叠网络。然而，每个堆叠的网络估计在原始帧之间的流量，而不是如SpyNet中的剩余流量。与FlowNet和SpyNet相反，他们使用由ShapeNet数据集（Savva et al。（2015））的移动3D模型构成的22k渲染静态3D场景的FlyingThings3D数据集（Mayer等（2016））。 FlowNet2与Sintel的最先进的方法保持一致，并且在KITTI 2015（表7）上胜过其他所有方法，同时是最快的。它们为8fps和140fps之间的频谱提供不同的网络变体，允许在精度和计算资源之间进行权衡。

- **Discussion**: Robust optical flow methods need to handle intensity changes not caused by the actual motion of interest but by illumination changes, reflections and transparency. In real world scenes, repetitive patterns and occlusions are frequent sources of errors. While illumination changes have been addressed with novel data terms (Black & Anandan (1993); Vogel et al. (2013)) the problems caused by reflection, transparency, ambiguities and occlusions remain largely unsolved. In Figure 26 we show the accumulated error of the 15 best performing methods on KITTI 2015 (Menze & Geiger (2015)). The highest error can be observed for regions moving outside the image domain. Untextured, reflective and transparent regions also result in large errors in many cases. A better understanding of the world is necessary to tackle these problems. Semantics (Bai et al. (2016); Sevilla-Lara et al. (2016)) and learned highcapacity models (Dosovitskiy et al. (2015); Ranjan & Black (2016); Ilg et al. (2016)) have already proven to improve optical flow estimation by resolving ambiguities in the data. In addition, scene flow methods which jointly reason about flow and depth have demonstrated encouraging performance.
- 讨论：坚固的光流方法需要处理不是由实际运动造成的强度变化，而是照明变化，反射和透明度。在现实世界的场景中，重复的模式和遮挡是常见的错误来源。虽然用新的数据术语（Black＆Anandan（1993）; Vogel等（2013））解决了照明变化，但反射，透明度，模糊和遮挡所引起的问题仍然很大程度上尚未得到解决。在图26中，我们显示了KITTI 2015（Menze＆Geiger（2015））上15种表现最好的方法的累积误差。对于在图像域外移动的区域，可以观察到最高的误差。在很多情况下，非纹理，反光和透明的区域也会导致大的错误。更好地了解世界是解决这些问题的必要条件。语义学（Bai et al。（2016）; Sevilla-Lara et al。（2016）），并学习了大容量模型（Dosovitskiy等（2015）; Ranjan＆Black（2016）; Ilg等（2016））已经证明通过解决数据中的模糊度来改善光流估计。另外，共同推动流量和深度的场景流动方法表现出令人鼓舞的表现。

- 8.2. 3D Motion Estimation – Scene Flow  3D 运动估测-场景流
- Stereo matching does not reveal any motion information, and optical flow from a single camera is not well constrained and lacks the depth information lost by the projection. On the other hand, humans are able to effortlessly integrate depth and motion cues from observations over time. That kind of reasoning is essential for many tasks in autonomous driving such as segmentation of moving objects in the 3D world. Scene flow generalizes optical flow to 3D, or alternatively, dense stereo to dynamic scenes. Given stereo image sequences, the goal is to estimate the three dimensional motion field that is a 3D motion vector for every point on every visible surface in the scene. The minimal setup for image-based scene flow estimation is given by two consecutive stereo image pairs as visualized in Figure 27. Establishing correspondences between the four images results in the 3D location of the surface point in both frames and hence fully describes the 3D motion of that surface point. A dense output is preferred, although there are some early sparse approaches for real-time purposes (Franke et al. (2005)). Scene flow shares some challenges with stereo and optical flow such as matching ambiguities in weakly textured regions and the aperture problem.
- 立体声匹配不会显示任何运动信息，并且来自单个摄像机的光流量没有受到很好的约束，并且缺少投影损失的深度信息。另一方面，人类能够随着时间的推移将观察结果的深度和运动线索轻松整合。这种推理对于自主驾驶中的许多任务至关重要，例如在3D世界中移动物体的分割。场景流将光流广泛化为3D，或将密集立体声广泛化为动态场景。给定立体图像序列，目标是估计作为场景中每个可见表面上每个点的三维运动矢量的三维运动场。基于图像的场景流估计的最小设置由图27中可视化的两个连续的立体图像对给出。建立四个图像之间的对应度导致两个帧中的表面点的3D位置，并且因此完全描述了三维运动那个表面点。尽管有一些早期的稀疏方法用于实时目的（Franke等人（2005）），密集输出是首选的。场景流动与立体声和光学流动有一些挑战，例如弱纹理区域中的匹配模糊度和孔径问题。

- **Variational Approaches**: Following the seminal work by Vedula et al. (1999), the problem is traditionally formulated in a variational setting where optimization proceeds in a coarse-tofine manner and local regularizers are leveraged to encourage smoothness in depth and motion. Wedel et al. (2008, 2011) propose a variational framework by decoupling the motion estimation from the disparity estimation while maintaining the stereo constraints. Starting from a precomputed disparity map at each time step, optical flow for the reference frame and disparity for the other view are estimated. The motivation for decoupling is mainly computational efficiency by choosing the optimal technique for each task. In addition, Wedel et al. (2011) propose a solution for varying lighting conditions based on residual images and provide an uncertainty measure which is shown to be useful for object segmentation. Rabe et al. (2010) integrate a Kalman filter to the decoupling approach for temporal smoothness and robustness.
- 变异方法：遵循Vedula等人的开创性工作。 （1999）中，问题传统上是在变化设置中进行的，其中优化以粗略到精细的方式进行，并且利用局部正则化来促进深度和运动的平滑度。 Wedel等（2008年，2011年）提出了一种变分框架，通过将运动估计与视差估计相结合，同时保持立体约束。从每个时间步长的预先计算的视差图开始，估计参考帧的光流和其他视图的视差。通过选择每个任务的最优技术，解耦的动机主要是计算效率。此外，Wedel等（2011）提出了一种基于残差图像改变照明条件的解决方案，并提供一种显示对对象分割有用的不确定性度量。 Rabe等人（2010）将卡尔曼滤波器整合到解耦方法中，以实现时间平滑和鲁棒性。

- **Piecewise Rigidity**: Similar to stereo and optical flow, prior assumptions about the geometry and motion can be exploited to better handle the challenges of the scene flow problem. Vogel et al. (2015) and Lv et al. (2016) represent the dynamic scene as a collection of rigidly moving planar regions as shown in Figure 28. Vogel et al. (2015) jointly recover this segmentation while inferring the shape and motion parameters of each region. They use a discrete optimization framework and incorporate occlusion reasoning as well as other scene priors in the form of spatial regularization of geometry, motion and segmentation. In addition, they reason over multiple frames by constraining the segmentation to remain stable over a temporal window. Their experiments show that their view-consistent multi-frame approach significantly improves accuracy in challenging scenarios, and achieves state-of-the-art performance on the KITTI benchmark (see Table 8). Using the same representation, Lv et al. (2016) focus on an efficient solution to the problem. They assume a fixed superpixel segmentation and perform optimization in the continuous domain for faster inference. Starting from an initialization based on Deep Matching, they independently refine the geometry and motion of the scene, and finally perform a global non-linear refinement using the Levenberg-Marquardt algorithm.
- 分段刚度：与立体声和光流相似，可以利用关于几何和运动的先前假设来更好地处理场景流问题的挑战。 Vogel等（2015）和Lv et al。 （2016）表示动态场景作为刚性移动平面区域的集合，如图28所示。Vogel等人（2015）共同回顾了这一分段，同时推断了每个地区的形状和运动参数。他们使用离散优化框架，并以几何，运动和分割的空间正则化的形式结合闭塞推理以及其他场景先验。此外，它们通过限制分割在多个帧上导致在时间窗口上保持稳定。他们的实验表明，他们的视图一致的多框架方法显着提高了挑战性场景的准确性，并在KITTI基准测试中获得了最先进的性能（见表8）。使用相同的表示，Lv et al。 （2016）专注于有效解决问题。他们假设固定的超像素分割，并在连续域中进行优化，以便更快的推理。从基于深度匹配的初始化开始，它们独立地改进场景的几何和运动，并且最终使用全局非线性细化Levenberg-Marquardt算法。

- **Piecewise Rigidity at the Object Level**: Menze & Geiger (2015) also follow a slanted plane approach, but in addition to Vogel et al. (2015); Lv et al. (2016), they model the decomposition of the scene into a small number of independently moving objects and the background. By conditioning on a superpixelization, they jointly estimate this decomposition as well as the rigid motion of the objects and the plane parameters of each superpixel in a discrete-continuous CRF. Compared to Vogel et al. (2015); Lv et al. (2016), they leverage a more compact representation, implicitly regularizing over larger distances. They also present a new scene flow dataset by annotating dynamic scenes from the KITTI raw data collection using detailed 3D CAD models. They further present an extension of this model in Menze et al. (2015b) where the pose and 3D shape of the objects are inferred in addition to the rigid motion and the segmentation. In particular, they incorporate a deformable 3D active shape model of vehicles into the scene flow approach.
- 对象级别的分段刚度：Menze＆Geiger（2015）也遵循斜面方法，但除了Vogel等人（2015）; Lv et al。 （2016），他们将场景的分解模型化为少量的独立移动物体和背景。通过调整超像素化，它们共同估计了离散连续CRF中的这种分解以及物体的刚性运动和每个超级像素的平面参数。与Vogel等人相比（2015）; Lv et al。 （2016），他们利用更加紧凑的表现，在更大的距离上隐含地规范化。他们还通过使用详细的3D CAD模型从KITTI原始数据收集中注释动态场景，呈现新的场景流数据集。他们进一步提出了这一模式在Menze等人的扩展。 （2015b）除了刚性运动和分割之外，推断物体的姿态和3D形状。特别地，它们将车辆的可变形3D活动形状模型结合到场景流动方法中。
-

- **State-of-the-art**: In Table 8, we show the ranking of methods on the KITTI scene flow 2015 benchmark (Menze & Geiger (2015)). The methods are compared according to the percentage of erroneous pixels. In particular, the columns show the percentage of stereo disparity outliers in first frame (D1), the percentage of stereo disparity outliers in second frame (D2), the percentage of optical flow outliers (Fl), and the percentage of scene flow outliers (SF), i.e. outliers in either D0, D1 or Fl. The outlier ratio separately for foreground/background regions can be found on the website of the benchmark30, it is omitted here for space reasons. The top performing methods (Vogel et al. (2015); Menze & Geiger (2015); Lv et al. (2016)) use the assumption of rigidly moving segments. In addition, Menze & Geiger (2015) model the motion of independently moving objects and perform better in FI and SF on foreground regions, but it takes longer than the other two. Lv et al. (2016) achieve good results faster by focusing on efficient optimization in continuous domain. Derome et al. (2016) propose a two stage approach on GPU which runs several order of magnitude faster than the other methods. First they compute the static flow using stereo and visual odometry and correct the dynamic flow with a realtime optical approach Plyer et al. (2014).
- 最先进的：在表8中，我们显示了KITTI现场流程2015基准测试方法的排名（Menze＆Geiger（2015））。根据错误像素的百分比对这些方法进行比较。特别地，列显示了第一帧（D1）中的立体声视差异常值的百分比，第二帧（D2）中的立体声视差异常值的百分比，光流异常值的百分比（F1）和场景流异常值的百分比SF），即D0，D1或F1中的异常值。前景/背景区域的异常值可以在基准测试30的网站上找到，因为空间原因在此省略。表现最好的方法（Vogel等人（2015）; Menze＆Geiger（2015）; Lv等人（2016））使用了刚性移动段的假设。此外，Menze＆Geiger（2015）模拟了独立移动物体的运动，并在前景区域的FI和SF中表现更好，但是需要比其他两个时间更长的时间。 Lv et al。 （2016）通过关注连续域的有效优化，更快地获得了良好的效果。 Derome等（2016）提出了GPU的两阶段方法，其运行速度比其他方法快几个数量级。首先，它们使用立体声和视觉测距法计算静态流量，并用实时光学方法校正动态流量Plyer等。 （2014）。

- **Discussion**: Scene flow estimation shares most of the challenges with stereo and optical flow, while integrating more information leading to better results. Ideally, methods should exploit depth and motion cues together to reason about dynamic 3D scenes. We show the accumulated errors of top 5 methods on KITTI scene flow benchmark in Figure 29. Car surfaces are the most problematic regions due to matching problems and independent motion of cars. Pixels close to the image boundary are another typical source of error, especially on the road surfaces in front of the car where large scale changes occur. Although local planarity and rigidity assumptions alleviate the problem, they are often violated due to complex geometric objects like vegetation, pedestrians or bicycles. Wrong estimation of planes, for example superpixels extending to multiple surfaces cause additional problems, especially at the boundaries of objects. Semantic image understanding could help with these issues, especially at the object level by segmenting car instances. Another way to integrate more information is to consider long-term temporal interactions.
- 讨论：场景流量估计与立体声和光流量共享大部分挑战，同时集成更多信息，从而获得更好的效果。理想情况下，方法应该将深度和运动线索一起用于动态3D场景。我们显示图29中KITTI场景流程基准的前5种方法的累积误差。由于匹配问题和汽车的独立运动，汽车表面是最有问题的区域。靠近图像边界的像素是另一个典型的误差源，特别是在发生大规模变化的汽车前方的路面上。虽然局部平面度和刚性假设减轻了这个问题，但由于复杂的几何物体，如植被，行人或自行车，它们经常被侵犯。对飞机的错误估计，例如延伸到多个表面的超像素会引起额外的问题，特别是在对象的边界。语义图像理解可以帮助这些问题，特别是通过分割汽车实例在对象层面。整合更多信息的另一种方法是考虑长期的时间相互作用。

- 8.3. Ego-Motion Estimation  Ego-Motion 估计
- The estimation of the ego-motion, the position and orientation of the car, is another fundamental problem to realize autonomous driving. Traditionally, this problem was addressed in wheel odometry with wheel encoders, which measure the rotation of the wheel, by integrating the measurements over time. These methods suffer from wheel slip in uneven terrain or adverse conditions and can not recover from errors in the measurements. Visual odometry or LiDAR-based odometry techniques, which estimate ego-motion from images or laser range measurements, became popular because they are less affected by these conditions and can correct estimation errors by recognizing already visited places which is called loop closure (Section8.4.1). A detailed tutorial to this topic was presented by Scaramuzza & Fraundorfer (2011) and Fraundorfer & Scaramuzza (2011).
- 自主运动的估计，汽车的位置和方向是实现自主驾驶的另一个根本问题。 传统上，这个问题在车轮测距中得到了解决，车轮编码器通过随着时间的推移积分测量轮来旋转车轮。 这些方法在不平坦的地形或不利条件下遭受车轮滑移，并且不能从测量中的误差中恢复。 估计图像或激光范围测量中的自身运动的视觉测距或基于LiDAR的测距技术变得流行，因为它们受这些条件的影响较小，并且可以通过识别已被访问的位置（称为环闭合）来校正估计误差（Section 8.4.4）。 Scaramuzza＆Fraundorfer（2011）和Fraundorfer＆Scaramuzza（2011）提供了有关此主题的详细教程。

- **Formulation**: In visual odometry the goal is to recover the full trajectory of one camera or a camera system from images. This is incrementally done by estimating the relative transformation between the camera positions at two time steps and accumulating all transformations over time to recover the full trajectory. The incremental approach is illustrated in Figure 30. The different methods can be divided into two categories: feature-based methods, that extract an intermediate representation (features) from raw measurements, and direct formulations, that directly operates on raw measurements. Feature-based methods typically work only in environments conforming the used feature type. Especially in man-made environments, important information about straight and curved edges is discarded considering keypoints. In contrast, direct methods leverage the gradient information of the whole image. Therefore, these methods usually achieve higher accuracy and robustness in environments with little keypoints. The field was dominated by featurebased methods since they typically were more efficient but direct formulations have recently grown in popularity. In featurebased and direct formulations, the extracted representation or raw measurements are usually used as input in a probabilistic model to compute unknown hidden model parameters such as the camera motion or a world model. A Maximum Likelihood approach typically finds the model parameters that maximiz the probability of obtaining the measurements.
- 配方：在视觉测距中，目标是从图像中恢复一个摄像机或摄像机系统的完整轨迹。通过估计两个时间步长的摄像机位置之间的相对变换并随着时间的推移累积所有变换以恢复完整轨迹，这是递增的。增量方法如图30所示。不同的方法可以分为两类：基于特征的方法，从原始测量中提取中间表示（特征），以及直接对原始测量进行操作的直接配方。基于特征的方法通常仅在符合所使用的特征类型的环境中有效。特别是在人造环境中，考虑到关键点，丢弃关于直线和弯曲边缘的重要信息。相比之下，直接方法利用整个图像的渐变信息。因此，这些方法通常在关键点很小的环境中实现更高的精度和鲁棒性。该领域由基于特征的方法主导，因为它们通常更有效率，但直接配方近来越来越受欢迎。在基于特征和直接配方中，提取的表示或原始测量通常用作概率模型中的输入，以计算未知的隐藏模型参数，例如相机运动或世界模型。最大似然法通常找到最大化获得测量概率的模型参数。

- **Drift**: The incremental approach greatly suffers from drift caused by the accumulation of estimation errors of the individual transformations. It is usually addressed with an iterative refinement over the last x images. This is done by reprojecting image points into 3D by triangulation and minimizing the sum of squared reprojection errors (sliding window bundle adjustment or windowed bundle adjustment). Another method to reduce drift is simultaneous localization and mapping (SLAM) (Lee et al. (2013a); Engel et al. (2015); Pire et al. (2015); MurArtal et al. (2015)) which jointly estimates the location and a map of the environment to recognize places that have been visited before. The detection of already mapped places is known as loop closure and is used to reduce the drift in the trajectory as well as the map and achieve global consistency. Some work focus on the loop closure detection in specific (Cummins & Newman (2008); Paul & Newman (2010); Lee et al. (2013b)) which will be discussed in detail in Section 8.4.1. These approaches are computationally expensive and a careful selection of the extracted features can already reduce the estimation error and drift. Kitt et al. (2010) for example use bucketing to obtain well distributed corner-like feature matches whereas Deigmoeller & Eggert (2016) use different heuristics on flow and depth estimation to reject non stable features.
- 漂移：增量方法极大地受到由各个变换的估计误差积累引起的漂移的影响。通常通过对最后x个图像进行迭代细化来处理它。这可以通过三角测量将图像点重新投影到3D中，并将平方重新投影误差（滑动窗口束调整或窗口束调整）的总和最小化。减少漂移的另一种方法是同时进行本地化和映射（SLAM）（Lee等人（2013a）; Engel等人（2015）; Pire等人（2015）; MurArtal等（2015）），其共同估计位置和环境地图，以识别以前访问过的地方。已经映射的位置的检测被称为循环闭环，用于减少轨迹中的漂移以及地图，并实现全局一致性。 （Cummins＆Newman（2008）; Paul＆Newman（2010）; Lee et al。（2013b））中的一些工作侧重于循环闭合检测，将在8.4.1节中详细讨论。这些方法在计算上是昂贵的，并且提取的特征的仔细选择已经可以减少估计误差和漂移。 Kitt等人（2010）例如使用压缩来获得分布好的角状特征匹配，而Deigmoeller＆Eggert（2016）对流动和深度估计使用不同的启发式来拒绝非稳定特征。

- **2D-to-2D Matching**: Depending on how corresponding points between two time steps are represented (2D or 3D), different methods must be used to obtain the camera transformation. In case of 2D feature matches (2D-to-2D) the essential matrix can be estimated which represents the epipolar geometry between the two cameras. The translation and rotation can directly be extracted from the essential matrix. The eight-point algorithm (Longuet-Higgins (1981)) is a simple solution working with calibrated and uncalibrated cameras whereas the five-point algorithm (Nister (2004)) is a minimal case solution which only applies to the scenario of calibrated cameras. Scaramuzza et al. (2009) estimate the essential matrix from monocular images with only one 2D feature correspondence using non-holonomic constraints of wheeled vehicles imposing a restrictive motion model. Lee et al. (2013a) extend this idea to a novel two point minimal solution that is able to obtain the metric scale using a multi-camera system. In contrast to the non-holonomic constraints, Lee et al. (2014) assume the vertical directions to be known (from an Inertial Measurement Unit) and propose a minimal four-point and linear eight-point algorithm for a multicamera system. Kitt et al. (2010) estimate the ego-motion using trifocal tensor which relates features between three images. Using these algorithms within RANSAC all 6 degrees of freedom can be robustly obtained in these special scenarios. The number of iterations necessary to guarantee that a correct solution is found with RANSAC depends on the number of points from which the model can be instantiated. Therefore, a reduced number of correspondences will reduce the number of iterations and the runtime of the approach.
- 2D到2D匹配：根据两个时间步长之间的对应点（2D或3D）的不同，必须使用不同的方法来获得相机转换。在2D特征匹配（2D到2D）的情况下，可以估计必需矩阵，其表示两个相机之间的对极几何。可以直接从基本矩阵中提取平移和旋转。八点算法（Longuet-Higgins（1981））是一种使用校准和未校准的摄像机的简单解决方案，而五点算法（Nister（2004））是仅适用于校准摄像机场景的最小情况解决方案。 Scaramuzza等人（2009）从使用非限制性运动模型的轮式车辆的非完整约束来估计单眼图像中仅有一个2D特征对应关键矩阵。 Lee et al。 （2013a）将这一想法扩展到一种新颖的两点最小解决方案，该解决方案能够使用多摄像机系统获得度量标度。与非完整约束相反，Lee et al。 （2014）假设垂直方向是已知的（来自惯性测量单元），并提出了一种用于多摄像机系统的最小四点和线性八点算法。 Kitt等人（2010）估计使用三焦点张量的自我运动，其涉及三个图像之间的特征。在这些特殊情况下，在RANSAC中使用这些算法可以强有力地获得所有6个自由度。使用RANSAC找到确保正确解决方案所需的迭代次数取决于可以实例化模型的点数。因此，减少的对应数量将减少迭代次数和方法的运行时间。

- **3D-to-2D Matching**: In the case of 3D features at the previous time step and 2D image features at the current time step (3D-to-2D) the transformation is estimated from stereo data (or triangulation when using monocular images). Geiger et al. (2011) present a real-time 3D reconstruction approach using visual odometry. They detect sparse features using blob, corner detector and estimate the ego-motion by minimizing the reprojection error. The estimation is refined with a Kalman filter while the dense 3D reconstruction is obtained by triangulating the image points. In contrast, Engel et al. (2013) continuously estimate a semi-dense inverse depth map to do real-time visual odometry with a monocular camera. The depth is estimated using multi-view stereo for pixel with non-eligible gradients and is represented by a Gaussian probability distribution. The depth estimation is propagated from frame to frame and the transformation is estimated using whole-image alignment. With this semi-dense formulation they achieve comparable performance to fully dense methods while not requiring a depth sensor. Engel et al. (2016) present a direct sparse approach for monocular visual odometry. They use a fully directed probabilistic model and jointly optimize all model parameters (camera poses, camera intrinsics, inverse depth).
- 3D到2D匹配：在当前时间步长（3D到2D）的上一个时间步长的3D特征和2D图像特征的情况下，从立体声数据估计变换（或使用单目图像时的三角测量） ）。盖革等人（2011）提出了一种使用视觉测距法的实时三维重建方法。他们使用斑点，拐角检测器检测稀疏特征，并通过最小化重投影误差来估计自我运动。利用卡尔曼滤波器对该估计进行了改进，而通过对图像点进行三角测量得到了致密的3D重建。相比之下，恩格尔等（2013）连续估计一个半密集反相深度图，用单目相机进行实时视觉测距。使用具有非合格梯度的像素的多视角立体声来估计深度并且由高斯概率分布表示。深度估计从帧到帧传播，并且使用全图像对齐来估计变换。通过这种半密度配方，它们可以实现与完全致密的方法相当的性能，而不需要深度传感器。恩格尔等人（2016）提出了一种直观的单目视觉测距法。他们使用完全定向的概率模型，并共同优化所有模型参数（摄像机姿态，摄像机内在参数，反向深度）。

- **3D-to-3D Matching**: When dealing with 3D correspondences (3D-to-3D), the transformation can be obtained by aligning the two sets of 3D features. In case of visual odometry the extracted features from images are projected into 3D using depth whereas LiDAR-based approaches such as Zhang & Singh (2014, 2015) directly obtain the 3D points from the sensor. The triangulated 3D points from stereo will exhibit a large anisotropic uncertainty due to the small baseline and the quadratic increase of errors with respect to distance. Thus it is more natural to minimize reprojection errors in the images where error statistics can be approximated more easily while laser-based approaches do not suffer from this problem and thus can be optimized more easily in 3D space.
- 3D到3D匹配：当处理3D对比（3D到3D）时，可以通过对齐两组3D特征来获得变换。 在视觉测距的情况下，图像的提取特征使用深度投影为3D，而基于LiDAR的方法（如Zhang＆Singh（2014，2015））直接从传感器获取3D点。 由于基线小，相对于距离的误差的二次增加，来自立体声的三角形3D点将呈现大的各向异性不确定性。 因此，最大限度地减少图像中的重投影误差，使误差统计量可以更容易地近似，而基于激光的方法不会受到此问题的影响，因此可以在3D空间中更容易地进行优化。

- 8.3.1. State-of-the-art 先进技术
- Only few datasets exist for visual odometry and most are too short or consists of low quality imagery. The KITTI benchmark Geiger et al. (2012b) discussed in Section 2 provides a large dataset of challenging sequences and evaluation metrics. We provide the KITTI leaderboard of monocular approaches in Table 9, stereo approaches in Table 10 and LiDAR-based approaches is provided in Table 11. The performance is measured with the average translational and rotational error for all possible subsequences of length (100; : : : ; 800) meters.
- 只有少数数据集存在用于视觉测距，大多数数据太短或由低质量图像组成。 KITTI基准Geiger等 （2012b）提供了一个具有挑战性的序列和评估指标的大数据集。 我们在表9中提供了KITTI单眼方法的排行榜，表10中的立体声方法和基于LiDAR的方法在表11中提供。性能用所有可能的长度（100〜800）子序列的平均平移和旋转误差进行测量。

- **Monocular Visual Odometry**: Monocular visual odometry methods can recover the motion only up to a scale factor. The absolute scale can then be determined by computing the size of objects in the scene, from motion constraints, or integration with other sensors. The eight-point method proposed by Longuet-Higgins (1981) performs poorly in the presence of noise, in particular with uncalibrated cameras. Mirabdollah&Mertsching (2014) investigated the second order statistics of the essential matrix to reduce the estimation error with the eight-point method. They use the Taylor expansion up to the second order terms to obtain a covariance matrix that acts as regularization term along with the coplanarity equations. The drifting problem is particularly difficult in monocular visual odometry because of the lack of depth information. With a ground plane estimation in a real-time monocular SfM system Song & Chandraker (2014) deal with scale-drift and improve upon Mirabdollah & Mertsching (2014) results in Table 9. They combine multiple cues with learned models to adaptively weight per-frame observation covariances for ground plane estimation. Mirabdollah & Mertsching (2015) present a real-time and robust monocular visual odometry approach using the iterative five-point method. They obtain the location of landmarks with uncertainties using a probabilistic triangulation method and estimate the scale of the motion with low quality features on the ground plane. With this approach they outperform all monocular visual odometry methods in Table 9. Since the KITTI dataset require metric output the scale estimate has a strong impact on the performance of the approaches.
- 单目视觉测距：单目视觉测距方法可以恢复运动，只能达到比例因子。然后可以通过计算场景中的对象的大小，运动约束或与其他传感器的集成来确定绝对刻度。 Longuet-Higgins（1981）提出的八点法在噪声的存在下表现不佳，特别是未校准的摄像机。 Mirabdollah＆Mertsching（2014）调查了基本矩阵的二阶统计量，以八点法减少估计误差。他们使用泰勒扩展直到二阶项来获得协方差矩阵，其作为正则化项以及共平面方程。由于缺乏深度信息，单目视觉测距中的漂移问题尤其困难。在实时单目SfM系统中，通过对地面飞机的估计，Song＆Chandraker（2014）对表9中的Mirabdollah＆Mertsching（2014）的结果进行了大规模的漂移和改进。他们将多个线索与学习模型相结合，框架观测协方差估计。 Mirabdollah＆Mertsching（2015）提出了一种使用迭代五点法的实时稳健的单目视觉测距法。他们使用概率三角测量法获得具有不确定性的地标位置，并估计地面上具有低质量特征的运动规模。使用这种方法，它们优于表9中的所有单目视觉测距方法。由于KITTI数据集需要度量输出，所以尺度估计对方法的性能有很大的影响。

- **Stereo Visual Odometry**: Stereo visual odometry methods do not have the problem of estimating the scale because it is directly known from the baseline between the cameras. In addition, they allow to deal with the drifting problem with a joint formulation of ego-motion estimation and mapping. Therefore, stereo methods are typically outperforming monocular methods on the KITTI dataset (see Table 9 and Table 10). Engel et al (2015) propose a real-time large-scale direct SLAM algorithm that couples temporal multi-view stereo with static stereo from a camera setup (Figure 31). This allows them to estimate depth of pixels that are under-constrained in static stereo while avoiding scale-drift that occurs using multi-view stereo. The images are directly aligned based on photoconsistency of high contrast pixel. Pire et al. (2015) divide the problem into camera tracking and map optimization that can be run in parallel. While sharing the same map the tracking task matches features, creates new points and estimates the camera pose whereas the map optimization refines the map with bundle adjustment. This formulation allows them to achieve the same performance while being faster. Deigmoeller & Eggert (2016) follow a very different way by relying exclusively on pure measurements as mentioned before. With the estimation of scene flow on Harris corners and rejection of features with different heuristics they outperform the two SLAM approaches in terms of translational error but have the highest rotational error and runtime in Table 10.
- 立体视觉眼镜：立体视觉测距方法没有估计尺度的问题，因为它是从相机之间的基线直接知道的。此外，它们允许通过自我运动估计和映射的联合计算来处理漂移问题。因此，立体声方法通常在KITTI数据集上优于单目标方法（见表9和表10）。 Engel等人（2015）提出了一种实时大规模直接SLAM算法，将时间多视点立体声与静态立体声从相机设置相结合（图31）。这允许他们估计在静态立体声中受限制的像素的深度，同时避免使用多视角立体声发生的尺度漂移。基于高对比度像素的相依性，图像直接对齐。皮尔等人（2015）将问题分解为并行运行的摄像机跟踪和地图优化。在共享相同的地图时，跟踪任务与特征相匹配，创建新点并估计摄像机姿态，而地图优化通过束调整来优化地图。这种配方允许他们在更快的时候达到相同的性能。 Deigmoeller＆Eggert（2016）遵循一种完全不同的方式，仅依赖于前面提到的纯粹的测量。通过对哈里斯角的场景流的估计和具有不同启发式的特征的拒绝，它们在平移误差方面优于两种SLAM方法，但是在表10中具有最高的旋转误差和运行时间。

- Persson et al. (2015) propose a stereo visual odometry system for automotive applications based on techniques from monocular visual odometry. In particular, they use motion model predicted tracking by matching, similar to Song et al. (2013), and delayed outlier identification. They argue that stereo techniques outperform monocular techniques because the problem formulation is easier. Monocular techniques should be more refined and robust because they need to deal with intrinsically more difficult problem. This allows them to outperform the others in the translational error in Table 10. The two best performing methods decouple the estimation of the rotation and translation as there is a fundamental difference between their estimation. The translation is dependent on the depth in contrast to the rotation. Buczko & Willert (2016a) claim that errors from depth estimation affect the rotation estimation in a coupled formulation and can be avoided by decoupling them. Thus, they use an initial rotation estimation to decouple the rotational and translational optical flow. The resulting characteristics are then used to exclude outliers. Cvisic & Petrovic (2015) compute the motion with a separate estimation of the rotation using the five point and the translation using the three point method. They also present a modified IMU-aided version of the algorithm suitable for embedded systems.
- Persson等人（2015）提出了一种基于单目视觉测距技术的汽车应用立体视觉测距系统。特别地，他们使用运动模型预测跟踪通过匹配，类似于宋等人。 （2013年），并延迟异常情况鉴定。他们认为立体声技术优于单眼技术，因为问题的制定更容易。单眼技术应该更加精细和强大，因为它们需要处理本质上更加困难的问题。这使得它们在表10中的平移误差中优于其他。两种最佳表现方法将旋转和平移的估计解耦，因为它们的估计存在根本差异。翻译取决于与旋转相反的深度。 Buczko＆Willert（2016a）声称，深度估计误差影响耦合公式中的旋转估计，可以通过解耦来避免。因此，它们使用初始旋转估计来解耦旋转和平移光学流。所得到的特征然后用于排除异常值。 Cvisic＆Petrovic（2015）通过使用五点和使用三点法的平移来单独估计旋转来计算运动。它们还提出了适用于嵌入式系统的修改的IMU辅助版本的算法。

- Kreso & Segvic (2015) observed that the camera calibration is critical for visual odometry and that the remaining calibration errors in pre-calibrated systems like KITTI have adversarial effects on the estimation results. They therefore propose to correct the calibration of the camera by exploiting the ground truth motion. The deformation field is recovered by optimizing the reprojection error of point feature correspondences in neighboring stereo frames under the groundtruth motion. Using the deformation field they obtained state-of-the-art results at the time.
- Kreso＆Segvic（2015）观察到，相机校准对于视觉测距是至关重要的，并且预校准系统（如KITTI）中的剩余校准误差对估计结果具有对抗性影响。 因此，他们建议通过利用地面真相运动来校正摄像机的校准。 通过在地面真相运动下优化相邻立体声帧中的点特征对应的重新投影误差来恢复变形场。 使用变形场，他们获得了当时最先进的结果。

- **LiDAR-based Odometry**: The best performing methods on KITTI are using point clouds for ego-motion estimation (Table 11). Zhang & Singh (2014) split the SLAM problem into LiDAR-based odometry at high frequency with low fidelity and LiDAR-mapping at low frequency illustrated in Figure 32. The LiDAR-based odometry matches two consecutive LiDAR scans whereas the LiDAR-mapping matches and registers the new scan to a map. This results in low drift and computational complexity without the need for high accuracy range or inertial measurements. Zhang & Singh (2015) extend this work by combining isual odometry at high frequency with LiDAR-mapping at low frequency which allows them to further improve.
- 基于LiDAR的测距：KITTI上表现最好的方法是使用点云进行自我运动估计（表11）。 Zhang＆Singh（2014）将SLAM问题分解为高频率，低保真度和低频下的LiDAR映射，如图32所示。基于LiDAR的测距仪与两个连续的LiDAR扫描匹配，而LiDAR映射匹配和 将新扫描注册到地图。 这导致低漂移和计算复杂度，而不需要高精度范围或惯性测量。 Zhang＆Singh（2015）通过将高频视觉测距与低频LiDAR映射相结合来扩展这项工作，使他们进一步完善。

- **Discussion**: Though several approaches have addressed the scale-drift problem in monocular visual odometry they cannot compete with approaches using 3D information on the KITTI dataset yet. While LiDAR provides the richest source of information for ego motion estimation, stereo-based methods show competitive results. In Figure 33 we visualize the average translational and rotational errors of the best performing visual odometry methods on the KITTI benchmark. The second row shows the translational error, the third row shows the rotational error while the last row shows the speed. The highest translational and rotational error can usually be obverse in strong turns. Furthermore, the error is correlated with speed and the amount of independently moving objects in the scene which lower the number of features in the background. While large errors can be observed for crowded highway scenes (second from right), only moderate errors occur when the highway is empty (right and second from left). Larger errors can also be observed in very narrow environments (fourth from right) where feature displacements are large. Overall, the most accurate motion estimation is achieved using 3D information, so far. However, stereo cameras are very cheap sensors in comparison to LiDAR laser scanners and stereo-based methods achieve competitive results.
- 讨论：虽然几种方法已经解决了单目视觉测距中的尺度漂移问题，但是它们不能与使用三维信息的方法在KITTI数据集上竞争。虽然LiDAR为自我运动估计提供了最丰富的信息来源，但基于立体的方法显示出有竞争力的结果。在图33中，我们可以看出在KITTI基准测试中表现最佳的视觉测距方法的平均平移和旋转误差。第二行显示平移误差，第三行显示旋转错误，而最后一行显示速度。最高的平移和旋转误差通常在强力转弯时是正面的。此外，该误差与场景中的速度和独立移动物体的数量相关，这降低了背景中的特征数量。虽然拥挤的高速公路场景（右二）可以观察到较大的误差，但高速公路空闲时（左右两侧）只发生中度误差。在特征位移较大的非常狭窄的环境（右四）也可以观察到较大的误差。总的来说，迄今为止，使用3D信息实现了最准确的运动估计。然而，与LiDAR激光扫描仪和基于立体声的方法相比，立体相机是非常便宜的传感器，可以获得竞争优势。

- 8.4. Simultaneous Localization and Mapping (SLAM) 同步定位与构图 (SLAM)
- A detailed map of the environment is a commonly exploited prerequisite for path planning and navigation of an autonomous car. However, in places where a map is not provided or incomplete, the autonomous car needs to locate itself while generating the map. Further, the map needs to be updated continuously to reflect environmental changes over time. In this context, SLAM refers to the task of simultaneous estimation of the location of an agent while continuously building up a map of the environment. One particular challenge in autonomous driving, is that these systems need to handle large-scale environments in realtime.
- 详细的环境地图是自主车路径规划和导航的常见的先决条件。 然而，在没有提供或不完整的地图的地方，自动汽车需要在生成地图的同时进行定位。 此外，地图需要不断更新以反映随着时间的变化。 在这种情况下，SLAM是指同时估计代理的位置，同时不断地构建环境地图的任务。 自主驾驶中的一个特殊挑战是，这些系统需要实时处理大规模环境。

- **Formulation**: Traditionally, the map is represented by a set of landmarks, as for example image features. Early approaches to SLAM have addressed the problem using Bayesian formulations using extended Kalman filters (Smith et al. (1987)) or particle filters (Montemerlo et al. (2002)). Given the last state and current observations, the current state, represented by pose, velocity and the locations of the landmarks is recursively updated. However, this formulation is not applicable to large environments since the belief state and time complexity of the filter update grow quadratically in the number of landmarks in the map. The belief state represents all correlations between all pairs of variables which is O(n2) and whenever a landmark is observed the correlation to all other variables need to be updated with the same complexity. One solution for reducing complexity is a filtering technique based on a Graphical Model that maintains a tractable approximation of the belief state using a thin junction tree as proposed by Paskin (2003). However, it is known that filtering always produces an inconsistent map when applied on nonlinear SLAM problems (Julier & Uhlmann (2001)), which is usually the case when dealing with real data. In contrast, full SLAM approaches, such as graph-based or least-squares formulations, can provide exact solutions considering all poses at once. Kaess et al. (2008) propose an incremental smoothing and mapping approach based on fast incremental matrix factorization. They extend their work Dellaert & Kaess (2006) on factorizing the matrix of the nonlinear least-squares problem to an incremental approach that only recalculates entries which change in the matrix. Kaess et al. (2012) have introduced the Bayes tree, a novel data structure, to allow a better understanding of the connection between graphical model inference and sparse matrix factorization in SLAM. Factored probability densities are encoded in the Bayes tree which naturally maps to a sparse matrix.
- 制定：传统上，地图由一组地标表示，例如图像特征。早期的SLAM方法使用扩展卡尔曼滤波器（Smith et al。（1987））或粒子滤波器（Montemerlo et al。（2002））使用贝叶斯方程来解决问题。给定最后一个状态和当前的观测值，以姿态，速度和地标位置为代表的当前状态递归更新。然而，这个公式不适用于大型环境，因为过滤器更新的信念状态和时间复杂度在地图上的地标数量上二次增长。置信状态表示O（n2）的所有变量对之间的所有相关性，并且每当观察到地标时，与所有其他变量的相关性需要以相同的复杂度进行更新。降低复杂性的一个解决方案是基于图形模型的过滤技术，该方法使用Paskin（2003）提出的薄连接树来维护置信状态的易处理近似。然而，已知当应用于非线性SLAM问题时，滤波总是产生不一致的映射（Julier＆Uhlmann（2001）），这通常是在处理真实数据时的情况。相比之下，完整的SLAM方法，如基于图形或最小二乘法的公式，可以提供考虑到所有姿势的精确解决方案。 Kaess等人（2008）提出了一种基于快速增量矩阵分解的增量平滑和映射方法。他们扩展了他们的工作Dellaert＆Kaess（2006），将非线性最小二乘问题的矩阵分解为仅重新计算矩阵变化的条目的增量方法。 Kaess等人（2012）引入了贝叶斯树，一种新颖的数据结构，以便更好地了解SLAM中图形模型推理和稀疏矩阵分解的连接。因子密度在Bayes树中编码，它自然地映射到稀疏矩阵。

- **Environmental Changes**: A major challenge in SLAM are changes in the environment that might not be represented bya map. To alleviate this problem, Levinson et al. (2007) create a map only consisting of features that are very likely to be static. Using 3D LiDAR they retain only flat surfaces and obtain an infrared reflectivity map of overhead views of the road surface. The map is then used to locate a vehicle with a particle filter in real-time. Levinson & Thrun (2010) extend this work considering maps as probability distributions over environment properties instead of a fixed representation. Specifically, every cell of the probabilistic map is represented as its own Gaussian distribution over remittance values. This allows them to represent the world more accurately and localize with fewer errors. In addition, they can use offline SLAM to align multiple passes of the same environment at different time to build an increasingly robust understanding of the world.
- 环境变化：SLAM中的一个主要挑战是在可能无法表现的环境中发生变化一张地图。 为了缓解这个问题，Levinson等 （2007）创建一个仅由很可能是静态的功能组成的地图。 使用3D LiDAR，它们只保留平面，并获得路面俯视图的红外反射率图。 然后，该地图用于实时地定位具有粒子滤波器的车辆。 Levinson＆Thrun（2010）扩展了这项工作，将地图视为环境属性的概率分布而不是固定表示。 具体来说，概率图的每个单元都被表示为其自身的高斯分布在汇款值上。 这允许他们更准确地代表世界，并以更少的错误进行本地化。 此外，他们可以使用离线SLAM来在不同时间对齐相同环境的多个通道，以建立对世界的越来越强的理解。

- 8.4.1. Loop Closure Detection 环路闭合检测
- The relocalization in already mapped areas is an important subproblem of SLAM, referred to as loop closure detection. Relocalization is used to correct drifts in the trajectory and inaccuracies in the map caused by drift. Cummins & Newman (2008) present a probabilistic approach for the recognition of places based on their appearance. They learn a generative model of place appearances using bag-of-words because distinctive combinations of visual words will often arise from common objects. The generative model is robust and works even in visually repetitive environments. The performance of the approach is demonstrated on a self recorded dataset and visualized in Figure 34. Paul & Newman (2010) extend this idea by incorporating distance between words coupled to the observation of pairs of visual words with a random graph. The random graph models the pairwise distance between words besides their distribution of occurrences. In contrast, Lee et al. (2013b) show that the relative pose with metric scale between two loopclosing pose-graph vertices can directly be obtained from the epipolar geometry of a multi-camera system with overlapping views. They simplify the problem using a planar constraint on the motion of a car and estimate the loop-constraint using least squares optimization.
- 已映射区域的重新定位是SLAM的一个重要的子问题，称为环路闭包检测。重新定位用于校正由漂移引起的轨迹和地图误差的漂移。康明斯＆纽曼（2008）提出了一种基于外观识别地位的概率方法。他们通过使用单词来学习一个生成模式的地方出现，因为常见的对象常常会产生视觉词汇的特殊组合。生成模型是健壮的，即使在视觉上重复的环境中也是如此。该方法的性能在自我记录的数据集上得到证明，并在图34中可视化。Paul＆Newman（2010）通过将与观察对话视觉词之间的距离与随机图相结合来扩展这一想法。随机图模型除了发生分布之外的词之间的成对距离。相比之下，Lee et al。 （2013b）显示，可以直接从具有重叠视图的多摄像机系统的对极几何获得两个闭环姿态图顶点之间的度量标度的相对姿态。他们使用对汽车运动的平面约束来简化问题，并使用最小二乘法优化来估计循环约束。

- **LiDAR-based**: Image-based loop closure detection can become unreliable in case of strong illumination changes or strong viewpoints changes. In contrast, LiDAR-based localization is not affected by changes in illumination and does not suffer as much from changes in viewpoint due to the captured 3D geometry. Dube et al. (2016) propose a loop closure detection algorithm based on matching 3D segments. Segments from the point cloud are extracted and described using a combination of descriptors. Matching of segments is performed by obtaining candidates with kd-tree search in feature space and estimating the matching score of the candidates with a random forest.
- 基于LiDAR：基于图像的闭环检测在强烈的照明变化或强烈的观点变化的情况下可能变得不可靠。 相比之下，基于LiDAR的定位不受照明变化的影响，并且由于所捕获的3D几何形状而不会受到视点变化的影响。 Dube等人 （2016）提出了一种基于匹配3D片段的闭环检测算法。 使用描述符的组合来提取和描述来自点云的分段。 通过在特征空间中获得具有kd-tree搜索的候选者并且利用随机森林来估计候选者的匹配分数来执行段的匹配。

- 8.4.2. Visual SLAM  视觉SLAM
- Lategahn et al. (2011) propose a dense stereo visual SLAM method that estimates a dense 3D map. Using a sparse visual SLAM system, they obtain the pose and a sparse map. For the dense 3D map, they compute a dense representation from stereo in a local coordinate system and continuously update the map by tracking the local coordinate systems with the sparse SLAM system. Engel et al. (2014) extend their semi-dense method for visual odometry (Engel et al. (2013)) by performing image alignment and loop closure detection using a formulation based on optimizing the similarity transformation. Semi-dense depth is estimated using multi-view stereo from small baselines to create and refine a semi-dense map using pose graph optimization. The fusion of visual and inertial cues proposed by Leutenegger et al. (2013) takes advantage of their complementary nature. Instead of filtering, they use a non-linear optimization approach and integrate the IMU error with the reprojection error of landmarks into a joint cost function. Mur-Artal et al. (2015) use the ORB features proposed by Rublee et al. (2011) for tracking, mapping, relocalization and loop closure. They combine methods from loop detection (Galvez-Lopez & Tardos (2012)), loop closing (Strasdat et al. (2010, 2011)) and pose graph optimization (Kummerle et al. (2011)) into one system.
- Lategahn等（2011）提出了一种密集的立体视觉SLAM方法来估计密集的3D地图。使用稀疏的视觉SLAM系统，它们获得姿态和稀疏映射。对于密集的3D地图，它们从局部坐标系中的立体声计算密集表示，并通过使用稀疏SLAM系统跟踪局部坐标系来连续更新地图。恩格尔等人（2014）通过使用基于优化相似变换的公式执行图像对准和闭环检测来扩展其用于视觉测距的半密集方法（Engel等人（2013））。使用来自小基线的多视角立体声估计半密度深度，以使用姿势图优化来创建和细化半密集地图。 Leutenegger等人提出的视觉和惯性线索的融合（2013）利用其互补性。他们不是过滤，而是使用非线性优化方法，将IMU错误与地标的重新投射错误整合到联合成本函数中。 Mur-Artal等（2015）使用Rublee等人提出的ORB特征（2011年），用于跟踪，绘制，重新定位和循环关闭。他们将来自循环检测的方法（Galvez-Lopez＆Tardos（2012）），循环关闭（Strasdat等（2010,2011））和姿态图优化（Kummerle等（2011））组合成一个系统。

- 8.4.3. Mapping  绘图
- For autonomous driving applications, metric and semantic maps at different level of details are required to solve different tasks. Metric maps allow accurate localization whereas semantic maps can provide problem specific information such as parking areas for automated parking. Those maps can also be generated offline with computationally expensive methods and later incorporated into an autonomous driving system.
- 对于自主驾驶应用，需要不同级别细节的度量和语义地图来解决不同的任务。 公制地图允许精确定位，而语义地图可以提供特定于问题的信息，例如用于自动停车的停车区域。 这些地图也可以用计算上昂贵的方法离线生成，然后再并入自主驾驶系统。
- **Metric Maps**: The Google Street View project (Anguelov et al. (2010)) is a prominent example for a large collection of panoramic imagery in cities around the world. For collecting the dataset, they estimate the pose in a Kalman-filter-based approach fusing data from GPS, wheel encoder and inertial navigation. Estimation at 100 Hz allows to accurately match image pixels from 15 small cameras to 3D rays from a laserscanner. The pose estimates are refined with a probabilistic graphical model of the network that represents all known roads and intersections in the world. From the image and laserscan data, they reconstruct the scene and obtain photorealistic 3D models by robustly fitting coarse meshes. Frahm et al. (2010) propose a dense 3D reconstruction approach from Internet-scale photo collections. Geometric relationships between the images are estimated using a combination of 2D appearance, color and 3D multi-view geometry constraints. They obtain the dense geometry of the scene via fast plane sweeping stereo and a depth map fusion approach. Exploiting the appearance and geometry constraints, they present a highly parallel approach which allows to process 3 million images within a day on a single computer. Figure 35 shows two example models reconstructed from Flickr images of Rome and Berlin. For autonomous driving applications, it is often suffcient to map the road surface in 2D (i.e., in bird’s eye view) which allows for localization with respect to features on the road such as road markings or imperfections in the road surface. Geiger (2009) present an approach for road mosaicing in dynamic environments to create obstacle-free bird eye views. The road surface is extracted using optical flow on Harris corners and approximated by a plane. This allows to describe the mapping between the images with homographies. The road images are finally combined using multi-band blending.
- 公制地图：Google街景视图项目（Anguelov et al（2010））是世界各地城市大型全景图像的一个突出例子。为了收集数据集，他们用基于卡尔曼滤波器的方法估计姿态，融合来自GPS，轮编码器和惯性导航的数据。在100Hz的估计允许从15个小型相机到激光扫描仪的3D光线的图像像素精确匹配。使用代表世界上所有已知道路和交叉点的网络的概率图形模型来改进姿态估计。从图像和激光扫描数据，他们通过鲁棒地拟合粗网格重建现场并获得逼真的3D模型。 Frahm et al。 （2010）提出了一个从互联网规模的照片收藏的密集3D重建方法。使用2D外观，颜色和3D多视图几何约束的组合来估计图像之间的几何关系。他们通过快速平面扫描立体声和深度图融合方法获得场景的密集几何。利用外观和几何约束，它们呈现出高度并行的方法，允许在单个计算机上在一天内处理300万张图像。图35显示了从罗马和柏林的Flickr图像重建的两个示例模型。对于自主驾驶应用，通常足以在2D（即鸟瞰图）中映射路面，其允许相对于道路上的特征（例如道路标记或道路表面的缺陷）进行定位。 Geiger（2009）在动态环境中提出了道路拼接方法，以创造无障碍鸟瞰图。路面使用Harris角上的光流提取，并用平面近似。这允许描述具有同形异义的图像之间的映射。道路图像最终使用多频带混合组合。

- **Semantic Maps**: All methods discussed so far focus on creating metric maps ignoring semantic information. However, for tasks like automated parking, a semantic map that is updated jointly with the metric map is necessary. Grimmett et al. (2015) fuse semantic and metric maps for vision-only automated parking. They update the map with static and dynamic labels and use active learning for lane, parking space and pedestrian crossings detection.
- 语义地图：迄今为止所讨论的所有方法都集中在创建忽略语义信息的度量标图。 然而，对于诸如自动停车的任务，需要与度量图一起更新的语义地图。 Grimmett等人 （2015）视觉专用自动停车的保险丝语义和度量地图。 他们用静态和动态标签更新地图，并使用主动学习车道，停车位和行人过路检测。

- 8.5. Localization 本地化
- Localization is a well-studied problem in both robotics and vision, covering a broad range of techniques from indoor localization of a robot using noisy sensory measurements to locating where a picture was taken in the entire world. From an autonomous driving perspective, the main task is to precisely localize the ego-vehicle on a map. Localization is also an important subroutine of SLAM approaches, where it is used for detecting loop-closures and correcting drift when mapping the environment, see Section 8.4.1.
- 本地化是机器人和视觉两个方面的一个很好的研究问题，涵盖了使用嘈杂的感官测量的机器人的室内定位到定位在整个世界拍摄照片的广泛的技术。 从自主驾驶的角度来看，主要任务是精确地将自己的车辆定位在地图上。 本地化也是SLAM方法的重要子程序，用于在映射环境时检测环路闭合和校正漂移，参见第8.4.1节。

- Localization can be performed using either sensors like a GPS system or visual information based on images. Using GPS alone typically provides an accuracy around 5 m. Although centimeter-level precision is possible in open spaces using combinations of sensors as in KITTI car (Geiger et al. (2012b)), it is often rendered infeasible in traffic scenes with several disturbing effects such as occlusions by vegetation and buildings or multi-path effects due to reflections. Therefore, image-based localization independent of satellite systems is highly relevant. Early image-based techniques (Li et al. (2009); Zheng et al. (2009)) approach the problem as classification into one of a predefined set of places which are referred as “landmarks”. Others (e.g. Hays & Efros (2008)) create a database of images with known locations and formulate the localization as an image retrieval problem. These methods require a similarity measure to compare images based on local or global appearance cues. The larger the database, the more diffcult the localization task becomes. Challenges include appearance changes, similar looking places, and the changes due to viewpoint or position.
- 可以使用诸如GPS系统的传感器或基于图像的视觉信息来执行本地化。单独使用GPS通常提供大约5米的精度。虽然在KITTI车（Geiger等人（2012b））中使用传感器组合的开放空​​间中厘米级精度是可能的，但是在具有若干扰动效果的交通场景中，通常会使其变得不可行，例如植被和建筑物或多由于反射导致的路径效应。因此，独立于卫星系统的图像定位是非常重要的。早期的基于图像的技术（Li et al。（2009）; Zheng et al。（2009））将问题作为一种被称为“地标”的预定义的一组地点的分类。其他（例如Hays＆Efros（2008））创建具有已知位置的图像数据库，并将其定位为图像检索问题。这些方法需要相似性度量来比较基于局部或全局外观线索的图像。数据库越大，本地化任务就越难。挑战包括外观变化，看似类似的地方，以及由观点或位置引起的变化。

- **Survey**: Lowry et al. (2016) provide a comprehensive review of the current state of place recognition research. They first define what qualifies as a place in the context of robotic navigation by referencing to studies in psychology and neuroscience. Then, they review ways of describing a place using local or global descriptors and/or metric range information. They also provide a taxonomy based on the level of physical abstraction in the map and whether or not metric information is included in the place description. They further discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment and finally provide some future directions with respect to advances in deep learning, semantic scene understanding, and video description.
- 调查：Lowry et al（2016）全面审查了目前状态识别研究的情况。 他们首先通过参考心理学和神经科学研究来定义在机器人导航的上下文中是什么。 然后，他们审查使用本地或全局描述符和/或度量范围信息描述场所的方法。 它们还提供基于地图中物理抽象级别的分类法，以及度量信息是否包含在地点描述中。 他们进一步讨论了地方识别解决方案如何隐含或明确地解释环境中的外观变化，并最终提供了深度学习，语义场景理解和视频描述方面的未来发展方向。

- **Monte Carlo Methods**: The problem of map localization has been traditionally approached using Monte Carlo methods which recover the probability distribution over the agents pose by drawing a set of samples. Dellaert et al. (1999) define indoor localization in two steps, global position estimation and local position tracking over time. Instead of modeling the probability density function itself, they represent uncertainty by maintaining a set of samples and update the representation over time using Monte Carlo methods. This allows them to model arbitrary multimodal distributions in a memory efficient way. Outdoor localization is in general more challenging compared to the indoor localization task due to its scale and often unreliable sensor information such as GPS failures. Oh et al. (2004) use semantic information available in maps to compensate for the failure cases of GPS sensors. By exploiting knowledge about the environment, they assign probabilities to the target zones on the map, such as zero probability to the buildings. They incorporate these map-based priors in the particle filter formulation to bias the motion model toward areas of higher probability.
- 蒙特卡罗方法：传统上使用蒙特卡罗方法处理地图定位的问题，通过绘制一组样本来恢复代理姿势的概率分布。 Dellaert等（1999）定义了两个步骤的室内定位，全局位置估计和局部位置跟踪随着时间的推移。不是对概率密度函数本身进行建模，而是通过维护一组样本来表示不确定性，并使用蒙特卡罗方法随时间更新表示。这允许他们以有效的方式对任意多模态分布进行建模。室内定位通常比室内定位任务更具挑战性，因为它的尺度和常常不可靠的传感器信息，如GPS故障。哦等（2004）使用地图中可用的语义信息来补偿GPS传感器的故障情况。通过利用有关环境的知识，他们将概率分配给地图上的目标区域，例如建筑物的零概率。他们将这些基于地图的先验结合在粒子滤波器公式中，以将运动模型偏向较高概率的区域。

- **Metric, Topological, Topometric**: Visual localization techniques are commonly classified into metric and topological methods. Metric localization is achieved by computing the 3D pose with respect to a map. Topological localization approaches provide a coarse estimate from a finite set of possible locations which are represented as nodes in a graph which are connected by edges that link them according to some distance or appearance criteria. Metric localization can be very accurate, but is usually not suitable for long sequences, while topological localization may be more reliable, but only provides rough estimates. Badino et al. (2012) propose a topometric approach as a combination of topological and metric localization to provide geometrically accurate localization using graph-based methods. In contrast to topological methods, the graph is more fine-grained and each node corresponds to a metric location without a semantic meaning. During mapping phase, the graph is constructed using the vehicle position from GPS at fixed distance intervals and associating visual or 3D features to the corresponding graph node. At runtime, real-time localization is performed using a Bayes filter to estimate the probability distribution of the vehicle position along the route by matching features extracted from the sensor data to the map’s feature database. Brubaker et al. (2016) also leverage a graph-based representation. In contrast to traditional localization approaches, however, they do not require a visual feature database of the environment, but instead directly build this graph from road networks extracted from OpenStreetMap. They further propose a probabilistic model which allows to infer a distribution over the vehicle location using visual odometry measurements. For tractability in very large environments, they leverage several analytic approximations for efficient inference yielding higher stability compared to particle-based filtering techniques which suffer from particle depletion when ambiguities persist over long periods.
- 公制，拓扑，拓扑学：视觉定位技术通常分为度量和拓扑方法。通过计算相对于地图的3D姿态来实现度量定位。拓扑定位方法从可能位置的有限集合提供粗略的估计，这些可能位置被表示为图中的节点，其通过根据一些距离或外观标准将它们连接的边缘连接。度量定位可以非常准确，但通常不适用于长序列，而拓扑定位可能更可靠，但仅提供粗略估计。 Badino等人（2012）提出了拓扑和度量定位的组合的拓扑方法，以使用基于图的方法提供几何精确的定位。与拓扑方法相反，图形更细粒度，每个节点对应于没有语义意义的度量位置。在映射阶段，使用GPS定位距离间隔的车辆位置构建图形，并将视觉或3D特征与相应的图形节点相关联。在运行时，使用贝叶斯滤波器执行实时定位，以通过从传感器数据提取的特征匹配到地图的特征数据库来估计沿着路线的车辆位置的概率分布。 Brubaker等人（2016）也利用了基于图形的表示。然而，与传统的本地化方法相比，它们不需要环境的视觉特征数据库，而是从OpenStreetMap提取的道路网络直接构建该图。他们进一步提出一个概率模型，其允许使用视觉测距测量来推断车辆位置上的分布。对于非常大的环境中的易处理性，与基于粒子的过滤技术相比，它们利用多个分析近似来进行有效推断，从而在长时间持续存在模糊性时，会遇到粒子耗尽。

- **Scale and Accuracy**: For the problem of localization, the scale of the target area is a distinctive property to compare different approaches and is related to the accuracy achieved. Both scale and accuracy depend on the methodology used, such as map-based approaches (Brubaker et al.(2016)) which might suffer from the errors on the map and descriptor-based approaches (Badino et al. (2012); Schreiber et al. (2013)) using global or local descriptors. While the descriptor based method of Badino et al. (2012) achieves an average localization accuracy of 1 m over an 8 km route, the road network based localization approach of Brubaker et al. (2016) attains an accuracy of 4 m on a 18 km2 map containing 2,150 km of drivable roads. Schreiber et al. (2013) point out that the required precision for autonomous driving and future driver assistance systems is in the range of a few centimeters and present a feature-based localization algorithm which can achieve this on approximately 50 km of rural roads. They approach the problem from the perspective of lane recognition. In a separate drive, they create a highly accurate map that contains road markings and curbs. While driving, they detect and match them to the map in order to determine the position of the vehicle relative to the markings.
- 规模和精度：对于本地化的问题，目标区域的规模是比较不同方法的独特属性，与实现的准确性有关。尺度和准确性取决于所使用的方法，例如基于地图的方法（Brubaker等人（2016）），其可能遭受地图上的错误和基于描述符的方法（Badino等（2012）; Schreiber et （2013）），使用全局或局部描述符。虽然Badino等人的基于描述符的方法（2012）在8公里路线上实现了1米的平均定位精度，Brubaker等人的基于道路网络的本地化方法。 （2016年）在18公里的地图上达到4米的精度，包含2,150公里的可驱动道路。 Schreiber et al。 （2013）指出，自主驾驶和未来驾驶员辅助系统所需的精度在几厘米的范围内，并提出了一种基于特征的定位算法，可以在约50公里的农村道路上实现这一点。他们从车道识别的角度来解决问题。在一个单独的驱动器，他们创建一个高度精确的地图，包含路标和路缘。驾驶时，他们检测并匹配他们到地图，以确定车辆相对于标记的位置。

- **Structure-based Localization**: While the output of traditional localization approaches is either a rough camera position or a distribution over positions, a more recent line of work which is known as “structure-based localization” aims to estimate all camera matrix parameters, including position, orientation, and camera intrinsics. Localization is realized as a 2D-to-3D matching problem where the 2D points on the images are matched to a large, geo-registered 3D point cloud and the pose is estimated with respect to correspondences as shown in Figure 36.
- 基于结构的本地化：虽然传统的本地化方法的输出是粗略的相机位置或位置分布，但是称为“基于结构的定位”的更新的工作线旨在估计所有相机矩阵参数，包括位置 ，方向和相机内在性。 本地化实现为2D到3D匹配问题，其中图像上的2D点与大的地理注册的3D点云匹配，并且相对于对应度估计姿势，如图36所示。

- In structure-based approaches, the pose estimate provides a powerful geometric constraint for validating the location estimate. However, a straightforward solution, for example direct matching by approximate nearest neighbor search using SIFT features, would result in many incorrect matches. With growing model size, the discriminative power of the descriptors decreases and matching becomes more ambiguous. Consequently, RANSAC techniques have diffculty finding the correct pose. To address this issue, Li et al. (2012) find statistical cooccurrences of 3D model points in images, and then use them as a sampling prior for RANSAC to exploit co-visibility relations. In addition, they employ a bidirectional matching scheme, forward from features in the image to points in the database and inverse from points to image features. They show that the bidirectional approach performs better than forward or inverse matching alone. Besides ambiguities, the amount of memory required for storing the large number of descriptors contained in the model is another problem related to large scale. Model compression by reducing the number of points produces fewer matches and increases the number of images which cannot be localized. Instead, more recent methods (Sattler et al. (2015, 2016)) use quantization into a fine vocabulary where each descriptor is represented by its word ID. Sattler et al. (2015) separate the diffcult problem of finding a unique 2D-3D matching into two simpler ones. They first establish locally unique 2D-3D matches using a fine visual vocabulary and a visibility graph which encodes the visibility relation between 3D points and cameras. Then, they disambiguate these matches by using a simple voting scheme to enforce the co-visibility of the selected 3D points. Their experiments show that matching based on a visual vocabulary can achieve state-of-the-art. Sattler et al. (2016) propose a prioritized matching scheme based on quantization, focusing on efficiency. They significantly accelerate 2D-to-3D matching by considering more likely features first and terminating the correspondence search as soon as enough matches are found.
- 在基于结构的方法中，姿态估计为验证位置估计提供了强大的几何约束。然而，直接的解决方案，例如通过使用SIFT特征的近似最近邻搜索的直接匹配将导致许多不正确的匹配。随着模型大小的增加，描述符的辨别力下降，匹配变得越来越模糊。因此，RANSAC技术难以找到正确的姿势。为了解决这个问题，Li et al。 （2012）在图像中发现3D模型点的统计同时性，然后将其用作RANSAC之前的抽样以利用共同可见性关系。此外，它们采用双向匹配方案，从图像中的特征向前转到数据库中的点，并从点到图像特征反向。他们表明双向方法比单向前向或反向匹配更好。除了模糊之外，存储模型中包含的大量描述符所需的内存量是与大规模相关的另一个问题。模型压缩通过减少点数产生较少的匹配，并增加不能本地化的图像数量。相反，更新的方法（Sattler等人（2015年，2016年））将量化用于精细词汇，其中每个描述符由其词ID表示。 Sattler等人（2015年）将找到独特的2D-3D匹配的困难问题分成两个简单的。他们首先使用精细的视觉词汇和可视化图来建立本地独特的2D-3D匹配，该图可以对3D点和相机之间的可见性关系进行编码。然后，他们通过使用简单的投票方案来强制所选3D点的共同可见性来消除这些比赛的歧义。他们的实验表明，基于视觉词汇的匹配可以实现最先进的。 Sattler等人（2016）提出了基于量化的优先匹配方案，重点关注效率。通过首先考虑更多可能的功能，并且一旦找到足够的匹配就终止对应搜索，从而显着加速了2D到3D的匹配。

- **Structure-based Localization using Deep Learning**: Kendall et al. (2015) and Walch et al. (2016) use a convolutional neural network to regress the camera pose from a single RGB image in an end-to-end manner. The motivation for using CNNs for this task is to eliminate the problems caused by large textureless areas, repetitive structures, motion blur, and illumination changes which can be challenging for feature based methods. In contrast to classical localization approaches whose runtime depends on several factors such as the number of features found in a query image or the number of 3D points in the model, the runtime of CNN-based approaches only depends on the size of the network. Kendall et al. (2015) modify GoogLeNet (Szegedy et al.(2015)) by replacing softmax classifiers with affine regressors and inserting another fully connected layer before the final regressor which can be used as a localization feature vector for further analysis. The final architecture, dubbed PoseNet is initialized by using the weights of classification networks trained on giant datasets such as ImageNet (Deng et al. (2009)) and Places (Zhou et al. (2014)). Further, it is fine-tuned on a new
pose dataset which was automatically created by using SfM to generate camera poses from a video of the scene. Walch et al. (2016) use a similar approach, but in addition they spatially correlate each element of the output of the CNN with Long Short-Term Memory (LTSM) units by exploiting their memorization capabilities. This way, the network is able to capture more contextual information and outperform PoseNet in different localization tasks including large-scale outdoor, small-scale indoor, and a newly proposed large-scale indoor localization benchmark. Although CNN-based approaches cannot match the precision of state-of the-art SIFT-based methods (Sattler et al. (2016)), their importance becomes more apparent in indoor environments with large textureless surfaces and repetitive scene elements where SIFT-based method cannot produce enough matches to obtain correct SfM reconstructions.
- 使用深度学习的基于结构的本地化：Kendall et al（2015）和Walch et al（2016）使用卷积神经网络以端对端的方式从单个RGB图像中回归相机姿态。使用CNN进行这项任务的动机是消除由无纹理区域，重复结构，运动模糊和照明变化引起的问题，这对基于特征的方法来说可能具有挑战性。与经典的本地化方法相比，其运行时间取决于几个因素，例如在查询图像中发现的特征数量或模型中的3D点的数量，基于CNN的方法的运行时间仅取决于网络的大小。 Kendall等人（2015）修改GoogLeNet（Szegedy等（2015）），通过用仿射回归器替换softmax分类器，并在最后的回归算子之前插入另一个完全连接的层，可以用作定位特征向量进行进一步分析。通过使用在诸如ImageNet（Deng等人（2009））和Places（Zhou等人（2014））等巨型数据集上训练的分类网络的权重，将最终的架构称为PoseNet。此外，它是一个新的微调
姿态数据集，通过使用SfM自动创建，以从场景的视频生成相机姿势。 Walch等人（2016）使用了类似的方法，但是通过利用其记忆功能，还可以将CNN的输出的每个元素与长时间内存（LTSM）单元空间相关。这样，网络能够捕获更多的语境信息，并且在不同的本地化任务（包括大型户外，小型室内）和新提出的大型室内本地化基准测试中胜过PoseNet。虽然基于CNN的方法不符合最先进的基于SIFT的方法的精度（Sattler等（2016）），但它们的重要性在具有大的无纹理表面和重复场景元素的室内环境中变得更加明显，其中SIFT-基于方法不能产生足够的匹配以获得正确的SfM重建。

- **Cross-view Localization**: It is diffcult to keep ground imagery around the world up to date, while it is much easier to establish live maps from aerial images and satellites. This gives rise to a new approach, geo-localization which tries to register ground-level images to aerial imagery. The underlying idea is to learn a mapping between ground-level and aerial image viewpoints to localize a ground-level query in an aerial image reference database. Lin et al. (2013) match ground-level queries to other ground-level reference photos as in traditional geolocalization, but then use the overhead appearance and land cover attributes of those ground-level matches to build slidingwindow classifiers in the aerial and land cover domain. In contrast to previous methods, they can often localize a query even if it has no corresponding ground-level images in the database by learning the co-occurrence of features in different views. Lin et al. (2015) collect a cross-view patch dataset using range data and camera parameters from Google street views to warp the dominant building surface plane to appear approximately like a 45% aerial view. Inspired by the success of face verification algorithms using deep learning, they train a Siamese network to match cross-view pairs of the same location. Workman et al. (2015) introduce another massive cross-view dataset. They first use CNNs for extracting ground-level image features and then, they learn to predict these features from aerial images of the same location. This way, the CNN is able to extract semantically meaningful features from aerial images without manually specifying semantic labels. They conclude that the crossview localization approach can obtain a precise estimate of the geographic locations which are distinctive from above. Otherwise, it can be used as a pre-processing step to a more expensive matching process.
- 横向本地化：将世界各地的地面图像保持原状，同时从航空图像和卫星建立实时地图更为容易。这产生了一种新的方法，地理定位尝试将地面图像注册到航空图像。其基本思想是学习地面和航空图像视点之间的映射，以便在空间图像参考数据库中本地化地面级查询。林等人（2013年）将传统地理定位中的地面级别查询与其他地面级参考照片进行匹配，然后利用这些地面级别匹配的架空外观和土地覆盖属性在空中和陆地覆盖域中构建滑动窗口分类器。与之前的方法相反，即使通过学习不同视图中的特征的同现，数据库中也没有相应的地面图像，所以它们通常可以定位查询。林等人（2015）将使用Google街景视图的范围数据和摄像机参数收集交叉视图修补程序数据集，以扭曲主要建筑物的平面，大致如45％的鸟瞰图。灵感来自使用深度学习的面部验证算法的成功，他们训练一个暹罗网络来匹配相同位置的交叉视图对。工人等（2015）引入了另一个巨大的跨视图数据集。他们首先使用CNN提取地面图像特征，然后学习从相同位置的航空图像预测这些特征。这样，CNN能够从空中图像中提取语义上有意义的特征，而无需手动指定语义标签。他们得出结论，跨视图本地化方法可以获得与上述不同的地理位置的精确估计。否则，它可以用作更加昂贵的匹配过程的预处理步骤。

- **Cross-view Localization: Buildings**: There are methods specialized to building facades in cross-view matching. The repeating patterns can yield a valuable matching indicator for regularity-driven approaches (Figure 37). By combining satellite and oblique bird’s eye-view, Bansal et al. (2011) first extract building outlines and facades and then match the ground image to oblique aerial images based on a statistical description of the facade pattern. Wolff et al. (2016) define a matching cost function to compare a street view motif to an aerial view motif based on similarity of color, texture and edge-based context features.
- 跨界本地化：建筑物：专门用于建立立体视角匹配的方法。 重复模式可以为规则性驱动的方法产生有价值的匹配指标（图37）。 通过结合卫星和倾斜的鸟瞰图，Bansal et al。 （2011）首先提取建筑物轮廓和立面，然后根据立面图案的统计描述将地面图像与倾斜航空图像进行匹配。 Wolff等人 （2016）定义了匹配成本函数，以基于颜色，纹理和基于边缘的上下文特征的相似性将街景图案与鸟瞰图图案进行比较。

- **Cross-view Localization**: Reconstructions: Another line of work addresses the problem of geo-referencing a reconstruction by automatic alignment with a satellite image, floor plan, map, or other overhead view. Kaminsky et al. (2009) compute the optimal alignment between SfM reconstructions and overhead images using an objective function that matches 3D points to image edges and imposes free space constraints based on the visibility of points in each camera. Matching ground and aerial images directly is a diffcult endeavor due to the large differences in their camera viewpoints, occlusions, and imaging conditions. Instead of seeking invariant feature detections, Shan et al. (2014) propose a viewpoint-dependent matching technique by exploiting approximate alignment information and underlying 3D geometry.
- 跨视图本地化：重建：另一行工作通过与卫星图像，平面图，地图或其他俯瞰视图进行自动对齐来解决地理参考重建的问题。 Kaminsky等人 （2009）使用将3D点与图像边缘匹配的目标函数计算SfM重建和开销图像之间的最佳对齐，并且基于每个相机中的点的可见性来施加自由空间约束。 由于摄像机视点，遮挡和成像条件的差异很大，直接匹配地面和航空图像是一个难题。 Shan等人，而不是寻求不变特征检测 （2014）通过利用近似对齐信息和底层3D几何提出了一种与视点相关的匹配技术。

- **Semantic Alignment from LiDAR**: Several companies acquire LiDAR data from scanners mounted on cars driving through cities to acquire 3D models of real-world urban environments. However, the accuracy of the 3D point positions acquired by the 3D scanners depends on the scanner poses predicted by GPS, inertial sensors, and SfM, which often fail in urban environments. These misalignments cause problems for point cloud registration methods. Yu et al. (2015) propose to align semantic features that can be matched robustly at different scales. By following a coarse-to-fine approach, they first successively align roads, facades, and poles, which can be matched robustly. In the following, they match cars and other small objects, which require better initial alignments to find correct correspondences. The use of semantic features provides a globally consistent alignment of LiDAR scans and their evaluation shows improvement over the initial alignments.
- 来自LiDAR的语义对齐：几家公司从驾驶城市车辆的扫描仪获取LiDAR数据，以获取现实城市环境的3D模型。 然而，由3D扫描仪获取的3D点位置的准确度取决于GPS，惯性传感器和SfM预测的扫描仪姿势，这些都在城市环境中经常失败。 这些不对齐会导致点云登记方法的问题。 Yu et al。 （2015）提出了可以在不同规模下强化匹配的语义特征。 通过遵循粗略的方法，他们首先连续对齐可以鲁棒匹配的道路，立面和极点。 在下文中，它们匹配汽车和其他小物体，这需要更好的初始对准以找到正确的对应关系。 使用语义特征提供了全局一致的LiDAR扫描对齐方式，并且它们的评估显示出比初始比对的改进。
