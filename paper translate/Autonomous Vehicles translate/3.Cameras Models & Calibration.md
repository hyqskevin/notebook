## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
自动驾驶技术的计算机视觉：问题，数据和前沿技术

### 3. Cameras Models & Calibration 摄像头模块&校准
- 3.1. Calibration 校准
- Multiple sensors including odometry, range sensors, and different types of cameras such as perspective and fish-eye are widely used in automotive context. Calibration is the problem of estimating intrinsic and extrinsic parameters of these sensors to relate 2D image points to 3D world points and represent sensed information in a common coordinate system in case of multiple sensors. Fiducial markings on checkerboard patterns are the standard tool for calibration. Almost all systems use them either for initialization or for joint optimization to improve the intrinsics. Reprojection error which is the pixel distance between a projected point and a measured one, is used as a way of measuring accuracy quantitatively. Accuracy of calibration is a key issue in driver assistance applications requiring 3D reasoning, and consequently in the safety of autonomous vehicles. Besides accuracy, other desired qualities in a calibration system are speed, robustness to varying imaging conditions, full automation, minimum restrictions in terms of assumptions such as overlapping field of view or information required such as an initial guess of the parameters.
- Modern systems are equipped with multiple sensors for different purposes. Geiger et al. (2012c) use a setup involving two cameras and a single range sensor such as Kinect or Velodyne laser scanner. They present two algorithms for camera-tocamera and camera-to-range calibration using a single image per sensor. They assume a common field of view for the sensors which is particularly useful for applications such as generating stereo or scene flow ground truth. Heng et al. (2013) and Heng et al. (2015) tackle the automatic intrinsic and extrinsic calibration of a multi-camera rig system with four fish-eye cameras and odometry without assuming overlapping fields of view. Heng et al. (2015) propose an improved version of Heng et al. (2013). While Geiger et al. (2012c) require fiducial markings to re-calibrate the system before every run, they remove the requirement to modify infrastructure by using a map and natural features instead. They first build a map of the calibration area and then perform calibration by using this map and image-based geo-localization. In contrast to SLAM-based selfcalibration methods, image-based localization removes the burden of exhaustive feature matching between dierent cameras and bundle adjustment.
多个传感器，包括测距仪，量程传感器以及不同类型的摄像机，如透视和鱼眼，广泛应用于汽车领域。校准是估计这些传感器的内在和外在参数以将2D图像点与3D世界点相关并且在多个传感器的情况下在公共坐标系中表示感测信息的问题。棋盘图案上的基准标记是校准的标准工具。几乎所有系统都使用它们进行初始化或联合优化来改进内在函数。作为投影点和测量点之间的像素距离的重新投影误差被用作定量测量精度的一种方式。校准的准确性是驾驶员辅助应用中的关键问题，需要3D推理，因此在自主车辆的安全性方面。除了准确性之外，校准系统中的其他期望的质量是对变化的成像条件的速度，鲁棒性，全自动化，在诸如重叠视野或所需信息（诸如参数的初始猜测）的假设方面的最小限制。
- 现代系统配备多个传感器用于不同的目的。盖革等人（2012c）使用一个包含两个摄像头和单范围传感器（如Kinect或Velodyne激光扫描仪）的设置。它们使用每个传感器单个图像呈现两种相机摄像机和摄像机到距离校准的算法。它们假设传感器的共同视野，这对于诸如产生立体声或场景流场实况的应用特别有用。 Heng等（2013）和Heng等（2015）利用四个鱼眼摄像机和距离测量法来处理多摄像机钻机系统的自动内在和外在校准，而不考虑重叠的视野。 Heng等（2015）提出了Heng等人的改进版。 （2013年）。而Geiger等人（2012c）要求在每次运行之前重新校准系统的基准标记，它们通过使用地图和自然特征来消除修改基础设施的要求。他们首先构建校准区域的图，然后使用该地图和基于图像的地理定位进行校准。与基于SLAM的自动校准方法相比，基于图像的定位消除了不同相机与捆绑调整之间的穷举特征匹配的负担。

- 3.2. Omnidirectional Cameras
- A panoramic field of view is desirable in autonomous driving to gain maximum information about the surrounding area for safe navigation. An omnidirectional camera with a 360-degree field of view provides enhanced coverage by eliminating the need for more cameras or mechanically turnable cameras. There are dierent types of omnidirectional cameras with a visual field that covers a hemisphere or even approximately the entire sphere. Catadioptric cameras combine a standard camera with a shaped mirror, such as a parabolic, hyperbolic, or elliptical mirror while dioptric cameras use purely dioptric fisheye
lenses. Polydioptric cameras use multiple cameras with overlapping field of view to provide a full spherical field of view.
- One classification often used in the literature for omnidirectional cameras is based on the projection center: central and noncentral. In central cameras, the optical rays to the viewed objects intersect in a single point in 3D which is known as the single effective viewpoint property. This property allows the generation of geometrically correct perspective images from the images captured by omnidirectional cameras and consequently, application of epipolar geometry which holds for any central camera. Central catadioptric cameras are built by choosing the mirror shape and the distance between the camera and the mirror.
在自主驾驶中需要全景视野以获得关于周围区域的最大信息以用于安全导航。具有360度视野的全向摄像机可以通过消除对更多摄像机或机械可转换摄像机的需求而提供更高的覆盖范围。有不同类型的全方位摄像机，其视野覆盖半球或甚至大致整个球体。反折射相机将标准相机与成型镜相结合，例如抛物线，双曲线或椭圆镜，而折光相机使用纯粹的折射鱼眼
镜头。多折照相机使用多个具有重叠视野的相机，以提供完整的球面视野。
- 文献中常用于全向摄像机的一个分类是基于投影中心：中央和非中央。在中央相机中，到所观看的物体的光线在3D中的单个点相交，这被称为单个有效视点属性。该属性允许从由全向照相机拍摄的图像产生几何正确的透视图像，并因此产生适用于任何中央相机的对极几何形状。通过选择镜面形状和相机与镜子之间的距离来构建中央反折射相机。

- In contrast to pinhole cameras, calibration of omnidirectional cameras cannot be modeled by a linear projection due to very high distortion. The model should take into account the reflection of the mirror in the case of a catadioptric camera or the refraction caused by the lens in the case of a fisheye camera. Geyer & Daniilidis (2000) provide a unifying theory for all central catadioptric systems which is known as unified projection model in the literature and widely used by different calibration toolboxes (Mei & Rives (2007); Heng et al. (2013, 2015)). They prove that every projection, both standard perspective and
catadioptric using a hyperbolic, parabolic, or elliptical mirror, can be modeled with projective mappings from the sphere to a plane where projection center is on a sphere diameter and the plane perpendicular to it. Scaramuzza & Martinelli (2006) propose modeling the imaging function by a Taylor series expansion whose degree and the coecients are the parameters to be estimated. Polynomials of order three or four are able to model accurately all catadioptric cameras and many types of fisheye cameras. Mei & Rives (2007) improve upon unified projection model of Geyer & Daniilidis (2000) to account for real-world errors by modeling distortions with well identified parameters.
- As desirable as it is, the single viewpoint property is often violated in practice due to varifocal lenses and difficulty of precise alignment. However, non-central models as the alternative are computationally demanding, hence not suitable for real-time applications. Sch¨onbein et al. (2014) extend a noncentral approach in order to accurately obtain the viewing ray orientations, and then propose a fast central approximation with a mapping to match the obtained orientations. This kind of approach, tested on hypercatadioptric cameras, achieves a reprojection error lower than the central models (Geyer & Daniilidis(2000); Scaramuzza & Martinelli (2006); Mei & Rives (2007)) and comparable to non-central models while being much faster.
- 与针孔相机不同，由于非常高的失真，全向摄像机的校准不能由线性投影建模。在反折射相机的情况下，该模型应考虑到镜子的反射，或者在鱼眼相机的情况下应考虑镜头引起的折射。 Geyer＆Daniilidis（2000）为所有中心反射折射系统提供了统一的理论，在文献中被称为统一投影模型，并被不同的校准工具箱广泛使用（Mei＆Rives（2007）; Heng等（2013，2015）） 。他们证明了每一个投影，既有标准的视角又有
使用双曲线，抛物面或椭圆镜的反射折射可以用从球体到投影中心在球体直径和垂直于其的平面的平面进行投影映射来建模。 Scaramuzza＆Martinelli（2006）提出了通过泰勒级数展开来建模成像函数，其程度和系数是要估计的参数。三阶或四阶的多项式能够准确地模拟所有反射折射相机和许多类型的鱼眼相机。 Mei＆Rives（2007）改进了Geyer＆Daniilidis（2000）的统一投影模型，通过建立具有良好识别参数的失真模型来解释现实世界的错误。
- 根据需要，由于变焦透镜和精确对准的困难，实际上常常违反单一视点特性。然而，非中心模型作为替代方案在计算上要求苛刻，因此不适合实时应用。 Sch¨onbein等（2014）扩展了非中心的方法，以便准确地获得观察射线取向，然后提出一个具有映射的快速中心近似以匹配获得的方向。这种在过度反射摄影机上进行测试的方法实现了比中央模型（Geyer＆Daniilidis（2000）; Scaramuzza＆Martinelli（2006）; Mei＆Rives（2007））更低的重现误差，并且与非中心模型相当，快多了。

- **Applications**: Omnidirectional cameras are more and more used in autonomous driving. For feature based applications such as navigation, motion estimation and mapping, large field of view enables extraction and matching of interesting points from all around the car. For instance, omnidirectional feature matches improve the rotation estimate significantly when doing visual odometry or simultaneous localization and mapping(SLAM). Scaramuzza&Siegwart (2008) estimate the ego-motion of the vehicle relative to the road from a single, central omnidirectional camera by using a homography based tracker for the ground plane and an appearance-based tracker for the rotation of the vehicle. 3D perception also benefits from the unified view offered by omnidirectional sensors, despite the limited effective resolution which leads to noisy reconstructions. Laserbased solutions as an alternative provide only sparse point clouds without color, are extremely expensive and suffer from rolling shutter effects. Sch¨onbein & Geiger (2014) propose a method for 3D reconstruction through joint optimization of disparity estimates from two temporally and two spatially adjacent omnidirectional view in a unified omnidirectional space by using plane-based priors. H¨ane et al. (2014) extend the planesweeping stereo matching for fisheye cameras by incorporating unified projection model for fisheye cameras directly into the plane-sweeping stereo matching algorithm. This kind of approach allows producing dense depth maps directly from fisheye images in real time using GPUs and opens the way for dense 3D reconstruction with large a field of view in real time.
- 应用：全自动摄像机越来越多地用于自主驾驶。对于基于功能的应用程序，如导航，运动估计和映射，大视野可以提取和匹配来自汽车周围的有趣点。例如，全方位特征匹配在进行视觉测距或同时定位和映射（SLAM）时会显着改善旋转估计。 Scaramuzza＆Siegwart（2008）通过使用用于地平面的基于单应性的跟踪器和用于车辆旋转的基于外观的跟踪器来估计来自单个中央全向照相机的车辆相对于道路的自主运动。尽管有限分辨率导致噪声重建，3D感知也受益于全向传感器提供的统一视图。作为替代的基于激光的解决方案仅提供没有颜色的稀疏云，是非常昂贵的并且受到滚动快门效应的影响。 Sch¨onbein＆Geiger（2014）提出了一种通过使用基于平面的先验在统一的全向空间中从两个时间和两个空间相邻的全向视差联合优化差异估计的三维重建方法。 H¨ane等（2014年）通过将鱼眼摄影机的统一投影模型直接纳入平扫立体匹配算法，扩展了鱼眼摄像机的扫描立体匹配。这种方法允许使用GPU实时直接从鱼眼图像生成密集的深度图，并且以实时的大视野打开密集3D重建的方式。

- 3.3. Event Cameras 活动相机
- Contrary to conventional frame-based imagers at constant frame rates, event-based sensors have very recently been introduced. They produce a stream of asynchronous events at microsecond resolution in case of a brightness change surpassing a pre-defined threshold (Dynamic Vision Sensor) as shown in Figure 4. An event contains the location, sign, and precise timestamp of the change. This kind of data is sparse in nature, thus reducing redundancy in transmission and processing. Another advantage is high temporal resolution, allowing the design of highly reactive systems. These properties, namely low latency and low bandwidth requirement make event-based sensors interesting for autonomous driving. However, standard computer-vision algorithms cannot be applied directly to the output of event-based vision sensors which is fundamentally different from intensity images. Events occur at high frequency and each event doesn’t carry enough information by itself. A straightforward solution is to generate intensity images by accumulating
events over a fixed time interval, but this kind of event to-frame conversion introduces some latency and obstructs the efficiency which comes with the high temporal resolution.
- Instead, algorithms should ideally exploit the high rate at which events are generated. Consequently several methods have recently been introduced which exploit the high temporal resolution and the asynchronous nature of the sensor for different problems in autonomous vision. The design goal of such algorithms is that each incoming event can asynchronously change the estimated state, thus respecting the event-based nature of the sensor and allowing for perception and state estimation in highly dynamic scenarios. For trajectory estimation, Mueggler et al. (2015b) propose a continuous temporal model as a natural representation of the pose trajectory described by a smooth parametric model. Rebecq et al. (2016) propose an event-based 3D reconstruction algorithm to produce a parallel tracking and mapping pipeline that runs in real-time on the CPU. Eventbased SLAM does not suffer from motion blur due to high speed motions and very high dynamic range scenes which can be challenging for standard camera approaches.
- Lifetime Estimation: In addition to enabling novel solutions for existing problems where low latency and high frame rates are required, event-based sensors also give rise to new problems. One such problem is lifetime estimation of events by modeling the set of active events. An event is considered active as long as the brightness gradient causing the event is visible by the pixel. Explicit modeling of active events can be used to generate sharp gradient images at any point in time, or for clustering of events in tracking of multiple objects. For this task, Mueggler et al. (2015a) propose using event-based optical flow with optional regularization, independent of a temporal window.
- 与传统的基于帧的成像器在恒定帧速率相反，基于事件的传感器最近已经被引入。在亮度变化超过预定阈值（动态视觉传感器）的情况下，它们以微秒分辨率产生异步事件流，如图4所示。事件包含更改的位置，符号和精确时间戳。这种数据本质上是稀疏的，从而减少传输和处理的冗余。另一个优点是高时间分辨率，允许设计高反应性系统。这些属性，即低延迟和低带宽需求使得基于事件的传感器对于自主驾驶是有意义的。然而，标准计算机视觉算法不能直接应用于与强度图像基本不同的基于事件的视觉传感器的输出。事件发生在高频率，每个事件本身不携带足够的信息。一个简单的解决方案是通过积累产生强度图像
事件在固定的时间间隔内，但这种事件帧间转换引入了一些延迟并阻碍了高时间分辨率带来的效率。
相反，算法应该理想地利用生成事件的高速率。因此，最近已经引入了几种方法，其利用自主视觉中的不同问题的传感器的高时间分辨率和异步性质。这种算法的设计目标是每个传入事件可以异步地改变估计状态，从而满足传感器的基于事件的性质，并允许在高度动态的情况下的感知和状态估计。对于轨迹估计，Mueggler et al。 （2015b）提出了一种连续时间模型作为由平滑参数模型描述的姿势轨迹的自然表示。 Rebecq等（2016）提出了一种基于事件的3D重建算法，以产生在CPU上实时运行的并行跟踪和映射管道。基于事件的SLAM由于高速运动和非常高的动态范围场景而不受运动模糊的影响，这对于标准相机方法来说可能是具有挑战性的。
- 寿命估算：除了为需要低延迟和高帧速率的现有问题提供新颖的解决方案外，基于事件的传感器也会引起新的问题。一个这样的问题是通过对一组活动事件建模来对事件进行寿命估计。只要导致事件的亮度梯度被像素看到，事件被认为是活动的。活动事件的显式建模可用于在任何时间点生成锐利梯度图像，或用于在跟踪多个对象时对事件进行聚类。对于这个任务，Mueggler等人（2015a）提出了使用基于事件的光流与可选正则化，独立于时间窗口。
