## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
è‡ªåŠ¨é©¾é©¶æŠ€æœ¯çš„è®¡ç®—æœºè§†è§‰ï¼šé—®é¢˜ï¼Œæ•°æ®å’Œå‰æ²¿æŠ€æœ¯

### 3. Cameras Models & Calibration æ‘„åƒå¤´æ¨¡å—&æ ¡å‡†
- 3.1. Calibration æ ¡å‡†
- Multiple sensors including odometry, range sensors, and different types of cameras such as perspective and fish-eye are widely used in automotive context. Calibration is the problem of estimating intrinsic and extrinsic parameters of these sensors to relate 2D image points to 3D world points and represent sensed information in a common coordinate system in case of multiple sensors. Fiducial markings on checkerboard patterns are the standard tool for calibration. Almost all systems use them either for initialization or for joint optimization to improve the intrinsics. Reprojection error which is the pixel distance between a projected point and a measured one, is used as a way of measuring accuracy quantitatively. Accuracy of calibration is a key issue in driver assistance applications requiring 3D reasoning, and consequently in the safety of autonomous vehicles. Besides accuracy, other desired qualities in a calibration system are speed, robustness to varying imaging conditions, full automation, minimum restrictions in terms of assumptions such as overlapping field of view or information required such as an initial guess of the parameters.
- Modern systems are equipped with multiple sensors for different purposes. Geiger et al. (2012c) use a setup involving two cameras and a single range sensor such as Kinect or Velodyne laser scanner. They present two algorithms for camera-tocamera and camera-to-range calibration using a single image per sensor. They assume a common field of view for the sensors which is particularly useful for applications such as generating stereo or scene flow ground truth. Heng et al. (2013) and Heng et al. (2015) tackle the automatic intrinsic and extrinsic calibration of a multi-camera rig system with four fish-eye cameras and odometry without assuming overlapping fields of view. Heng et al. (2015) propose an improved version of Heng et al. (2013). While Geiger et al. (2012c) require fiducial markings to re-calibrate the system before every run, they remove the requirement to modify infrastructure by using a map and natural features instead. They first build a map of the calibration area and then perform calibration by using this map and image-based geo-localization. In contrast to SLAM-based selfcalibration methods, image-based localization removes the burden of exhaustive feature matching between dierent cameras and bundle adjustment.
å¤šä¸ªä¼ æ„Ÿå™¨ï¼ŒåŒ…æ‹¬æµ‹è·ä»ªï¼Œé‡ç¨‹ä¼ æ„Ÿå™¨ä»¥åŠä¸åŒç±»å‹çš„æ‘„åƒæœºï¼Œå¦‚é€è§†å’Œé±¼çœ¼ï¼Œå¹¿æ³›åº”ç”¨äºæ±½è½¦é¢†åŸŸã€‚æ ¡å‡†æ˜¯ä¼°è®¡è¿™äº›ä¼ æ„Ÿå™¨çš„å†…åœ¨å’Œå¤–åœ¨å‚æ•°ä»¥å°†2Då›¾åƒç‚¹ä¸3Dä¸–ç•Œç‚¹ç›¸å…³å¹¶ä¸”åœ¨å¤šä¸ªä¼ æ„Ÿå™¨çš„æƒ…å†µä¸‹åœ¨å…¬å…±åæ ‡ç³»ä¸­è¡¨ç¤ºæ„Ÿæµ‹ä¿¡æ¯çš„é—®é¢˜ã€‚æ£‹ç›˜å›¾æ¡ˆä¸Šçš„åŸºå‡†æ ‡è®°æ˜¯æ ¡å‡†çš„æ ‡å‡†å·¥å…·ã€‚å‡ ä¹æ‰€æœ‰ç³»ç»Ÿéƒ½ä½¿ç”¨å®ƒä»¬è¿›è¡Œåˆå§‹åŒ–æˆ–è”åˆä¼˜åŒ–æ¥æ”¹è¿›å†…åœ¨å‡½æ•°ã€‚ä½œä¸ºæŠ•å½±ç‚¹å’Œæµ‹é‡ç‚¹ä¹‹é—´çš„åƒç´ è·ç¦»çš„é‡æ–°æŠ•å½±è¯¯å·®è¢«ç”¨ä½œå®šé‡æµ‹é‡ç²¾åº¦çš„ä¸€ç§æ–¹å¼ã€‚æ ¡å‡†çš„å‡†ç¡®æ€§æ˜¯é©¾é©¶å‘˜è¾…åŠ©åº”ç”¨ä¸­çš„å…³é”®é—®é¢˜ï¼Œéœ€è¦3Dæ¨ç†ï¼Œå› æ­¤åœ¨è‡ªä¸»è½¦è¾†çš„å®‰å…¨æ€§æ–¹é¢ã€‚é™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼Œæ ¡å‡†ç³»ç»Ÿä¸­çš„å…¶ä»–æœŸæœ›çš„è´¨é‡æ˜¯å¯¹å˜åŒ–çš„æˆåƒæ¡ä»¶çš„é€Ÿåº¦ï¼Œé²æ£’æ€§ï¼Œå…¨è‡ªåŠ¨åŒ–ï¼Œåœ¨è¯¸å¦‚é‡å è§†é‡æˆ–æ‰€éœ€ä¿¡æ¯ï¼ˆè¯¸å¦‚å‚æ•°çš„åˆå§‹çŒœæµ‹ï¼‰çš„å‡è®¾æ–¹é¢çš„æœ€å°é™åˆ¶ã€‚
- ç°ä»£ç³»ç»Ÿé…å¤‡å¤šä¸ªä¼ æ„Ÿå™¨ç”¨äºä¸åŒçš„ç›®çš„ã€‚ç›–é©ç­‰äººï¼ˆ2012cï¼‰ä½¿ç”¨ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ‘„åƒå¤´å’Œå•èŒƒå›´ä¼ æ„Ÿå™¨ï¼ˆå¦‚Kinectæˆ–Velodyneæ¿€å…‰æ‰«æä»ªï¼‰çš„è®¾ç½®ã€‚å®ƒä»¬ä½¿ç”¨æ¯ä¸ªä¼ æ„Ÿå™¨å•ä¸ªå›¾åƒå‘ˆç°ä¸¤ç§ç›¸æœºæ‘„åƒæœºå’Œæ‘„åƒæœºåˆ°è·ç¦»æ ¡å‡†çš„ç®—æ³•ã€‚å®ƒä»¬å‡è®¾ä¼ æ„Ÿå™¨çš„å…±åŒè§†é‡ï¼Œè¿™å¯¹äºè¯¸å¦‚äº§ç”Ÿç«‹ä½“å£°æˆ–åœºæ™¯æµåœºå®å†µçš„åº”ç”¨ç‰¹åˆ«æœ‰ç”¨ã€‚ Hengç­‰ï¼ˆ2013ï¼‰å’ŒHengç­‰ï¼ˆ2015ï¼‰åˆ©ç”¨å››ä¸ªé±¼çœ¼æ‘„åƒæœºå’Œè·ç¦»æµ‹é‡æ³•æ¥å¤„ç†å¤šæ‘„åƒæœºé’»æœºç³»ç»Ÿçš„è‡ªåŠ¨å†…åœ¨å’Œå¤–åœ¨æ ¡å‡†ï¼Œè€Œä¸è€ƒè™‘é‡å çš„è§†é‡ã€‚ Hengç­‰ï¼ˆ2015ï¼‰æå‡ºäº†Hengç­‰äººçš„æ”¹è¿›ç‰ˆã€‚ ï¼ˆ2013å¹´ï¼‰ã€‚è€ŒGeigerç­‰äººï¼ˆ2012cï¼‰è¦æ±‚åœ¨æ¯æ¬¡è¿è¡Œä¹‹å‰é‡æ–°æ ¡å‡†ç³»ç»Ÿçš„åŸºå‡†æ ‡è®°ï¼Œå®ƒä»¬é€šè¿‡ä½¿ç”¨åœ°å›¾å’Œè‡ªç„¶ç‰¹å¾æ¥æ¶ˆé™¤ä¿®æ”¹åŸºç¡€è®¾æ–½çš„è¦æ±‚ã€‚ä»–ä»¬é¦–å…ˆæ„å»ºæ ¡å‡†åŒºåŸŸçš„å›¾ï¼Œç„¶åä½¿ç”¨è¯¥åœ°å›¾å’ŒåŸºäºå›¾åƒçš„åœ°ç†å®šä½è¿›è¡Œæ ¡å‡†ã€‚ä¸åŸºäºSLAMçš„è‡ªåŠ¨æ ¡å‡†æ–¹æ³•ç›¸æ¯”ï¼ŒåŸºäºå›¾åƒçš„å®šä½æ¶ˆé™¤äº†ä¸åŒç›¸æœºä¸æ†ç»‘è°ƒæ•´ä¹‹é—´çš„ç©·ä¸¾ç‰¹å¾åŒ¹é…çš„è´Ÿæ‹…ã€‚

- 3.2. Omnidirectional Cameras
- A panoramic field of view is desirable in autonomous driving to gain maximum information about the surrounding area for safe navigation. An omnidirectional camera with a 360-degree field of view provides enhanced coverage by eliminating the need for more cameras or mechanically turnable cameras. There are dierent types of omnidirectional cameras with a visual field that covers a hemisphere or even approximately the entire sphere. Catadioptric cameras combine a standard camera with a shaped mirror, such as a parabolic, hyperbolic, or elliptical mirror while dioptric cameras use purely dioptric fisheye
lenses. Polydioptric cameras use multiple cameras with overlapping field of view to provide a full spherical field of view.
- One classification often used in the literature for omnidirectional cameras is based on the projection center: central and noncentral. In central cameras, the optical rays to the viewed objects intersect in a single point in 3D which is known as the single effective viewpoint property. This property allows the generation of geometrically correct perspective images from the images captured by omnidirectional cameras and consequently, application of epipolar geometry which holds for any central camera. Central catadioptric cameras are built by choosing the mirror shape and the distance between the camera and the mirror.
åœ¨è‡ªä¸»é©¾é©¶ä¸­éœ€è¦å…¨æ™¯è§†é‡ä»¥è·å¾—å…³äºå‘¨å›´åŒºåŸŸçš„æœ€å¤§ä¿¡æ¯ä»¥ç”¨äºå®‰å…¨å¯¼èˆªã€‚å…·æœ‰360åº¦è§†é‡çš„å…¨å‘æ‘„åƒæœºå¯ä»¥é€šè¿‡æ¶ˆé™¤å¯¹æ›´å¤šæ‘„åƒæœºæˆ–æœºæ¢°å¯è½¬æ¢æ‘„åƒæœºçš„éœ€æ±‚è€Œæä¾›æ›´é«˜çš„è¦†ç›–èŒƒå›´ã€‚æœ‰ä¸åŒç±»å‹çš„å…¨æ–¹ä½æ‘„åƒæœºï¼Œå…¶è§†é‡è¦†ç›–åŠçƒæˆ–ç”šè‡³å¤§è‡´æ•´ä¸ªçƒä½“ã€‚åæŠ˜å°„ç›¸æœºå°†æ ‡å‡†ç›¸æœºä¸æˆå‹é•œç›¸ç»“åˆï¼Œä¾‹å¦‚æŠ›ç‰©çº¿ï¼ŒåŒæ›²çº¿æˆ–æ¤­åœ†é•œï¼Œè€ŒæŠ˜å…‰ç›¸æœºä½¿ç”¨çº¯ç²¹çš„æŠ˜å°„é±¼çœ¼
é•œå¤´ã€‚å¤šæŠ˜ç…§ç›¸æœºä½¿ç”¨å¤šä¸ªå…·æœ‰é‡å è§†é‡çš„ç›¸æœºï¼Œä»¥æä¾›å®Œæ•´çš„çƒé¢è§†é‡ã€‚
- æ–‡çŒ®ä¸­å¸¸ç”¨äºå…¨å‘æ‘„åƒæœºçš„ä¸€ä¸ªåˆ†ç±»æ˜¯åŸºäºæŠ•å½±ä¸­å¿ƒï¼šä¸­å¤®å’Œéä¸­å¤®ã€‚åœ¨ä¸­å¤®ç›¸æœºä¸­ï¼Œåˆ°æ‰€è§‚çœ‹çš„ç‰©ä½“çš„å…‰çº¿åœ¨3Dä¸­çš„å•ä¸ªç‚¹ç›¸äº¤ï¼Œè¿™è¢«ç§°ä¸ºå•ä¸ªæœ‰æ•ˆè§†ç‚¹å±æ€§ã€‚è¯¥å±æ€§å…è®¸ä»ç”±å…¨å‘ç…§ç›¸æœºæ‹æ‘„çš„å›¾åƒäº§ç”Ÿå‡ ä½•æ­£ç¡®çš„é€è§†å›¾åƒï¼Œå¹¶å› æ­¤äº§ç”Ÿé€‚ç”¨äºä»»ä½•ä¸­å¤®ç›¸æœºçš„å¯¹æå‡ ä½•å½¢çŠ¶ã€‚é€šè¿‡é€‰æ‹©é•œé¢å½¢çŠ¶å’Œç›¸æœºä¸é•œå­ä¹‹é—´çš„è·ç¦»æ¥æ„å»ºä¸­å¤®åæŠ˜å°„ç›¸æœºã€‚

- In contrast to pinhole cameras, calibration of omnidirectional cameras cannot be modeled by a linear projection due to very high distortion. The model should take into account the reflection of the mirror in the case of a catadioptric camera or the refraction caused by the lens in the case of a fisheye camera. Geyer & Daniilidis (2000) provide a unifying theory for all central catadioptric systems which is known as unified projection model in the literature and widely used by different calibration toolboxes (Mei & Rives (2007); Heng et al. (2013, 2015)). They prove that every projection, both standard perspective and
catadioptric using a hyperbolic, parabolic, or elliptical mirror, can be modeled with projective mappings from the sphere to a plane where projection center is on a sphere diameter and the plane perpendicular to it. Scaramuzza & Martinelli (2006) propose modeling the imaging function by a Taylor series expansion whose degree and the coecients are the parameters to be estimated. Polynomials of order three or four are able to model accurately all catadioptric cameras and many types of fisheye cameras. Mei & Rives (2007) improve upon unified projection model of Geyer & Daniilidis (2000) to account for real-world errors by modeling distortions with well identified parameters.
- As desirable as it is, the single viewpoint property is often violated in practice due to varifocal lenses and difficulty of precise alignment. However, non-central models as the alternative are computationally demanding, hence not suitable for real-time applications. SchÂ¨onbein et al. (2014) extend a noncentral approach in order to accurately obtain the viewing ray orientations, and then propose a fast central approximation with a mapping to match the obtained orientations. This kind of approach, tested on hypercatadioptric cameras, achieves a reprojection error lower than the central models (Geyer & Daniilidis(2000); Scaramuzza & Martinelli (2006); Mei & Rives (2007)) and comparable to non-central models while being much faster.
- ä¸é’ˆå­”ç›¸æœºä¸åŒï¼Œç”±äºéå¸¸é«˜çš„å¤±çœŸï¼Œå…¨å‘æ‘„åƒæœºçš„æ ¡å‡†ä¸èƒ½ç”±çº¿æ€§æŠ•å½±å»ºæ¨¡ã€‚åœ¨åæŠ˜å°„ç›¸æœºçš„æƒ…å†µä¸‹ï¼Œè¯¥æ¨¡å‹åº”è€ƒè™‘åˆ°é•œå­çš„åå°„ï¼Œæˆ–è€…åœ¨é±¼çœ¼ç›¸æœºçš„æƒ…å†µä¸‹åº”è€ƒè™‘é•œå¤´å¼•èµ·çš„æŠ˜å°„ã€‚ Geyerï¼†Daniilidisï¼ˆ2000ï¼‰ä¸ºæ‰€æœ‰ä¸­å¿ƒåå°„æŠ˜å°„ç³»ç»Ÿæä¾›äº†ç»Ÿä¸€çš„ç†è®ºï¼Œåœ¨æ–‡çŒ®ä¸­è¢«ç§°ä¸ºç»Ÿä¸€æŠ•å½±æ¨¡å‹ï¼Œå¹¶è¢«ä¸åŒçš„æ ¡å‡†å·¥å…·ç®±å¹¿æ³›ä½¿ç”¨ï¼ˆMeiï¼†Rivesï¼ˆ2007ï¼‰; Hengç­‰ï¼ˆ2013ï¼Œ2015ï¼‰ï¼‰ ã€‚ä»–ä»¬è¯æ˜äº†æ¯ä¸€ä¸ªæŠ•å½±ï¼Œæ—¢æœ‰æ ‡å‡†çš„è§†è§’åˆæœ‰
ä½¿ç”¨åŒæ›²çº¿ï¼ŒæŠ›ç‰©é¢æˆ–æ¤­åœ†é•œçš„åå°„æŠ˜å°„å¯ä»¥ç”¨ä»çƒä½“åˆ°æŠ•å½±ä¸­å¿ƒåœ¨çƒä½“ç›´å¾„å’Œå‚ç›´äºå…¶çš„å¹³é¢çš„å¹³é¢è¿›è¡ŒæŠ•å½±æ˜ å°„æ¥å»ºæ¨¡ã€‚ Scaramuzzaï¼†Martinelliï¼ˆ2006ï¼‰æå‡ºäº†é€šè¿‡æ³°å‹’çº§æ•°å±•å¼€æ¥å»ºæ¨¡æˆåƒå‡½æ•°ï¼Œå…¶ç¨‹åº¦å’Œç³»æ•°æ˜¯è¦ä¼°è®¡çš„å‚æ•°ã€‚ä¸‰é˜¶æˆ–å››é˜¶çš„å¤šé¡¹å¼èƒ½å¤Ÿå‡†ç¡®åœ°æ¨¡æ‹Ÿæ‰€æœ‰åå°„æŠ˜å°„ç›¸æœºå’Œè®¸å¤šç±»å‹çš„é±¼çœ¼ç›¸æœºã€‚ Meiï¼†Rivesï¼ˆ2007ï¼‰æ”¹è¿›äº†Geyerï¼†Daniilidisï¼ˆ2000ï¼‰çš„ç»Ÿä¸€æŠ•å½±æ¨¡å‹ï¼Œé€šè¿‡å»ºç«‹å…·æœ‰è‰¯å¥½è¯†åˆ«å‚æ•°çš„å¤±çœŸæ¨¡å‹æ¥è§£é‡Šç°å®ä¸–ç•Œçš„é”™è¯¯ã€‚
- æ ¹æ®éœ€è¦ï¼Œç”±äºå˜ç„¦é€é•œå’Œç²¾ç¡®å¯¹å‡†çš„å›°éš¾ï¼Œå®é™…ä¸Šå¸¸å¸¸è¿åå•ä¸€è§†ç‚¹ç‰¹æ€§ã€‚ç„¶è€Œï¼Œéä¸­å¿ƒæ¨¡å‹ä½œä¸ºæ›¿ä»£æ–¹æ¡ˆåœ¨è®¡ç®—ä¸Šè¦æ±‚è‹›åˆ»ï¼Œå› æ­¤ä¸é€‚åˆå®æ—¶åº”ç”¨ã€‚ SchÂ¨onbeinç­‰ï¼ˆ2014ï¼‰æ‰©å±•äº†éä¸­å¿ƒçš„æ–¹æ³•ï¼Œä»¥ä¾¿å‡†ç¡®åœ°è·å¾—è§‚å¯Ÿå°„çº¿å–å‘ï¼Œç„¶åæå‡ºä¸€ä¸ªå…·æœ‰æ˜ å°„çš„å¿«é€Ÿä¸­å¿ƒè¿‘ä¼¼ä»¥åŒ¹é…è·å¾—çš„æ–¹å‘ã€‚è¿™ç§åœ¨è¿‡åº¦åå°„æ‘„å½±æœºä¸Šè¿›è¡Œæµ‹è¯•çš„æ–¹æ³•å®ç°äº†æ¯”ä¸­å¤®æ¨¡å‹ï¼ˆGeyerï¼†Daniilidisï¼ˆ2000ï¼‰; Scaramuzzaï¼†Martinelliï¼ˆ2006ï¼‰; Meiï¼†Rivesï¼ˆ2007ï¼‰ï¼‰æ›´ä½çš„é‡ç°è¯¯å·®ï¼Œå¹¶ä¸”ä¸éä¸­å¿ƒæ¨¡å‹ç›¸å½“ï¼Œå¿«å¤šäº†ã€‚

- **Applications**: Omnidirectional cameras are more and more used in autonomous driving. For feature based applications such as navigation, motion estimation and mapping, large field of view enables extraction and matching of interesting points from all around the car. For instance, omnidirectional feature matches improve the rotation estimate significantly when doing visual odometry or simultaneous localization and mapping(SLAM). Scaramuzza&Siegwart (2008) estimate the ego-motion of the vehicle relative to the road from a single, central omnidirectional camera by using a homography based tracker for the ground plane and an appearance-based tracker for the rotation of the vehicle. 3D perception also benefits from the unified view offered by omnidirectional sensors, despite the limited effective resolution which leads to noisy reconstructions. Laserbased solutions as an alternative provide only sparse point clouds without color, are extremely expensive and suffer from rolling shutter effects. SchÂ¨onbein & Geiger (2014) propose a method for 3D reconstruction through joint optimization of disparity estimates from two temporally and two spatially adjacent omnidirectional view in a unified omnidirectional space by using plane-based priors. HÂ¨ane et al. (2014) extend the planesweeping stereo matching for fisheye cameras by incorporating unified projection model for fisheye cameras directly into the plane-sweeping stereo matching algorithm. This kind of approach allows producing dense depth maps directly from fisheye images in real time using GPUs and opens the way for dense 3D reconstruction with large a field of view in real time.
- åº”ç”¨ï¼šå…¨è‡ªåŠ¨æ‘„åƒæœºè¶Šæ¥è¶Šå¤šåœ°ç”¨äºè‡ªä¸»é©¾é©¶ã€‚å¯¹äºåŸºäºåŠŸèƒ½çš„åº”ç”¨ç¨‹åºï¼Œå¦‚å¯¼èˆªï¼Œè¿åŠ¨ä¼°è®¡å’Œæ˜ å°„ï¼Œå¤§è§†é‡å¯ä»¥æå–å’ŒåŒ¹é…æ¥è‡ªæ±½è½¦å‘¨å›´çš„æœ‰è¶£ç‚¹ã€‚ä¾‹å¦‚ï¼Œå…¨æ–¹ä½ç‰¹å¾åŒ¹é…åœ¨è¿›è¡Œè§†è§‰æµ‹è·æˆ–åŒæ—¶å®šä½å’Œæ˜ å°„ï¼ˆSLAMï¼‰æ—¶ä¼šæ˜¾ç€æ”¹å–„æ—‹è½¬ä¼°è®¡ã€‚ Scaramuzzaï¼†Siegwartï¼ˆ2008ï¼‰é€šè¿‡ä½¿ç”¨ç”¨äºåœ°å¹³é¢çš„åŸºäºå•åº”æ€§çš„è·Ÿè¸ªå™¨å’Œç”¨äºè½¦è¾†æ—‹è½¬çš„åŸºäºå¤–è§‚çš„è·Ÿè¸ªå™¨æ¥ä¼°è®¡æ¥è‡ªå•ä¸ªä¸­å¤®å…¨å‘ç…§ç›¸æœºçš„è½¦è¾†ç›¸å¯¹äºé“è·¯çš„è‡ªä¸»è¿åŠ¨ã€‚å°½ç®¡æœ‰é™åˆ†è¾¨ç‡å¯¼è‡´å™ªå£°é‡å»ºï¼Œ3Dæ„ŸçŸ¥ä¹Ÿå—ç›Šäºå…¨å‘ä¼ æ„Ÿå™¨æä¾›çš„ç»Ÿä¸€è§†å›¾ã€‚ä½œä¸ºæ›¿ä»£çš„åŸºäºæ¿€å…‰çš„è§£å†³æ–¹æ¡ˆä»…æä¾›æ²¡æœ‰é¢œè‰²çš„ç¨€ç–äº‘ï¼Œæ˜¯éå¸¸æ˜‚è´µçš„å¹¶ä¸”å—åˆ°æ»šåŠ¨å¿«é—¨æ•ˆåº”çš„å½±å“ã€‚ SchÂ¨onbeinï¼†Geigerï¼ˆ2014ï¼‰æå‡ºäº†ä¸€ç§é€šè¿‡ä½¿ç”¨åŸºäºå¹³é¢çš„å…ˆéªŒåœ¨ç»Ÿä¸€çš„å…¨å‘ç©ºé—´ä¸­ä»ä¸¤ä¸ªæ—¶é—´å’Œä¸¤ä¸ªç©ºé—´ç›¸é‚»çš„å…¨å‘è§†å·®è”åˆä¼˜åŒ–å·®å¼‚ä¼°è®¡çš„ä¸‰ç»´é‡å»ºæ–¹æ³•ã€‚ HÂ¨aneç­‰ï¼ˆ2014å¹´ï¼‰é€šè¿‡å°†é±¼çœ¼æ‘„å½±æœºçš„ç»Ÿä¸€æŠ•å½±æ¨¡å‹ç›´æ¥çº³å…¥å¹³æ‰«ç«‹ä½“åŒ¹é…ç®—æ³•ï¼Œæ‰©å±•äº†é±¼çœ¼æ‘„åƒæœºçš„æ‰«æç«‹ä½“åŒ¹é…ã€‚è¿™ç§æ–¹æ³•å…è®¸ä½¿ç”¨GPUå®æ—¶ç›´æ¥ä»é±¼çœ¼å›¾åƒç”Ÿæˆå¯†é›†çš„æ·±åº¦å›¾ï¼Œå¹¶ä¸”ä»¥å®æ—¶çš„å¤§è§†é‡æ‰“å¼€å¯†é›†3Dé‡å»ºçš„æ–¹å¼ã€‚

- 3.3. Event Cameras æ´»åŠ¨ç›¸æœº
- Contrary to conventional frame-based imagers at constant frame rates, event-based sensors have very recently been introduced. They produce a stream of asynchronous events at microsecond resolution in case of a brightness change surpassing a pre-defined threshold (Dynamic Vision Sensor) as shown in Figure 4. An event contains the location, sign, and precise timestamp of the change. This kind of data is sparse in nature, thus reducing redundancy in transmission and processing. Another advantage is high temporal resolution, allowing the design of highly reactive systems. These properties, namely low latency and low bandwidth requirement make event-based sensors interesting for autonomous driving. However, standard computer-vision algorithms cannot be applied directly to the output of event-based vision sensors which is fundamentally different from intensity images. Events occur at high frequency and each event doesnâ€™t carry enough information by itself. A straightforward solution is to generate intensity images by accumulating
events over a fixed time interval, but this kind of event to-frame conversion introduces some latency and obstructs the efficiency which comes with the high temporal resolution.
- Instead, algorithms should ideally exploit the high rate at which events are generated. Consequently several methods have recently been introduced which exploit the high temporal resolution and the asynchronous nature of the sensor for different problems in autonomous vision. The design goal of such algorithms is that each incoming event can asynchronously change the estimated state, thus respecting the event-based nature of the sensor and allowing for perception and state estimation in highly dynamic scenarios. For trajectory estimation, Mueggler et al. (2015b) propose a continuous temporal model as a natural representation of the pose trajectory described by a smooth parametric model. Rebecq et al. (2016) propose an event-based 3D reconstruction algorithm to produce a parallel tracking and mapping pipeline that runs in real-time on the CPU. Eventbased SLAM does not suffer from motion blur due to high speed motions and very high dynamic range scenes which can be challenging for standard camera approaches.
- Lifetime Estimation: In addition to enabling novel solutions for existing problems where low latency and high frame rates are required, event-based sensors also give rise to new problems. One such problem is lifetime estimation of events by modeling the set of active events. An event is considered active as long as the brightness gradient causing the event is visible by the pixel. Explicit modeling of active events can be used to generate sharp gradient images at any point in time, or for clustering of events in tracking of multiple objects. For this task, Mueggler et al. (2015a) propose using event-based optical flow with optional regularization, independent of a temporal window.
- ä¸ä¼ ç»Ÿçš„åŸºäºå¸§çš„æˆåƒå™¨åœ¨æ’å®šå¸§é€Ÿç‡ç›¸åï¼ŒåŸºäºäº‹ä»¶çš„ä¼ æ„Ÿå™¨æœ€è¿‘å·²ç»è¢«å¼•å…¥ã€‚åœ¨äº®åº¦å˜åŒ–è¶…è¿‡é¢„å®šé˜ˆå€¼ï¼ˆåŠ¨æ€è§†è§‰ä¼ æ„Ÿå™¨ï¼‰çš„æƒ…å†µä¸‹ï¼Œå®ƒä»¬ä»¥å¾®ç§’åˆ†è¾¨ç‡äº§ç”Ÿå¼‚æ­¥äº‹ä»¶æµï¼Œå¦‚å›¾4æ‰€ç¤ºã€‚äº‹ä»¶åŒ…å«æ›´æ”¹çš„ä½ç½®ï¼Œç¬¦å·å’Œç²¾ç¡®æ—¶é—´æˆ³ã€‚è¿™ç§æ•°æ®æœ¬è´¨ä¸Šæ˜¯ç¨€ç–çš„ï¼Œä»è€Œå‡å°‘ä¼ è¾“å’Œå¤„ç†çš„å†—ä½™ã€‚å¦ä¸€ä¸ªä¼˜ç‚¹æ˜¯é«˜æ—¶é—´åˆ†è¾¨ç‡ï¼Œå…è®¸è®¾è®¡é«˜ååº”æ€§ç³»ç»Ÿã€‚è¿™äº›å±æ€§ï¼Œå³ä½å»¶è¿Ÿå’Œä½å¸¦å®½éœ€æ±‚ä½¿å¾—åŸºäºäº‹ä»¶çš„ä¼ æ„Ÿå™¨å¯¹äºè‡ªä¸»é©¾é©¶æ˜¯æœ‰æ„ä¹‰çš„ã€‚ç„¶è€Œï¼Œæ ‡å‡†è®¡ç®—æœºè§†è§‰ç®—æ³•ä¸èƒ½ç›´æ¥åº”ç”¨äºä¸å¼ºåº¦å›¾åƒåŸºæœ¬ä¸åŒçš„åŸºäºäº‹ä»¶çš„è§†è§‰ä¼ æ„Ÿå™¨çš„è¾“å‡ºã€‚äº‹ä»¶å‘ç”Ÿåœ¨é«˜é¢‘ç‡ï¼Œæ¯ä¸ªäº‹ä»¶æœ¬èº«ä¸æºå¸¦è¶³å¤Ÿçš„ä¿¡æ¯ã€‚ä¸€ä¸ªç®€å•çš„è§£å†³æ–¹æ¡ˆæ˜¯é€šè¿‡ç§¯ç´¯äº§ç”Ÿå¼ºåº¦å›¾åƒ
äº‹ä»¶åœ¨å›ºå®šçš„æ—¶é—´é—´éš”å†…ï¼Œä½†è¿™ç§äº‹ä»¶å¸§é—´è½¬æ¢å¼•å…¥äº†ä¸€äº›å»¶è¿Ÿå¹¶é˜»ç¢äº†é«˜æ—¶é—´åˆ†è¾¨ç‡å¸¦æ¥çš„æ•ˆç‡ã€‚
ç›¸åï¼Œç®—æ³•åº”è¯¥ç†æƒ³åœ°åˆ©ç”¨ç”Ÿæˆäº‹ä»¶çš„é«˜é€Ÿç‡ã€‚å› æ­¤ï¼Œæœ€è¿‘å·²ç»å¼•å…¥äº†å‡ ç§æ–¹æ³•ï¼Œå…¶åˆ©ç”¨è‡ªä¸»è§†è§‰ä¸­çš„ä¸åŒé—®é¢˜çš„ä¼ æ„Ÿå™¨çš„é«˜æ—¶é—´åˆ†è¾¨ç‡å’Œå¼‚æ­¥æ€§è´¨ã€‚è¿™ç§ç®—æ³•çš„è®¾è®¡ç›®æ ‡æ˜¯æ¯ä¸ªä¼ å…¥äº‹ä»¶å¯ä»¥å¼‚æ­¥åœ°æ”¹å˜ä¼°è®¡çŠ¶æ€ï¼Œä»è€Œæ»¡è¶³ä¼ æ„Ÿå™¨çš„åŸºäºäº‹ä»¶çš„æ€§è´¨ï¼Œå¹¶å…è®¸åœ¨é«˜åº¦åŠ¨æ€çš„æƒ…å†µä¸‹çš„æ„ŸçŸ¥å’ŒçŠ¶æ€ä¼°è®¡ã€‚å¯¹äºè½¨è¿¹ä¼°è®¡ï¼ŒMueggler et alã€‚ ï¼ˆ2015bï¼‰æå‡ºäº†ä¸€ç§è¿ç»­æ—¶é—´æ¨¡å‹ä½œä¸ºç”±å¹³æ»‘å‚æ•°æ¨¡å‹æè¿°çš„å§¿åŠ¿è½¨è¿¹çš„è‡ªç„¶è¡¨ç¤ºã€‚ Rebecqç­‰ï¼ˆ2016ï¼‰æå‡ºäº†ä¸€ç§åŸºäºäº‹ä»¶çš„3Dé‡å»ºç®—æ³•ï¼Œä»¥äº§ç”Ÿåœ¨CPUä¸Šå®æ—¶è¿è¡Œçš„å¹¶è¡Œè·Ÿè¸ªå’Œæ˜ å°„ç®¡é“ã€‚åŸºäºäº‹ä»¶çš„SLAMç”±äºé«˜é€Ÿè¿åŠ¨å’Œéå¸¸é«˜çš„åŠ¨æ€èŒƒå›´åœºæ™¯è€Œä¸å—è¿åŠ¨æ¨¡ç³Šçš„å½±å“ï¼Œè¿™å¯¹äºæ ‡å‡†ç›¸æœºæ–¹æ³•æ¥è¯´å¯èƒ½æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚
- å¯¿å‘½ä¼°ç®—ï¼šé™¤äº†ä¸ºéœ€è¦ä½å»¶è¿Ÿå’Œé«˜å¸§é€Ÿç‡çš„ç°æœ‰é—®é¢˜æä¾›æ–°é¢–çš„è§£å†³æ–¹æ¡ˆå¤–ï¼ŒåŸºäºäº‹ä»¶çš„ä¼ æ„Ÿå™¨ä¹Ÿä¼šå¼•èµ·æ–°çš„é—®é¢˜ã€‚ä¸€ä¸ªè¿™æ ·çš„é—®é¢˜æ˜¯é€šè¿‡å¯¹ä¸€ç»„æ´»åŠ¨äº‹ä»¶å»ºæ¨¡æ¥å¯¹äº‹ä»¶è¿›è¡Œå¯¿å‘½ä¼°è®¡ã€‚åªè¦å¯¼è‡´äº‹ä»¶çš„äº®åº¦æ¢¯åº¦è¢«åƒç´ çœ‹åˆ°ï¼Œäº‹ä»¶è¢«è®¤ä¸ºæ˜¯æ´»åŠ¨çš„ã€‚æ´»åŠ¨äº‹ä»¶çš„æ˜¾å¼å»ºæ¨¡å¯ç”¨äºåœ¨ä»»ä½•æ—¶é—´ç‚¹ç”Ÿæˆé”åˆ©æ¢¯åº¦å›¾åƒï¼Œæˆ–ç”¨äºåœ¨è·Ÿè¸ªå¤šä¸ªå¯¹è±¡æ—¶å¯¹äº‹ä»¶è¿›è¡Œèšç±»ã€‚å¯¹äºè¿™ä¸ªä»»åŠ¡ï¼ŒMuegglerç­‰äººï¼ˆ2015aï¼‰æå‡ºäº†ä½¿ç”¨åŸºäºäº‹ä»¶çš„å…‰æµä¸å¯é€‰æ­£åˆ™åŒ–ï¼Œç‹¬ç«‹äºæ—¶é—´çª—å£ã€‚
