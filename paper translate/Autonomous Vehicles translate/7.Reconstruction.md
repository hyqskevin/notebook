## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
自动驾驶技术的计算机视觉：问题，数据和前沿技术

### 7. Reconstruction 再构建
- 7.1. Stereo 立体
- Stereo estimation is the process of extracting 3D information from 2D images captured by stereo cameras, without need for special range measurement devices. In particular, stereo algorithms estimate depth information by finding correspondences in two images taken at the same point in time, typically by two cameras mounted next to each other on a fixed rig. These correspondences are projections of the same physical surface in the 3D world. Depth information is crucial for applications in autonomous driving or driver assistance systems. Accurate estimation of dense depth maps is a necessary step for 3D reconstruction, and many other problems such as obstacle detection, free space analysis, and tracking benefit from the availability of depth estimates.
- 立体声估计是从立体相机拍摄的2D图像中提取3D信息的过程，而不需要特殊的距离测量装置。 特别地，立体声算法通过在同一时间点上拍摄的两个图像中的对应关系来估计深度信息，通常通过在固定的钻机上彼此相邻地安装两个相机。 这些对应是3D世界中相同物理表面的投影。 深度信息对于自主驾驶或驾驶员辅助系统中的应用至关重要。 精确估计密集深度图是3D重建的必要步骤，诸如障碍物检测，自由空间分析和跟踪等许多其他问题可从深度估计的可用性中获益。

- **Taxonomies**: Multiple taxonomies for stereo matching have been proposed in the literature. Guided by the computational restrictions, the earliest one is based on the density of the output (Franke & Joos (2000)). Feature-based methods provide only sparse depth maps based on edges while area-based methods, such as block matching, generate dense outputs at the expense of computation time. A more recent and commonly referred taxonomy of stereo algorithms is based on the optimization as local and global. Local methods compute the disparity by simply selecting the lowest matching cost which is known as the winner takes all (WTA) solution. Global methods formulate disparity computation as an energy-minimization framework based on the smoothness assumption between neighboring pixels or regions. There are various ways of finding the minimum of a global energy function, including variational approaches in continuous domain and discrete approaches using dynamic programming, Graph Cuts, and Belief Propagation.
- 分类：文献中已经提出了立体匹配的多重分类法。在计算限制的基础上，最早的是基于输出密度（Franke＆Joos（2000））。基于特征的方法仅提供基于边缘的稀疏深度图，而基于区域的方法（例如块匹配）以牺牲计算时间为代价产生密集输出。立体声算法的更新和普遍引用的分类法是基于本地和全局的优化。本地方法通过简单地选择被称为获胜者获得所有（WTA）解的最低匹配成本来计算差异。全局方法基于相邻像素或区域之间的平滑度假设，将视差计算作为能量最小化框架。有各种各样的方法可以找到全局能量函数的最小值，包括使用动态规划，图表切换和信念传播的连续域和离散方法的变分方法。

- Matching Cost Function: Stereo matching is a correspondence problem where the goal is to identify the matching points between left and right image based on a cost function. The algorithms usually assume images are rectified, and the search space is reduced to a horizontal line where the correspondence between a left and right point is encoded by the distance on this line, which is defined as disparity. The matching cost computation is the process of computing a cost function at each pixel for all possible disparities which takes its minimal value at the true disparity. However, it is hard to design such a cost function in practice, therefore stereo algorithms make the assumption of constant appearance between matching points. This assumption is often violated in real-world situations, such as cameras with slightly dierent settings causing exposure changes, vignetting, image noise, non-Lambertian surfaces, illumination changes, etc. Hirschm¨uller & Scharstein (2007) call these changes radiometric differences and systematically investigate their effect on commonly used matching cost functions, namely absolute differences, filter-based costs (LoG, Rank and Mean), hierarchical mutual information (HMI), and normalized crosscorrelation. They found that the performance of a cost function depends on the stereo method that uses it. On images with simulated and real radiometric differences, rank filter performed best for correlation-based methods. For global methods, in tests with global radiometric changes or noise, HMI performed best, while in the presence of local radiometric variations, Rank and LoG filters performed better than HMI. Qualitative results show that filter-based cost cause blurred object boundaries when used with global methods. None of the matching costs evaluated could succeed at handling strong lighting changes.
- 匹配成本函数：立体匹配是一个对应问题，其目的是基于成本函数来识别左右图像之间的匹配点。算法通常假设图像被整流，并且搜索空间减少到水平线，其中左和右点之间的对应被该行上的距离编码，其被定义为视差。匹配成本计算是对于所有可能的差异，在每个像素处计算成本函数的过程，其在真实差异下采用其最小值。然而，在实践中很难设计这样的成本函数，因此立体声算法使匹配点之间的外观不变。这种假设在现实世界的情况下经常遭到侵犯，例如具有轻微不同设置的摄影机，会引起曝光变化，渐晕，图像噪声，非朗伯表面，照明变化等。Hirschmüller＆Scharstein（2007）称这些变化辐射度差异，并系统地研究其对常用匹配成本函数的影响，即绝对差异，基于过滤器的成本（LoG，Rank和Mean），分层互信息（HMI）和归一化互相关。他们发现成本函数的性能取决于使用它的立体声方法。在具有模拟和实际辐射度差异的图像上，等级滤波器对于基于相关的方法表现最佳。对于全局方法，在具有全局辐射度变化或噪声的测试中，HMI性能最好，而在存在局部辐射度变化的情况下，Rank和LoG滤波器的性能优于HMI。定性结果表明，当与全局方法一起使用时，基于过滤器的成本导致对象边界模糊。评估的匹配成本中没有一个可以成功处理强烈的照明变化。

- **SGM**: Semi-Global Matching (SGM) (Hirschm¨uller (2008)) has become very influential due to its speed and high accuracy as evidenced in various benchmarks such as Middlebury (Scharstein & Szeliski (2002)) or KITTI (Geiger et al. (2012b)). SGM is also recently used on top of CNN features, since simply outputting the most likely configuration for every pixel is not competitive with modern stereo algorithms (Zbontar & LeCun (2016); Luo et al. (2016)). The energy function has two levels of penalization for small and large disparity differences with a weighting based on the local intensity gradient for the latter one. The energy is calculated by summing costs along 1D paths from multiple directions towards each pixel using dynamic programming and the result is determined by WTA. There are a couple of follow-up works investigating the practical and theoretical sides of SGM. Gehrig et al. (2009) propose a real-time, low-power implementation of the SGM with algorithmic extensions for automotive applications on a reconfigurable hardware platform. Drory et al. (2014) offer a principled explanation for the success of SGM by clarifying its relation to belief propagation and tree-reweighted message passing with an uncertainty measure as an outcome.
- The performance of SGMs can be further improved by incorporating confidences of the stereo estimation. Seki & Pollefeys (2016) leverage CNNs to predict the confidences for stereo estimations. Taking into account ideas from conventional confidences features, that neighboring pixel which are consistent are more likely to be correct and the disparity estimated from the other image should correspond, they design a two-channel disparity patch which is used as input for the CNN. In order to acquire dense disparity, the confidences are incorporated into SGM by weighting each pixel according to the estimated confidence.
- SGM：半全球配对（Hirschmuller（2008））由于其速度和准确性而变得非常有影响力，如米德伯勒（Scharstein＆Szeliski（2002））或KITTI（Geiger （2012b））。 SGM也最近被用于CNN功能之上，因为简单地输出每个像素的最可能的配置与现代立体声算法（Zbontar＆LeCun（2016）; Luo等（2016））不具有竞争力。能量函数对于小和大的差异差异具有两个级别的惩罚，基于对后者的局部强度梯度的加权。通过使用动态规划将来自多个方向的1D路径的成本与每个像素相加来计算能量，并且结果由WTA确定。有几项后续工作调查了上海证券交易所的实际和理论方面。 Gehrig等人（2009）提出了在可重配置硬件平台上实现汽车应用的算法扩展的SGM的实时低功耗实现。 Drory等（2014年）通过澄清其与信仰传播的关系和树重加权的消息传递以不确定性度量作为结果，为SGM的成功提供了原则性的解释。
- 通过引入立体声估计的信心，可以进一步提高SGM的性能。 Seki＆Pollefeys（2016）利用CNN来预测立体声估计的信心。考虑到常规信号特征的想法，一致的相邻像素更可能是正确的，并且从另一个图像估计的视差应该对应，它们设计一个双通道视差补丁，用作CNN的输入。为了获得密集的差异，通过根据估计的置信度对每个像素进行加权，将信息并入SGM。

- **Variable Baseline/Resolution**: Stereo estimates can be fused to yield a more complete reconstruction of the static parts of the three-dimensional scene. However, assuming fixed baseline, focal length, field of view might not always be the best strategy. Gallup et al. (2008) point out two problems with traditional stereo methods: dropping accuracy in the far range and unnecessary computation time spent in the near range. Given that choice of views for stereo is quite flexible in many applications such as structure from motion, Gallup et al. (2008) propose to dynamically select the best cameras with the appropriate baseline for accurate estimation in the far range from a set of possible cameras recording images at the same time. Further, they reduce the resolution to speed up the computation in the near range. In contrast to traditional fixed-baseline stereo, the proposed variable baseline/resolution stereo algorithm achieves constant accuracy over the reconstructed volume by evenly spreading the computation throughout the volume.
- **Planarity**: The inherent ambiguity in appearance based matching costs can be overcome by regularization, i.e., by introducing prior knowledge about the expected disparity map into the stereo estimation process. The simplest prior favors neighboring pixels to take on the same disparity value. However, such generic smoothness priors fail to reconstruct poorly-textured and slanted surfaces, as they favor fronto-parallel planes. A more generic approach to handle arbitrary smoothness priors is using higher-order connections beyond pairwise. Higherorder priors are able to express more realistic assumptions about depth images, but usually at additional computational cost. One very common way to deal with slanted surfaces in the literature is to assume piecewise planarity. Geiger et al. (2010) build a prior over the disparity space by forming a triangulation on a set of robustly matched correspondences, called support points. This reduces matching ambiguities and results in an efficient algorithm by restricting the search to plausible regions. Gallup et al. (2010) first train a classifier to segment an image into piecewise planar and non-planar regions and then enforce a piecewise planarity prior only for planar regions. Non-planar regions are modeled by the output of a standard multi-view stereo algorithm.
- 可变基准/分辨率：可以融合立体声估计，以产生三维场景的静态部分的更完整的重建。然而，假设固定基线，焦距，视野可能不总是最好的策略。盖洛普等人（2008）指出了传统立体声方法的两个问题：在远程范围内降低精度，在近距离内消耗不必要的计算时间。鉴于立体声的选择在许多应用中是非常灵活的，例如来自运动的结构，Gallup等人（2008）提出动态选择具有适当基准的最佳相机，以便在远距离范围内进行精确估计，从一组可能的摄像机同时记录图像。此外，它们降低了分辨率，以加速近似范围内的计算。与传统的固定基线立体声相比，所提出的可变基线/分辨率立体声算法通过在整个音量上均匀地展开计算，在重建体积上实现了恒定的精度。
- 平面：基于外观的匹配成本的固有歧义可以通过正则化来克服，即通过将关于预期视差图的先前知识引入到立体声估计过程中。最简单的先前有利于相邻像素具有相同的差异值。然而，这种通用平滑度先验不能重构不良纹理和倾斜的表面，因为它们有利于前平行平面。处理任意平滑先验的更通用的方法是使用成对以外的高阶连接。高阶先验能够对深度图像表达更真实的假设，但通常以额外的计算成本。在文献中处理倾斜表面的一个很常见的方法是假设分段平面。盖革等人（2010）通过在称为支持点的一组鲁棒匹配的对应上形成三角测量来构建差异空间的前一个。这减少了匹配模糊度，并通过将搜索限制到合理的区域来产生有效的算法。盖洛普等人（2010）首先训练分类器将图像分割成分段平面和非平面区域，然后仅在平面区域之前实施分段平面度。非平面区域由标准多视点立体声算法的输出建模。

- **Variational Approaches**: Similarly, in variational approaches, commonly used smoothness prior, Total Variation (TV) does not produce convincing results in the presence of weak and ambiguous observations, since it encourages piecewise constant regions leading to stair-casing artifacts. Haene et al. (2012) introduce patch-based priors into a TV framework in the form of small, piecewise planar dictionaries. Total Generalized Variation (TGV) (Bredies et al. (2010)) is argued to be a better prior than TV, since it does not penalize piecewise affine solutions. However, it is restricted to convex data terms in contrast to TV, where global solutions can be computed even in the presence of non-convex data terms. Coarse-to-fine approaches as an approximation to non-convex problem of stereo matching often end up with loss of details. To preserve fine details, Kuschk & Cremers (2013) integrate an adaptive regularization weight into the TGV framework by using edge detection and report improved results compared to a coarse-to-fine approach. Ranftl et al. (2013) obtain even better results by proposing a decomposition of the non-convex functional into two subproblems which can be solved globally where one is convex, and the other can be made convex by lifting the functional to a higher dimensional space.
- **State-of-the-art**: In Table 6 we show the ranking of stereo methods on the KITTI stereo 2015 benchmark. The KITTI benchmark reports the percentage of erroneous (bad) pixels over background regions (D1-bg), foreground regions (D1-fg) and over all regions (D1-all). The best performing method G¨uney & Geiger (2015) use object knowledge to compensate for the weak data term on the reflecting and textureless surfaces. Seki &Pollefeys (2016) achieve the best performance on background regions with the prediction of stereo correspondence confidences and integration into SGM. Recently, deep learning approaches (Zbontar&LeCun (2016); Luo et al. (2016); Mayer et al. (2016)) were proposed achieving state-of-the-art performance. The deep learning approach presented by Mayer et al. (2016) is one of the fastest approaches.
- 变异方法：类似地，在变分方法中，普遍使用的平滑度之前，总变异（TV）不会产生令人信服的结果存在弱和模糊的观察，因为它鼓励分段恒定区域导致楼梯套管伪影。 Haene等人（2012）将补丁为基础的先验引入电视框架，形式为小型，分段平面字典。总广义变异（TGV）（Bredies等人（2010））被认为比电视更好，因为它不惩罚分段仿射解决方案。然而，与TV相反，它仅限于凸数据项，即使在存在非凸数据项的情况下，也可以计算全局解。粗略到精细的方法作为立体匹配的非凸问题的近似通常最终导致细节的损失。为了保持细节，Kuschk＆Cremers（2013）通过使用边缘检测将自适应正则化权重整合到TGV框架中，并报告与粗略到粗略方法相比的改进结果。 Ranftl等人（2013）通过提出将非凸函数分解成两个子问题来获得更好的结果，这两个子问题可以在一个凸起的情况下被全局求解，另一个可以通过将功能提升到更高维度的空间来制作凸起。
- 最先进的：在表6中，我们展示了KITTI立体声2015基准的立体声方法的排名。 KITTI基准测试报告了背景区域（D1-bg），前景区域（D1-fg）和所有区域（D1-all）之间的错误（差）像素的百分比。最好的表现方法G¨uney＆Geiger（2015）使用对象知识来补偿反射和无纹理表面上的弱数据项。 Seki＆Pollefeys（2016）在背景区域实现了最佳性能，预测了立体声通信的信心并融入了SGM。最近，提出了深入学习的方法（Zbontar＆LeCun（2016）; Luo等（2016）; Mayer等（2016））提出了最先进的表现。 Mayer等人提出的深度学习方法（2016）是最快捷的方法之一。

- **Superpixels**: An alternative way of modeling piecewise planarity is to explicitly partition the image into superpixels and modeling the surface at each superpixel as a slanted plane (Yamaguchi et al. (2012); G¨uney & Geiger (2015)). However, care must be taken that the super-pixelization is indeed an over-segmentation of the image with respect to planarity, i.e., that no superpixel contains two surfaces which are not co-planar. Yamaguchi et al. (2012) jointly reason about occlusion boundaries and depth in a hybrid MRF composed of both continuous and discrete random variables. Guney & Geiger (2015) use a similar framework to incorporate object-category specific 3D shape proposals which regularize over larger distances. By leveraging semantic segmentation and 3D CAD models, they resolve ambiguities in reflective and textureless regions originating from highly specular surface of cars in the scene as shown in Figure 19.
- **Deep Learning**: In the last years, deep learning approaches (Mayer et al. (2016); Zbontar&LeCun (2016); Luo et al. (2016)) gained popularity in stereo estimation. Mayer et al. (2016) adapt the encoder-decoder architecture proposed by Dosovitskiy et al. (2015) that was used for optical flow estimation (see Section 8.1). The encoder computes abstract features while the decoder reestablishes the original resolution with additional crosslinks between the contracting and expanding network parts. In contrast to the encoder-decoder architecture, ˇ Zbontar & LeCun (2016); Luo et al. (2016) use Siamese network which consists of two sub-networks with shared weights and a final score computation layer. The idea is to train the network for computing the matching cost by learning a similarity measure on small image patches. Zbontar & LeCun (2016) define positive/negative examples as matching and non-matching patches and use a margin loss to train either a fast architecture with a simple dot-product layer in the end or a slow but more accurate architecture which learns score computation with a set of fully connected layers. Luo et al. (2016) use a similar architecture, but formulate the problem as multi-class classification over all possible disparities to capture correlations between different disparities implicitly as visualized in Figure 20.
- 超像素：分段平面建模的另一种方法是将图像明确地划分为超像素，并将每个超像素的表面作为倾斜平面进行建模（Yamaguchi et al。（2012）;G¨uney＆Geiger（2015）） 。然而，必须注意，超像素化确实是相对于平面度的图像的过度分割，即，没有超像素包含不共面的两个表面。山口等（2012）共同理解了由连续和离散随机变量组成的混合MRF中的闭塞边界和深度。 Guney＆Geiger（2015）使用类似的框架来整合在更大距离上规则化的对象​​类特定3D形状提案。通过利用语义分割和3D CAD模型，它们解决了源自现场汽车高度镜面的反射和无纹理区域的模糊性，如图19所示。
- 深度学习：在过去几年中，深度学习方法（Mayer等（2016）; Zbontar＆LeCun（2016）; Luo等（2016））在立体声估计中得到普及。 Mayer等人（2016）适应Dosovitskiy等人提出的编码器 - 解码器架构。 （2015年），用于光流估计（见第8.1节）。编码器计算抽象特征，而解码器重新建立原始分辨率，并在合同和扩展网络部分之间附加交叉链接。与编码器 - 解码器架构相比，Zbontar＆LeCun（2016）;罗等人（2016）使用由具有共享权重的两个子网络和最终分数计算层组成的暹罗网络。这个想法是通过学习小图像补丁的相似性度量来训练网络来计算匹配成本。 Zbontar＆LeCun（2016）将正/负的例子定义为匹配和非匹配补丁，并使用边际损失来训练最终的简单点阵产品层的快速架构，或者学习分数计算的慢但更准确的架构与一套完全连接的层。罗等人（2016）使用了类似的架构，但是将所有可能的差异作为多类别分类来形成问题，以便在图20中隐含地隐藏不同差异之间的相关性。


- Discussion: Stereo estimation has shown great progress in the last years both in terms of accuracy and efficiency. However, some inherent problems refrain it from being marked as solved. Stereo matching is ultimately searching for correspondences in two images based on the assumption of constant appearance. However, appearance frequently changes by cues different than geometry, furthermore occluded regions or pixels leaving the frame cannot be matched. Therefore, failure in those cases is inevitable for methods that solely rely on appearance matching without any other prior assumptions about the geometry. We show accumulated errors of top 15 methods on KITTI stereo benchmark Geiger et al. (2012b) in Figure 21. The most common example of failure case in the autonomous driving context are car surfaces due to shiny and reflective regions. G¨uney & Geiger (2015) specifically address this problem by integrating prior knowledge on possible car shapes. Similarly, windows that are reflective and transparent cannot be matched reliably. As concluded by Hirschm¨uller & Scharstein (2007), strong illumination changes constitute another common source of error such as inside a tunnel or over-exposure on road surfaces. Pixels leaving the frame and occlusions often cause errors for many methods and both require reasoning beyond matching and local interactions. Other specific examples of problematic regions include thin structures like traffic signs, or repetitive ones like fences.
- 讨论：立体声估计在过去几年中在精度和效率方面都取得了很大进展。然而，一些固有的问题避免被标记为解决。基于恒定外观的假设，立体匹配最终搜索两个图像中的对应关系。然而，外观通常由与几何不同的线索改变，而离开框架的遮挡区域或像素也不能匹配。因此，在这些情况下的失败对于仅依赖于外观匹配的方法来说是不可避免的，而对于几何形状没有任何其他先前的假设。我们在KITTI立体声基准Geiger等人显示了前15种方法的累积误差。 （2012b）。自主驾驶环境中故障案例的最常见例子是汽车表面，由于光线和反射区域。 G¨uney＆Geiger（2015）专门针对这个问题，整合了可能的汽车形状的先前的知识。类似地，反射和透明的窗口不能可靠地匹配。如Hirschmuller＆Scharstein（2007）所做的那样，强烈的照明变化构成了隧道内的另一个常见的错误来源，或者在路面上过度曝光。离开帧和遮挡的像素通常会导致许多方法的错误，并且都需要超出匹配和本地交互的推理。有问题的地区的其他具体例子包括像交通标志这样的薄结构，或像栅栏这样的重复的结构。

- 7.2. Multi-view 3D Reconstruction 多视角3D重构
- The goal of multi-view 3D reconstruction is to model the underlying 3D geometry by inverting the image formation process often under certain prior or smoothness assumptions. In contrast to two-view stereo, multi-view reconstruction algorithms in particular address the problems of varying viewpoints and the complete reconstruction of 3D scenes from more than two and potentially a very large number of images. If the camera parameters are known, solving for the 3D geometry of the scene is equivalent to solving the correspondence problem, based on a photo-consistency function which measures the agreement between different viewpoints.
- Taxonomies: Several categorizations of multi-view reconstruction algorithms have been proposed in the literature, typically considering the form of the photo-consistency function, the scene representation, visibility computation, priors, and initialization requirements as in Seitz et al. (2006). From an application perspective, the scene representation is a common way of classifying multi-view reconstruction approaches into depth map, point cloud, mesh, and volumetric.
- 多视图3D重建的目标是通过在某些先前或平滑假设下经常反转图像形成过程来对底层3D几何进行建模。与双视图立体声相比，多视图重建算法特别解决了来自两个以上且潜在的大量图像的变化视点和3D场景的完整重建的问题。如果相机参数是已知的，则解决场景的3D几何相当于基于测量不同视点之间的一致性的照片一致性函数来解决对应问题。
- 分类：文献中已经提出了多视图重建算法的几个分类，通常考虑到照片一致性函数的形式，场景表示，可视性计算，先验和初始化要求，如Seitz等人。 （2006年）。从应用的角度来看，场景表示是将多视图重建方法分为深度图，点云，网格和体积的常见方式。

- **Representations: Depth Map**: The depth map representation typically consists of a depth map for each input view estimated with a 3D modeling pipeline which starts with image matching followed by pose estimation and dense stereo. This representation is usually preferred in scene analysis due to its flexibility and scalability to large scenes. One strategy which is particularly effiective for urban scenes is Plane Sweeping Stereo algorithm (Collins (1996)). It sweeps a family of parallel planes in a scene, projects images onto a plane via planar homographies, then evaluates photo-consistency values on each plane. In large scenes, one of the challenges is to handle massive amount of data in real-time. Pollefeys (2008) propose a large scale, realtime 3D reconstruction system based on depth map representation. The real-time performance is achieved by incorporating a set of components which are particularly efficient on typical urban scenes such as a 2D feature tracker with automatic gain adaptation for handling large dynamic range in natural scenes, and parallel implementations of plane sweeping stereo and depth map fusion on GPU.
- **Representations: Point-cloud**: In contrast to a partial depth map for each view, point-cloud or patch based surface representations reconstruct a single 3D point-cloud model using all the input images. Under spatial consistency assumptions, the pointcloud on the surface of the scene can grow or expand which provides easy model manipulation such as merging and splitting. The representative work for these kind of approaches is Patch-based Multi-View Stereo (PMVS) by Furukawa & Ponce (2010). PMVS starts with a feature matching step to generate a sparse set of patches and then iterate between a greedy expansion step and a filtering step to make patches dense and remove erroneous matches.
- 表示：深度图：深度图表示通常由用3D建模流水线估算的每个输入视图的深度图组成，3D建模流水线以图像匹配开始，随后是姿态估计和密集立体。由于场景分析的灵活性和可扩展性，因此这种表示在场景分析中通常是首选的。一种对城市场景特别有效的策略是平面扫描立体声算法（Collins（1996））。它在场景中扫描一系列平行平面，通过平面同图将图像投影到平面上，然后评估每个平面上的照片一致性值。在大型场景中，挑战之一是实时处理大量数据。 Pollefeys（2008）提出了一种基于深度图表示的大规模实时三维重建系统。实时性能是通过结合一套在典型城市场景上特别有效的组件来实现的，例如具有自动增益适应的2D特征跟踪器，用于处理自然场景中的大动态范围，以及平面扫描立体声和深度图的并行实现融合在GPU上
- 表示：点云：与每个视图的部分深度图相反，点云或基于贴片的表面表示使用所有输入图像重建单个3D点云模型。在空间一致性假设下，场景表面上的点云可以增长或扩展，这提供了容易的模型操作，如合并和分割。这些方法的代表性工作是Furukawa＆Ponce（2010）的基于Patch的多视点立体声（PMVS）。 PMVS从特征匹配步骤开始，以生成一组稀疏的补丁，然后在贪心扩张步骤和过滤步骤之间进行迭代，以使补丁密集并删除错误的匹配项。

- **Representations: Volumetric**: Volumetric approaches represent geometry on a regularly sampled 3D grid, i.e. volume, either as a discrete occupancy function (Kutulakos&Seitz (2000)) or a function encoding distance to the closest surface (level-set) (Faugeras & Keriven (1998)). More recent approaches use a probability map defined at regular voxel locations to encode the probability of occupancy (Bhotika et al. (2002); Pollard & Mundy (2007); Ulusoy et al. (2015)). The amount of memory required is the main limitation for volumetric approaches. There is a variety of methods for dealing with this problem such as voxel hashing (Nießner et al. (2013)) or a data adaptive discretization of the space in the form of a Delaunay triangulation (Labatut et al. (2007)). One effective solution is an octree data structure which is essentially an adaptive voxel grid to allocate high resolution cells only near the surfaces.
- **Representations: Mesh or Surface**: The final representation in reconstruction is typically triangular mesh-based surfaces. Volumetric surface extraction fuses 3D information from an intermediate representation such as depth maps, point clouds, volumes or scans into a single, clean mesh model. Seminal work by Curless & Levoy (1996) proposes an algorithm to accumulate surface evidence into a voxel grid using signed distance functions. The surface is implicitly represented as the zero crossing of the aggregated signed distance functions. It can be extracted using the Marching Cube algorithm Lorensen & Cline (1987) or using volumetric graph cuts to label each voxel as interior or exterior. There are approaches which directly start from images and refine a mesh model using an energy function composed of a data term based on photo-consistency function and a regularization term for smoothness. In these approaches, the energy is usually optimized using gradient descent, where the movement of each vertex is determined by the gradient of the objective function.
- 表示：体积：体积方法表示定期采样的3D网格上的几何，即体积，作为离散占用函数（Kutulakos＆Seitz（2000））或函数编码距离最接近的表面（水平集）的距离（Faugeras ＆Keriven（1998））。最近的方法使用在常规体素位置定义的概率图来编码占用概率（Bhotika等（2002）; Pollard＆Mundy（2007）; Ulusoy等（2015））。所需的内存量是体积方法的主要限制。处理这个问题的方法有很多种，如体素散列（Nießneret al。（2013））或以Delaunay三角剖分形式对空间进行数据自适应离散化（Labatut et al。（2007））。一个有效的解决方案是八叉树数据结构，它本质上是一个适应性体素网格，仅在表面附近分配高分辨率单元。
- 表示：网格或曲面：重建中的最终表示通常是三角形网格的曲面。体积表面提取将3D信息从诸如深度图，点云，体积或扫描的中间表示融合到单个，干净的网格模型中。 Curless＆Levoy（1996）的精神工作提出了一种使用带符号距离函数将表面证据积累到体素网格中的算法。表面隐含地表示为聚合有符号距离函数的过零点。它可以使用Marching Cube算法Lorensen＆Cline（1987）提取，或使用体积图切割将每个体素标记为内部或外部。存在直接从图像开始的方法，并且使用由基于光一致性函数的数据项组成的能量函数和用于平滑度的正则化项来细化网格模型。在这些方法中，通常使用梯度下降优化能量，其中每个顶点的移动由目标函数的梯度确定。

- **Urban Reconstruction**: In this survey, we focus on multiview reconstruction from an autonomous driving perspective which mainly concerns the reconstruction of large urban areas, up to whole cities. The goal of urban reconstruction algorithms is to produce fully automatic, high-quality, dense reconstructions of urban areas by addressing inherent challenges such as lighting conditions, occlusions, appearance changes, high-resolution inputs, and large scale outputs. Musialski et al. (2013) provide a survey of urban reconstruction approaches by following an output-based ordering, namely buildings and semantics, facades and images, and finally blocks and cities.
- **Input Data**: Musialski et al. (2013) point out that ground, aerial and satellite imagery, as well as Light Detection and Ranging (LiDAR) scans are the most commonly used sensors for urban reconstruction. Ground-level imagery is the most prevalent one due to easy acquisition, storage and exchange. Aerial and satellite imagery have become more easily available due to the advances of Web-mapping projects. In contrast to aerial or multi-view imagery, satellite imagery provides a worldwide coverage at a high frequency with lower costs, but also with lower resolution. LiDAR delivers semi-dense 3D point-clouds which are fairly precise, both ground-level and aerial. Some approaches also incorporate several of these data types together in order to combine their complementary strengths. To deal with the challenging conditions of outdoor scenes, other methods leverage additional data sources, like Digital Surface Models (DSMs) which capture the Earth’s surface. DSMs are 2:5D representations of an urban scene that provide a height for each point on a regular grid. In the following, we provide recent examples of dierent input modalities.
- 城市重建：在这次调查中，我们从自主驾驶的角度重点关注多视角重建，主要涉及到大城市，直到整个城市的重建。城市重建算法的目标是通过解决诸如照明条件，遮挡，外观变化，高分辨率输入和大规模输出等内在挑战，来实现城市全自动，高质量，密集的重建。 Musialski等（2013年）通过遵循基于输出的排序，即建筑物和语义，外墙和图像，以及最终的街区和城市，提供城市重建方法的调查。
- 输入数据：Musialski et al。 （2013）指出，地面，空中和卫星图像以及光检测和测距（LiDAR）扫描是城市重建中最常用的传感器。地平面图像是最流行的，因为易于采集，存储和交换。由于Web-mapping项目的进步，空中和卫星图像变得更容易获得。与空中或多视点图像相比，卫星图像在高频率下提供了全球覆盖，成本更低，而且分辨率更低。 LiDAR提供半密度3D点云，这些云点相当精确，包括地面和天线。一些方法也将这些数据类型中的几种结合在一起，以便结合它们的互补优势。为了应对户外场景的挑战性条件，其他方法利用了捕获地球表面的数字表面模型（DSM）等附加数据源。帝斯曼是城市场景的2：5D表示，为常规网格上的每个点提供高度。在下文中，我们提供了不同输入模式的最近例子。

- **Stereo Sequences**: Cornelis et al. (2008) point out that the extraction of detailed 3D information from video streams incur high computational cost for reconstruction algorithms. By keeping the necessary level of detail low, they focus on creating compact, memory ecient 3D city models from a stereo pair, at high speed based on simplified geometry assumptions, namely ruled surfaces for facade and road surfaces. Since objects such as cars which are prevalent in urban scenes violate these assumptions, they integrate the detection and localization of cars into the reconstruction. By leveraging efficient stereo matching, Geiger et al. (2011) propose a system to generate accurate 3D reconstructions of static scenes from stereo sequences in realtime. For online reconstruction, they employ two threads: the first thread performs feature matching and ego-motion estimation, while the second thread performs dense stereo matching and 3D reconstruction.
- **Digital Surface Models (DSM)**: Digital Surface Models are either generated from aerial LiDAR point clouds or Multi-View Stereo (MVS) and adapted to geometric descriptions of urban scenes. MVS-based DSMs can be very noisy and therefore Lafarge et al. (2010) propose to generate DSMs from MVS imagery by reconstructing buildings with an assemble of simple urban structures extracted from a library of 3D parametric blocks. In contrast to MVS-based DSMs, laser scans have been also very popular to acquire 3D city models. Lafarge & Mallet (2012) provide a more complete description of urban scenes by simultaneously reconstructing trees and topologically complex ground surfaces in addition to the buildings from point clouds generated by aerial data. They model the original hybrid representation of buildings by combining two dierent types of 3D representations: primitives for regular parts of buildings as in Lafarge et al. (2010) and mesh patches for modeling atypical surfaces such as irregular roofs.
- 立体声序列：Cornelis et al。 （2008）指出，从视频流中提取详细的3D信息会导致重建算法的高计算成本。通过保持必要的细节水平，他们专注于从立体声对，基于简化的几何假设，即立面和路面的规则表面高速创建紧凑，记忆效率的3D城市模型。由于诸如城市场面普遍存在的汽车等物品违反了这些假设，将汽车的检测和本地化整合到重建中。通过利用高效的立体匹配，Geiger等（2011）提出了一种从立体声序列实时生成静态场景的精确3D重建的系统。对于在线重建，它们采用两个线程：第一个线程执行特征匹配和自主运动估计，而第二个线程执行密集的立体匹配和3D重建。
- 数字表面模型（DSM）：数字表面模型可以从空中LiDAR点云或多视点立体声（MVS）生成，并适用于城市场景的几何描述。基于MVS的DSM可能非常嘈杂，因此Lafarge等人（2010）提出从MVS图像生成DSM，通过从3D参数块库提取的简单城市结构的组合重建建筑物。与基于MVS的DSM相反，激光扫描也非常受欢迎，以获得3D城市模型。拉法基和马勒（2012）通过同时重建树木和拓扑复杂的地面以及由航空数据产生的点云的建筑物，提供了更完整的城市场景描述。他们通过组合两种不同类型的3D表示来模拟建筑物的原始混合表示：如Lafarge等人的常规建筑部分的原始图形。 （2010）和用于建模非典型表面（如不规则屋顶）的网格补丁。

- **Air and Street level**: Fruh et al. (2005) register a series of vertical 2D surface scans and camera images to airborne data (DSMs) to generate textured facade meshes of cities. They propose a class of data processing techniques to create visually appealing facade meshes by removing noisy foreground objects and filling holes in the geometry and texture of building facades. B´odis-Szomor´u et al. (2016) point out that airborne and mobile mapping data provide complementary information and need to be exploited together in order to produce complete and detailed large-scale city models. Airborne sensors can acquire roof structures, ground, and vegetation at large scale while on-road mobile mapping by multi-view stereo approaches or LiDAR provide the facade and street-side details. They propose
a solution to fuse a detailed on-road mobile mapping and a coarser but more complete point cloud from airborne acquisition in a joint surface mesh. Their evaluation shows that the quality of the model improves substantially by fusing streetside details into the airborne model.
- **Stereo Satellite**: Duan & Lafarge (2016) propose a method to produce compact 3D city models composed of ground and building objects from stereo pairs of satellite images. They represent the scene using convex polygons and perform joint classification and reconstruction of the semantic class (ground, roof, and facade) and the elevation of each polygon. Although their evaluation shows that the obtained results are not as accurate as LiDAR scans, the proposed method can produce fast, compact, and semantic-aware models robust to low resolution and occlusion problems.
- 空气和街道等级：Fruh et al。 （2005）将一系列垂直二维表面扫描和摄像机图像注册到机载数据（DSM），以生成城市的纹理外观网格。他们提出了一类数据处理技术，通过去除嘈杂的前景物体并填充建筑立面的几何和纹理中的孔，来创建视觉上吸引人的立面网格。 B'odis-Szomor'u等（2016）指出，机载和移动地图数据提供了补充信息，需要一起利用，以便制作完整和详细的大型城市模型。机载传感器可以大规模获得屋顶结构，地面和植被，而通过多视角立体声方式或LiDAR的道路移动地图可提供立面和街道细节。他们提出
一种解决方案，用于融合详细的道路上移动地图以及从联合表面网格中的空中采集获取更粗糙但更完整的点云。他们的评价表明，通过将街头细节融入机载模型，模型的质量大大提高。
- 立体声卫星：Duan＆Lafarge（2016）提出了一种制作紧凑型3D城市模型的方法，由地面和建筑物体组成的立体声对卫星图像。它们使用凸多边形代表场景，并进行语义类（地面，屋顶和立面）的联合分类和重建以及每个多边形的高程。虽然他们的评估表明所获得的结果不如LiDAR扫描的准确性，但是所提出的方法可以产生对低分辨率和闭塞问题坚固的快速，紧凑和语义感知模型。

- 7.3. Reconstruction and Recognition
- In autonomous driving, it is important to understand both the structural and semantic information of the surroundings. Traditionally, image segmentation methods employ priors entirely in the 2D image domain, i.e., spatial smoothness terms, and reconstruction methods usually encourage piecewise smooth surfaces. It has been long argued that semantics and 3D reconstruction carry valuable information to each other. Similarly to stereo, the motivation to incorporate semantics in reconstruction is photo-consistency failing in case of imperfect and ambiguous image information due to specularities, lack of texture, repetitive structures, or strong lighting changes. Semantic labels provide geometric cues about likely surface orientations at a certain location and help resolving inherent ambiguities. 3D reconstruction lifts the reasoning from 2D to 3D and acts as a strong regularizer by enforcing geometric consistency over multiple images for segmentation.
- Planarity and Primitives: Micusik & Kosecka (2009) present a method to overcome these difficulties by exploiting image segmentation cues as well as presence of dominant scene orientations and piecewise planar structures. In particular, they adopt a super-pixel based dense stereo reconstruction method by using the Manhattan world assumption with three orthogonal plane normals in the MRF formulation. Another way of exploiting piecewise planar structures and the shape repetition is to use primitives such as planes, spheres, cylinders, cones and tori (Lafarge et al. (2010); Lafarge & Mallet (2012); Lafarge et al. (2013)). Primitive arrangement-based approaches provide compactness and reduce complexity. However, they remain simplistic representations and fail to model fine details and irregular shapes. Therefore, Lafarge et al. (2013) propose a hybrid approach which is both compact and detailed. Starting from an initial mesh-based reconstruction, they use primitives for regular structures such as columns and walls, while irregular elements are still described by meshes for preserving details.
- 在自主驾驶中，了解周边环境的结构和语义信息很重要。传统上，图像分割方法在2D图像域中完全使用先验，即空间平滑度项，重建方法通常会促使分段平滑表面。长期以来，语义和3D重建相互传递有价值的信息。与立体声类似，在重建中纳入语义的动机是由于镜面反射，缺乏纹理，重复结构或强烈的照明变化而导致的图像信息不完整和模糊的情况下的照片一致性失败。语义标签提供关于某个位置处可能的表面取向的几何线索，并帮助解决固有的模糊性。 3D重建将推理从2D提升到3D，并通过对多个图像执行几何一致性进行分割，并作为强正则化器。
平面性和原始性：Micusik＆Kosecka（2009）提出了一种通过利用图像分割线索以及主要场景取向和分段平面结构的存在来克服这些困难的方法。特别地，它们采用基于超像素的密集立体重建方法，通过在MRF公式中使用具有三个正交平面法线的曼哈顿世界假设。使用分段平面结构和形状重复的另一种方法是使用诸如平面，球体，圆柱体，锥体和托里（Lafarge等人（2010）; Lafarge＆Mallet（2012）; Lafarge等人（2013））的原语。基于原始布置的方法提供了紧凑性并降低了复杂性。然而，它们仍然是简单的表示，并且不能对精细细节和不规则形状进行建模。因此，拉法基等（2013）提出一种既紧凑又详细的混合方法。从初始的基于网格的重建开始，它们使用基本原理用于常规结构，例如列和墙壁，而不规则元素仍由网格描述以保留细节。

- Volumetric: Volumetric scene reconstruction typically segments the volume into occupied and free-space regions. Haene et al. (2013) present the mathematical framework to extend it to a multi-label volumetric segmentation framework which assigns object classes or a free-space label to voxels as shown in Figure 22. They first learn appearance likelihoods and classspecific geometry priors for surface orientations from the training data. Then, these data-driven priors are used to define unary and pairwise potentials in a continuous formulation for volumetric segmentation. Joint reasoning benefits from typical class-specific geometry, such as the normals of the ground plane pointing upwards. In addition, it provides a class-specific smoothness prior in cases of weak cues for the scene geometry. Their evaluation shows the benefit of such a prior over standard smoothness assumptions such as Total Variation.
- Zhou et al. (2015) propose a method for 3D reconstruction of street scenes from a sequence of fisheye cameras by introducing semantic priors. Motivated by recurring objects of similar 3D shapes in outdoor scenes, they first localize buildings and vehicles using 3D object detectors and then jointly reconstruct them while learning a volumetric model of their shape. This allows to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations of the respective category.
- 体积：体积场景重建通常将体积分成占用和自由空间区域。 Haene等人（2013）提出了数学框架，将其扩展到多标签体积分割框架，该框架将对象类或自由空间标签分配给体素，如图22所示。他们首先学习表面取向的外观似然性和类特定几何先验从训练数据。然后，这些数据驱动的先验被用来定义连续公式中的一元和成对的电位，用于体积分割。联合推理受益于典型的类别特定几何，如地平面向上的法线。此外，在场景几何的弱提示的情况下，它提供了一个类别的平滑度。他们的评估显示了这样一个先前过度的标准平滑假设（如总变异）的好处。
- Zhou et al。 （2015）提出了一种通过引入语义先验从一系列鱼眼摄影机的街道场景3D重建的方法。由于户外场景中类似3D形状的反复出现的对象，他们首先使用3D物体检测器对建筑物和车辆进行本地化，然后在学习其形状的体积模型的同时重建它们。这允许在完成缺失表面时减少噪声，因为具有类似形状的对象受益于相应类别的所有观察。

- **Monocular Video**: Failures in multi-view stereo cause problems for approaches like Haene et al. (2013) which require dense depth measurements. Using a monocular image stream as input, Kundu et al. (2014) propose another joint reasoning approach over a sparse point cloud from SfM and dense semantic labeling of the frames. This way, 3D semantic representation is temporally coherent without additional cost. They model the problem with a higher order CRF in 3D which allows realistic scene constraints and priors such as 3D object support. In addition, they explicitly model the free space which provides cues to reduce ambiguities, especially along weakly supported surfaces. Their evaluation on monocular datasets Camvid and Leuven shows improved 3D structure compared to traditional SfM and state-of-the-art multi-view stereo as well as better segmentation quality over video segmentation methods in terms of both per pixel accuracy and temporal consistency.
- 单目视频：多视点立体声故障导致Haene等人等方法出现问题。 （2013年），需要深度测量。 使用单目图像流作为输入，Kundu et al。 （2014）提出了从SfM的稀疏点云和框架的密集语义标注的另一个联合推理方法。 这样，3D语义表示在时间上相干而没有额外的成本。 他们用3D中的高阶CRF对问题进行建模，这样可以实现场景约束和3D对象支持等先修。 此外，他们明确地模拟了提供线索的自由空间，以减少模糊，特别是在弱支撑的表面上。 他们对单目数据集的评估Camvid和Leuven显示出与传统SfM和最先进的多视点立体声相比改进的3D结构以及在每像素精度和时间一致性方面对视频分割方法的更好的分割质量。

- **Volumetric**: Large-scale: Previous works on semantic reconstruction (Haene et al. (2013); Kundu et al. (2014)) are limited to small scenes and low resolution, because of their large memory footprint and computational cost. To scale them up to large scenes, Blaha et al. (2016) point out that high resolution is not required for large regions such as free space, parts under the ground, or inside the building. They propose an extension of Haene et al. (2013) by employing an adaptive octree data structure with coarse-to-fine optimization, in an application to generate 3D city models from terrestrial and aerial images. Starting from a coarse voxel grid, they solve a sequence of problems in which the solution is gradually refined only near the predicted surfaces. The adaptive refinement saves memory and runs much faster while still being as accurate as the fixed voxel discretization at the highest target resolution, both in geometric reconstruction and semantic labeling.
- Besides the spatial extent, the number of different semantic labels is also a problem for scalability due to increasing memory requirements. The complexity is quadratic in the number of labels due to indicator variables for the transitions between the different labels. Cherabier et al. (2016) propose to divide the scene into blocks in which only a set of relevant labels is active, since absence of many semantic classes from a specific block can be determined early on. Accordingly, they can deactivate a label right from the beginning of the optimization which leads to a more efficient processing. The set of active labels in each block is updated during the iterative optimization to recover from wrong initializations. Their evaluation shows that they can increase the number of labels from six to nine with a significant gain in memory compared to Haene et al. (2013).
- 体积：大规模：以前的语义重建（Haene et al。（2013）; Kundu et al。（2014））仅限于小场景和低分辨率，因为它们具有较大的内存占用和计算成本。为了将它们扩大到大型场景，Blaha等（2016年）指出，大型地区，如自由空间，地下部分，建筑物内部都不需要高分辨率。他们提出延长Haene等（2013）通过在从地面和航空图像生成3D城市模型的应用中采用具有粗略到优化的自适应八叉树数据结构。从粗体素网格开始，它们解决了一系列问题，其中解决方案仅在预测表面附近逐渐完善。自适应细化可以节省内存并且运行得更快，同时在几何重建和语义标注中仍然与最高目标分辨率下的固定体素离散度一样准确。
- 除空间范围外，由于内存需求的增加，不同语义标签的数量也是可扩展性的问题。由于不同标签之间的转换的指示符变量，标签数量的复杂度是二次方。 Cherabier等人（2016）提出将场景划分为只有一组相关标签是活动的块，因为可以在早期确定不存在来自特定块的许多语义类别。因此，它们可以从优化开始就停用标签，这导致更有效的处理。每个块中的活动标签集在迭代优化期间被更新，以从错误的初始化中恢复。他们的评估表明，与Haene等人相比，他们可以将标签的数量从6个增加到9个，记忆显着增加。 （2013年）。

- **Shape Priors**: Advances in sensors to acquire 3D shapes and the performance of object detection algorithms have encouraged the use of 3D shape priors in 3D reconstruction. Dimensionality reduction is an effective and popular way of representing shape knowledge. Early approaches use linear dimensionality reduction such as PCA to capture the shape variance in low dimensional latent shape spaces. More recent approaches use nonlinear dimensionality reduction such as Gaussian Process Latent Variable Models (GP-LVM) (Dame et al. (2013)). Dame et al. (2013) investigate the importance of shape priors in a monocular SLAM approach. In parallel with depth estimation, they refine an object’s pose, shape and scale to match an initial segmentation and depth cues. It is finally fused into the volumetric representation. Their experiments show improvement in transparent and specular surfaces, and even in unobserved parts of the scene. In addition to mean shape, Bao et al. (2013) propose to learn a set of anchor points as representative of object shape across several instances. They first perform an initial alignment using 2D object detectors. Next, they align the point cloud from SfM with the mean shape by matching anchor points, and then warp and refine it to approach the actual shape. Their evaluation demonstrates that the model is general enough to learn semantic priors for different object categories such as car, fruit, and keyboard by handling large shape variations across instances.
- While previous approaches (Dame et al. (2013); Bao et al. (2013)) try to fit a parametric shape model to input data, Haene et al. (2014) model the local distribution of normals for an object. They propose an object class specific shape prior in the form of spatially varying anisotropic smoothness terms. Similar to multi-label segmentation approach of Haene et al. (2013), they divide the reconstruction into object region and the supporting ground and apply the shape prior only on object to guide the optimization to the right shape.
- 先前的形状：获取3D形状的传感器的进步和对象检测算法的性能已经鼓励在3D重建中使用3D形状先验。尺寸减少是表示形状知识的有效和受欢迎的方式。早期方法使用线性维数降低（如PCA）来捕获低维隐形形状空间中的形状方差。更近期的方法使用非线性维数降低，如高斯过程潜在变量模型（GP-LVM）（Dame等（2013））。 Dame等人（2013）研究形状先验在单眼SLAM方法中的重要性。与深度估计并行，它们优化对象的姿态，形状和尺度以匹配初始分割和深度线索。它最终融合到体积表示中。他们的实验显示透明和镜面表面的改进，甚至在场景的未观察的部分。除了平均形状外，Bao et al。 （2013）提出在几个实例中学习一组锚点作为对象形状的代表。他们首先使用2D物体检测器进行初始对准。接下来，他们通过匹配锚点将SfM的点云与平均形状对齐，然后扭曲和细化以接近实际形状。他们的评估表明，该模型足以通过处理实例之间的大的形状变化来学习不同对象类别（如汽车，水果和键盘）的语义预测。
- 尽管以前的方法（Dame等人（2013）; Bao等（2013））试图将参数化形状模型拟合为输入数据，Haene等人（2014）模拟了一个对象的法线的局部分布。他们以空间变化各向异性平滑度项的形式提出了对象类特定形状。类似于Haene等人的多标签分割方法（2013），他们将重建划分为物体区域和支撑地面，然后仅在物体上应用形状，将优化引导到正确的形状。

- **Data-Driven**: Instead of modeling a semantic prior for each object explicitly, Wei et al. (2014) propose a data-driven regularization to transfer the shape information of the disparity or flow from semantically matched patches in the training database using the SIFT flow algorithm. They represent the shape information as the relative relationship of scene properties instead of absolute values. It is mainly for reusability of scene properties, such as modeling disparity of car independent of its position. They compare their data-driven prior against popular smoothness terms on Sintel and show improved performance while being comparable to state-of-the-art on KITTI.
- 数据驱动：而不是明确地为每个对象建模一个语义先验，Wei et al。 （2014）提出了一种数据驱动的正则化方法，使用SIFT流程算法从训练数据库中的语义匹配补丁中传输视差或流量的形状信息。 它们将形状信息表示为场景属性而不是绝对值的相对关系。 它主要用于场景属性的可重用性，如独立于其位置的汽车模型差异。 他们比较了他们的数据驱动先前与Sintel上的流行平滑度条款，并显示出改进的性能，同时与KITTI上的最先进技术相媲美。
