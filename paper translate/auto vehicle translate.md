## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
自动驾驶技术的计算机视觉：问题，数据和前沿技术

###Abstract 摘要
- Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. |As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. |While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists.|This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. |Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. |Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.
Keywords: Computer Vision, Autonomous Vehicles, Autonomous Vision
- 概述：人类见证了最近几年AI相关领域的惊人进步，如计算机视觉，机器学习和自动驾驶。<p>
然而任何一个快速发展的领域，保持领先或刚开始进入这些领域变得越来越难(业内人员难以跟上行业节奏或者业外人员难入行)。<p>
尽管已经(有人)发表了一些这方面的专题研究文章，但在自动驾驶技术中，计算机视觉的问题、数据和方法至今没有普遍的研究。<p>
对于这个话题，这篇论文试图通过提供对前沿技术的研究来减少这种缺口。我们的研究包括最相关的历史资料和当前最前沿的技术，包括识别、重建运动估测、追踪场景理解和端到端学习等。<p>
为完成这个目标，我们首先通过分类学对每一个方法进行分类，然后在一些具有挑战性的基础数据集上，如KITTI、ISPRS、MOT和Cityscapes上分析每一个方法在前沿技术上的表现<p>
此外，我们还讨论了一些开放问题和当前研究的挑战，为了轻松访问和适应缺失的参考，我们将提供一个具有主题和方法的驾驶交互平台，并提供额外信息和每篇论文的项目链接<p>
关键词：计算机视觉，自动驾驶，自主视觉

### previous 前言
- Since the first successful demonstrations in the 1980s (Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988)), great progress has been made in the field of autonomous vehicles. |Despite these advances, however, it is safe to believe that fully autonomous navigation in arbitrarily complex environments is still decades away. |The reason for this is two-fold: First, autonomous systems which operate in complex dynamic environments require artificial intelligence which generalizes to unpredictable situations and reasons in a timely manner. |Second, informed decisions require accurate perception, yet most of the existing computer vision systems produce errors at a rate which is not acceptable for autonomous navigation.
- 从20世纪80年代首次成功展示以来(Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988))(Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988))，自动驾驶技术领域已经取得了很大进展<p>
尽管有了这些进展，但在任意复杂环境中，实现完全自动驾驶仍然被认为需要几十年<p>
原因有两点：第一，在复杂的、动态的环境中运行的自动驾驶系统需要人工智能来归纳不可预测的情形和原因，给出及时的方法<p>
第二，信息的决策需要准确的感知，目前大多数已有的计算机视觉系统有一定的错误率，这是自动驾驶技术无法接受的

- In this paper, we focus on the second aspect which we call autonomous vision and investigate the performance of current perception systems for autonomous vehicles. |Towards this goal, we first provide a taxonomy of problems and classify existing datasets and techniques using this taxonomy, describing the pros and cons of each method. Second, we analyze the current state-of-the-art performance on several popular publicly available benchmarking datasets. |In particular, we provide a novel in-depth qualitative analysis of the KITTI benchmark which shows the easiest and most difficult examples based on the methods submitted to the evaluation server. |Based on this analysis, we discuss open research problems and challenges. To ease navigation, we also provide an interactive online tool which visualizes our taxonomy using a graph and provides additional information and links to project pages in an easily accessible manner. |We hope that our survey will become a useful tool for researchers in the field of autonomous vision and lowers the entry barrier for beginners by providing an exhaustive overview over the field.
- 在这篇论文中，我们关注第二个方面的问题，也就是自动驾驶视觉，同时调查最近的自动驾驶视觉中感知系统的表现<p>
为完成这个目标，我们首先给出了问题的分类，归类了已有的数据和可使用的技术，描述每种方法的优缺点。第二，我们在几个流行的公开数据集上分析了最近前沿成果的表现<p>
特别是我们给出一种KITTI基准的新的深入定性分析，这些分析展示了提交给评价服务器的方法中最简单和最困难的例子<p>
基于这些分析，我们讨论了开放的研究问题和挑战，为了简化学习，我们也给出一个在线交互式工具，用图像可视化了分类，并提供额外信息和一个简单可行的方法与项目页链接<p>
我们希望我们的研究能够成为自动驾驶领域研究人员的一个有用的工具，并通过透彻的概述，降低新人进入该领域的门槛

- There exist several other related surveys. Winner et al. (2015) explains in detail systems for active safety and driver assistance, considering both their structure and their function. |Their focus is to cover all aspects of driver assistance systems and the chapter about machine vision covers only the most basic concepts of the autonomous vision problem. |Klette (2015) provide an overview over vision-based driver assistance systems. They describe most aspects of the perception problem at a high level, but do not provide an in-depth review of the state-of-the-art in each task as we pursue in this paper. |Complementary to our survey, Zhu et al. (2017) provide an overview of environment perception for intelligent vehicles, focusing on lane detection, traffic sign/light recognition as well as vehicle tracking. |In contrast, our goal is to bridge the gap between the robotics, intelligent vehicles, photogrammetry and computer vision communities by providing an extensive overview and comparison which includes works from all fields.
- 目前也有一些其它相关的研究，Winner et al. (2015)详细地解释了主动安全性和驾驶辅助系统，同时考虑了它们的结构和功能<p>
这些研究注重覆盖辅助驾驶系统的所有方面，但关于机器视觉的章节只覆盖到了自动驾驶技术中最基础的概念。<p>
Klette (2015)提供了一个基于视觉的辅助驾驶系统的概述，他们描述了高层次感知问题的大部分方面，但并没有像我们在论文中追求的一样，给出在各种前沿任务中比较深入的评测<p>
Zhu et al. (2017)提出了智能汽车环境感知的概述，聚焦在车道检测，交通信号灯识别和机车追踪问题，这与我们的研究相互补充。<p>
相比较下，我们的目标是通过提供广泛的概述和比较，包括在这个领域所有的成果，在机器人、智能汽车、摄影测绘和计算机视觉之间建立起一座桥梁

### 1.History of Autonomous Driving 自动驾驶技术历史
- ####1.1. Autonomous Driving Projects 自动驾驶项目
- Many governmental institutions worldwide started various projects to explore intelligent transportation systems (ITS). The PROMETHEUS project started 1986 in Europe and involved more than 13 vehicle manufacturers, several research units from governments and universities of 19 European countries. |One of the first projects in the United States was Navlab Thorpe et al. (1988) by the Carnegie Mellon University which achieved a major milestone in 1995, by completing the first autonomous drive from Pittsburgh, PA and Sand Diego, CA. |After many initiatives were launched by universities, research centers and automobile companies, the U.S. government established the National Automated Highway System Consortium (NAHSC) in 1995. |Similar to the U.S., Japan established the Advanced Cruise-Assist Highway System Research Association in 1996 among many automobile industries and research centers to foster research on automatic vehicle guidance. |Bertozzi et al. (2000) survey many approaches to the challenging task of autonomous road following developed during these projects. They concluded that suffcient computing power is become increasingly available, but diffculties like reflections, wet road, direct sunshine, tunnels and shadows still make data interpretation challenging. |Thus, they suggested the enhancement of sensor capabilities. They also pointed out that the legal aspects related to the responsibility and impact of automatic driving on human passengers need to be considered carefully. |In summary, the automation will likely be restricted to special infrastructures and will be extended gradually.
- 世界各地的许多政府机构启动各式各样的项目来开发智能交通系统（ITS）。PROMETHEUS这个项目1986年在欧洲启动，包括超过13个交通工具生产商，当中的许多研究成员来自19个欧洲国家的政府和高校。<p>
美国的其中一个项目就是由卡耐基梅隆大学的Navlab Thorpe等人(1988)创建的。这个项目完成了第一次从Pittsburgh，PA,Sand Diego和CA的自动驾驶，在1995年是一个重要的里程碑。<p>
在许多大学，研究中心和自动驾驶公司的倡议下，美国政府在1995年成立了自动化公路系统联盟（NAHSC）。<p>
和美国一样，日本于1996年成立了高级巡航公路系统研究协会(Advanced Cruise-Assist Highway System Research Association)，包括各大自动驾驶公司和研究中心，来促进自动驾驶导航的研究。<p>
Bertozzi等人（2000）调查了许多具有挑战性的任务(通过这些项目发展的自动道路跟随),给出解决方法。他们得出结论，计算能力逐渐得到满足，但像反射，湿面潮湿，阳光直射，隧道和阴影这样的困难仍然使数据解释具有挑战性。<p>
因此，他们建议提高传感器性能，同时也指出，关系到自动驾驶对行人法律方面的责任和影响，应该认真的考虑<p>  
总之，自动化技术(发展)可能会受限于特殊的基础设施，然后再慢慢的普及开来。

- Motivated by the success of the PROMETHEUS projects to drive autonomously on highways, Franke et al. (1998) describe a real-time vision system for autonomous driving in complex urban traffic situations. |While highway scenarios have been studied intensively, urban scenes have not been addressed before. Their system included depth-based obstacle detection and tracking from stereo as well as a framework for monocular detection and recognition of relevant objects such as traffic signs.
- The fusion of several perception systems developed by Vis-Lab have led to several proto-type vehicles including ARGO Broggi et al. (1999), TerraMax Braid et al. (2006), and BRAiVE Grisleri & Fedriga (2010). |BRAiVE is the latest vehicle proto-type which is now integrating all systems that VisLab has developed so far. Bertozzi et al. (2011) demonstrated the robustness of their system at the VisLab Intercontinental Autonomous Challenge, a semi-autonomous drive from Italy to China. |The onboard system allows to detect obstacles, lane marking, ditches,berms and identify the presence and position of a preceding vehicle. The information produced by the sensing suite is used to perform different tasks such as leader-following and stop & go.
- PROMETHEUS项目可以实现在高速公路上自动驾驶，在这个成功的案例推动下，Franke等人描述了在复杂的城市交通场景下的自动驾驶的实时视觉系统。<p>
虽然在此之前公路场景情况已经有很多深入的研究，但城市场景却从未得到解决。他们的系统包括基于深度的障碍检测和立体追踪，以及针对相关物体（比如：交通信号）的单目检测和识别框架。<p>
- Vis-Lab发展的多种传感系统的融合促成了几款原型车包括ARGO Broggi（1999），TerraMax Braid（2006）和BRAiVE Grisleri & Fedriga（2010）的出现<p>
BRAiVE是目前VisLab开发的整合所有系统的最新车型。 Bertozzi等人（2011）在VisLab洲际自治挑战赛（VisLab Intercontinental Autonomous Challenge，意大利到中国的半自主驾驶）展示了其系统的稳健性（鲁棒性）。<p>
车载系统允许检测障碍物，标记车道、沟渠、护堤，并识别前方是否存在车辆和车辆位置。感应套件提供的信息被用于执行不同的任务，如(leader-following)和前进/停止。<p>?

- The PROUD project Broggi et al. (2015) slightly modified the BRAiVE prototype Grisleri & Fedriga (2010) to drive in urban roads and freeways open to regular traffic in Parma. |Towards this goal they enrich an openly licensed map with information about the maneuver to be managed (e.g. pedestrian crossing, traffic light, . . . ). |The vehicle was able to handle complex situations such as roundabouts, intersections, priority roads, stops, tunnels, crosswalks, traffic lights, highways, and urban roads without any human intervention.
- The V-Charge project Furgale et al. (2013) presents an electric automated car outfitted with close-to-market sensors. A fully operational system is proposed including vision-only localization, mapping, navigation and control. |The project supported many works on different problems such as calibration Heng et al. (2013, 2015), stereo H¨ane et al. (2014), reconstruction Haene et al. (2012, 2013, 2014), SLAM Grimmett et al.(2015) and free space detection H¨ane et al. (2015). In addition to these research objectives, the project keeps a strong focus on deploying and evaluating the system in realistic environments.
- PROUD的项目Broggi（2015）略微修改了BRAiVE原型Grisleri & Fedriga（2010）使得汽车可以在parma城市道路和高速公路的常规交通情况下开车。<p>
为了实现这一目标，他们丰富了一份公开授权的地图，其中包含有待完成的机动信息（比如行人过路，交通信号灯等）。<p>
该车辆能够在没有人为干涉的情况下处理复杂的场景，例如回旋处，交叉口，优先道路，站点，隧道，人行横道，交通信号灯，高速公路和城市道路。<p>
- V-Charge项目Furgale等人 （2013年）提供配备了近距离市场（close-to-market）传感器的电动自动车。提出了一个全面可使用的系统，包括视觉定位，映射，导航和控制。<p>
该项目解决了诸多困难比如，Heng et al. (2013, 2015)的校准问题, H¨ane(2014)的立体问题,Haene(2012, 2013, 2014)的重建问题, Grimmett(2015)的SLAM问题和 H¨ane(2015)的空白区域检测的问题。除了这些研究目标，该项目还非常重视在现实环境中部署和系统评估。

- Google started their self-driving car project in 2009 and completed over 1,498,000 miles autonomously until March 2016 in Mountain View, CA, Austin, TX and Kirkland, WA. |Different sensors (i.a. cameras, radars, LiDAR, wheel encoder, GPS) allow to detect pedestrians, cyclists, vehicles, road work and more in all directions. |According to their accident reports, Google’s self-driving cars were involved only in 14 collisions while 13 times were caused by others. In 2016, the project was split off to Waymo, an independent self-driving technology company.
- Tesla Autopilot is an advanced driver assistant system developed by Tesla which was first rolled out(推出) in 2015 with version of their software. The automation level of the system allows full automation but requires the full attention of the driver to take control if necessary. |From October 2016, all vehicles produced by Tesla were equipped with eight cameras, twelve ultrasonic sensors and a forward-facing radar to enable full self-driving capability.
- Google于2009年开始了自驾车项目，直到2016年3月完成了超过1,498,000英里的驾驶距离，在美国加利福尼亚州奥斯汀市的Mountain View，WA和柯克兰。<p>
不同的传感器（例如摄像机，雷达，LiDAR，车轮编码器，GPS）可以全方位的检测行人，骑自行车的人，车辆，道路工作等等。<p>
据他们的事故报道，Google的自动驾驶车只涉及14次碰撞，13次是由别人造成的。 在2016年，这个项目分引入到了一家独立的自动驾驶技术公司Waymo。<p>
- Tesla Autopilot是由特斯拉开发的高级驾驶员辅助系统，该系统于2015年第一次推出其视觉软件。系统的自动化级别允许完全的自动化，但是仍然需要 要求驾驶员集中注意来控制。<p>
从2016年10月起，特斯拉生产的所有车辆配备了8台摄像机，12台超声波传感器和一个前置雷达，以实现全自动驾驶能力。<p>

- **Long Distance Test Demonstrations**: In 1995 the team within the PROMETHEUS project Dickmanns et al. (1990); Franke et al. (1994); Dickmanns et al. (1994) performed the first autonomous long-distance drive from Munich, Germany, to Odense, Denmark, at velocities up to 175 km/h with about 95% autonomous driving. |Similarly, in the U.S. Pomerleau & Jochem (1996) drove from Washington DC to San Diego in the ’No hands across America’ tour with 98% automated steering yet manual longitudinal control.
- In 2014, Ziegler et al. (2014) demonstrated a 103 km ride from Mannheim to Pforzheim Germany, known as Bertha Benz memorial route, in nearly fully autonomous manner. |They present an autonomous vehicle equipped with close-to-production sensor hardware. Object detection and free-space analysis is performed with radar and stereo vision. Monocular vision is used for traffic light detection and object classification. |Two complementary vision algorithms, point feature based and lane marking based, allow precise localization relative to manually annotated digital road maps. They concluded that even thought the drive was successfully completed the overall behavior is far inferior to the performance level of an attentive human driver.
- 长距离测试演示：1995年，PROMETHEUS项目里Dickmanns（1990）、Franke（1994）、Dickmanns（1994年）的团队演示了从德国慕尼黑（Munich）到丹麦欧登塞（Odense）进行的第一次自动长途驾驶，速度达175公里/小时，其中约95％为自主驾驶。<p>
同样，在美国Pomerleau和Jochem（1996）在‘No hands across from America ???’中从华盛顿特区开往圣地亚哥，整个行程中有98％的自动驾驶和偶尔的手动纵向控制。<p>
- 2014年，Zieglar（2014）以近乎完全自动的方式，展示了从曼海姆（Mannheim）到德国普福尔茨海姆（Pforzheim Germany）的103km的骑行，也就是众人所熟知的Bertha Benz纪念路线。<p>
他们展示了一种装配有接近生产(close-to-production)的传感器硬件的自动驾驶车辆。由雷达radar和立体视觉来进行物体检测和空白区域分析。单目视觉用来检测交通信号灯和目标分类。<p>
两种互补的算法，基于点特征和基于场景标记，允许相对于手动注释的数字路线图进行精确定位。他们得出结论，甚至认为自动驾驶虽然成功完成了，但是整体行为远远达不到细心的驾驶司机的水平。

- Recently, Bojarski et al. (2016) drove autonomously 98% of the time from Holmdel to Atlantic Highlands in Monmouth County NJ as well as 10 miles on the Garden State Parkway without intervention. |Towards this goal, a convolutional neural network which predicts vehicle control directly from images is used in the NVIDIA DRIVETM PX self-driving car. The system is discussed in greater detail in Section 11.
- While all aforementioned performed impressively, the general assumption of precisely annotated road maps as well as prerecorded maps for localization demonstrates that autonomous systems are still far from human capabilities. |Most importantly, robust perception from visual information but also general artificial intelligence are required to reach human level reliability and react safely even in complex innercity situations.
- 最近，Bojarski（2016）从霍尔姆德尔（Holmdel）到新泽西州蒙茅斯县（Monmouth）的大西洋高原，以及在花园州立大道没有任何干扰的自动行驶了10英里，其中98%是在自动驾驶。<p>
为了实现这一目标，在NVIDIA DRIVETM PX自动驾驶车中使用了一种从图像直接预测车辆控制的卷积神经网络。该系统在第11节中有更详细的讨论。<p>
- 虽然所有上述表现令人印象深刻，但精确注释路线图的一般假设，以及用于定位的预先载入的地图证实了自主性系统仍然差强人意。<p>
最重要的是，这不仅需要视觉信息的强大的感知，也需要一般的人工智能达到和人一样的可靠性，并且在复杂的城市情况下也能安全地做出反应。<p>

- 1.2. Autonomous Driving Competitions 自动驾驶竞赛
- The European Land Robot Trial (ELROB) is a demonstration and competition of unmanned systems in realistic scenarios and terrains, focusing mainly on military aspects such as reconnaissance and surveillance, autonomous navigation and convoy transport. In contrast to autonomous driving challenges, ELROB scenarios typically include navigation in rough terrain.
- The first autonomous driving competition focusing on road scenes (though primarily dirt roads) has been initiated by the American Defense Advanced Research Projects Agency (DARPA) in 2004. The DARPA Grand Challenge 2004 offered a prize money of $1 million for the team first finishing a 150 mile route which crossed the border from California to Nevada. |However, none of the robot vehicles completed the route. One year later, in 2005, DARPA announced a second edition of its challenge with 5 vehicles successfully completing the route (Buehler et al.(2007)). The third competition of the DARPA Grand Challenge, known as the Urban Challenge (Buehler et al. (2009)), took place on November 3, 2007 at the site of the George Air Force Base in California. The challenge involved a 96 km urban area course where traffic regulations had to be obeyed while negotiating with other vehicles and merging into traffic.
- The Grand Cooperative Driving Challenge (GCDC, see also Geiger et al. (2012a)), a competition focusing on autonomous cooperative driving behavior was held in Helmond, Netherlands in 2011 for the first time and in 2016 for a second edition. During the competition, teams had to negotiate convoys, join convoys and lead convoys. The winner was selected based on a system that assigned points to randomly mixed teams.
- European Land Robot Trial （ELROB）是现实场景和地形中无人系统的示范与竞赛，主要集中在军事方面，如侦察监视，自动导航和车队运输。与自主驾驶挑战相反，ELROB场景通常包括崎岖地形的导航。<p>
- 2004年，美国国防高级研究计划署（DARPA）发起了第一个专注于道路场景（主要是泥土路）的自动驾驶比赛。挑战赛提供了100万美元的奖金给首先完成从加利福尼亚州内华达州过境的150英里的路线。<p>
然而，机器人车辆都没有完成路线。 一年后，DARPA公布了第二版的挑战，5辆车顺利完成了路线（Buehler（2007））。DARPA大挑战赛的第三场比赛，被称为城市挑战赛（Buehler（2009）），于2007年11月3日在乔治航空加利福尼亚州的基地。<p>
这个挑战涉及到一个96公里的城市地区航线，在这段路程中车辆在对其他车辆进行判断并汇合车流时，必须遵守交通法规。
- 专注于自动合作驾驶行为的大型合作驾驶挑战（GCDC，Geiger et al（2012a））在荷兰赫尔蒙德（Helmond）举行,2011年首次，2016年第二次。在比赛中，团队需要判断，加入和引导车队。获胜者是基于给随机混合团队分配点数的系统选出来的。<p>

###2.datasets & Benchmarks 数据集和基准
- Datasets have played a key role in the progress of many research fields by providing problem specific examples with ground truth. They allow quantitative evaluation of approaches providing key insights about their capacities and limitations. |In particular, several of these datasets Geiger et al. (2012b); Scharstein & Szeliski (2002); Baker et al. (2011); Everingham et al. (2010); Cordts et al. (2016) also provide online evaluation servers which allow for a fair comparison on held-out test sets and provide researchers in the field an up-to-date overview over the state-of-the-art. |This way, current progress and remaining challenges can be easily identified by the research community. In the context of autonomous vehicles, the KITTI dataset Geiger et al. (2012b) and the Cityscapes dataset Cordts et al.(2016) have introduced challenging benchmarks for reconstruction, motion estimation and recognition tasks, and contributed to closing the gap between laboratory settings and challenging real-world situations. |Only a few years ago, datasets with a few hundred annotated examples were considered sufficient for many problems. The introduction of datasets with many hundred to thousands of labeled examples, however, has led to spectacular breakthroughs in many computer vision disciplines by training high-capacity deep models in a supervised fashion. |However, collecting a large amount of annotated data is not an easy endeavor, in particular for tasks such as optical flow or semantic segmentation. This initiated a collective effort to produce that kind of data in several areas by searching for ways to automate the process as much as possible such as through semi-supervised learning or synthesization.
- 数据集在许多研究领域进展方面发挥了关键作用，提供了真实的(ground truth)问题特例。它们允许通过提供有关其能力与局限的核心信息，数据集还可以对方法进行量化评估。<p>
- 特别地，这些数据集中的几个比如Geiger（2012b）;Scharstein＆Szeliski（2002）; Baker（2011）;Everinghamet al（2010）; Cordts（2016）也提供在线评估服务器允许在延期测试（held-out）中进行公平的比较，而且为该领域的研究人员提供更新的目前最好的算法。<p>
- 这种方式可以让研究团队很容易地确定目前的进展和剩下的挑战。在自主车辆的环境中，KITTI数据集Geiger（2012b）和Cityscapes数据集Cordts （2016）为重建、运动估计和识别任务引入了挑战性的基准，因此缩小了实验室设置与挑战现实世界的情况之间的差距。<p>
- 几年前，有数百个注释例子的数据集对于解决很多问题是足够的。然而，有数百到数千个有标签的例子的数据集的引入，通过以监督的方式训练大容量深度模型，已经使得许多计算机视觉学科的重大突破。<p>
- 然而，收集大量的注释数据不是一个容易的事情，特别是对于诸如光流或者语义分割的任务。这使得集体努力通过搜索尽可能多的方式来自动化过程，例如通过半监督学习或合成，从而在多个领域产生了这种数据。<p>

- 2.1. Real-World Datasets 真实数据集
- While several algorithmic aspects can be inspected using synthetic data, real-world datasets are necessary to guarantee performance of algorithms in real situations. For example, algorithms employed in practice need to handle complex objects and environments while facing challenging environmental conditions such as direct lighting, reflections from specular surfaces, fog or rain. The acquisition of ground truth is often labor intensive because very often this kind of information cannot be directly obtained with a sensor but requires tedious manual annotation. |For example, (Scharstein & Szeliski (2002),Baker et al. (2011)) acquire dense pixel-level annotations in a controlled lab environment whereas Geiger et al. (2012b); Kondermann et al. (2016) provide sparse pixel-level annotations of real street scenes using a LiDAR laser scanner.
- Recently, crowdsourcing with Amazon’s Mechanical Turk9 have become very popular to create annotations for large scale datasets, e.g., Deng et al. (2009); Lin et al. (2014); Leal-Taix´e et al. (2015); Milan et al. (2016). However, the annotation quality obtained via Mechanical Turk is often not sufficient to be considered as reference and significant efforts in post-processing and cleaning-up the obtained labels is typically required. |In the following, we will first discuss the most popular computer vision datasets and benchmarks addressing tasks relevant to autonomous vision. Thereafter, we will focus on datasets particularly dedicated to autonomous vehicle applications.
- 虽然可以使用合成数据检查几个算法方面，但实际数据集对于确保算法在实际情况下的性能是必要的。例如，在实践中使用的算法需要处理复杂的对象和环境，同时面对挑战性的环境条件，例如直接照明，镜面反射，雾或雨。获取ground truth通常是劳动密集型的，因为这种信息通常不能用传感器直接获得，而是需要繁琐的手动注释。<p>
- 例如，（Scharstein＆Szeliski（2002），Baker（2011））在受控实验室环境中获得了密集的像素级注释，而Geiger等人（2012B）; Kondermann等人（2016）使用LiDAR激光扫描仪提供实际街景场景的稀疏像素级注解。<p>
- 最近，亚马逊的Mechanical Turk的众包已经变得非常受欢迎，为大型数据集创建注释，例如Deng（2009）;Lin（2014）; Leal-Taix'e（2015）; Milan（2016）。然而，通过Mechanical Turk获得的注释质量通常不太合适被认为是参考，并且通常需要在后处理中最初的重大努力和清理所获得的标签中也是非常需要的。<p>
- 在下文中，我们将首先讨论最流行的计算机视觉数据集和基准，以解决与自主视觉相关的任务。此后，我们将专注于数据集，尤其致力于自动驾驶车辆的应用。

- **Stereo and 3D Reconstruction**: The Middlebury stereo benchmark introduced by Scharstein & Szeliski (2002) provides several multi-frame stereo data sets for comparing the performance of stereo matching algorithms. |Pixel-level ground truth is obtained by hand labeling and reconstructing planar components in piecewise planar scenes. Scharstein & Szeliski (2002) further provide a taxonomy of stereo algorithms that allows the comparison of design decisions and a test bed for quantitative evaluation. |Approaches submitted to their benchmark website are evaluated using the root mean squared error and the percentage of bad pixels between the estimated and ground truth disparity maps.
- Scharstein & Szeliski (2003) and Scharstein et al. (2014) introduced novel datasets to the Middlebury benchmark comprising more complex scenes and including ordinary objects like chairs, tables and plants. In both works a structured lighting system was used to create ground truth. |For the latest version Middlebury v3, Scharstein et al. (2014) generate highly accurate ground truth for high-resolution stereo images with a novel technique for 2D subpixel correspondence search and self-calibration of cameras as well as projectors. This new version achieves significantly higher disparity and rectification accuracy than those of existing datasets and allows a more precise evaluation. An example depth map from the dataset is illustrated in Figure 1.
- The Middlebury multi-view stereo (MVS) benchmark11 by Seitz et al. (2006) is a calibrated multi-view image dataset with registered ground truth 3D models for the comparison of MVS approaches. The benchmark played a key role in the advances of MVS approaches but is relatively small in size with only two scenes. |In contrast, the TUD MVS dataset12 by Jensen et al. (2014) provides 124 different scenes that were also recorded in controlled laboratory environment. Reference data is obtained by combining structured light scans from each camera position and the resulting scans are very dense, each containing 13.4 million points on average. For 44 scenes the full 360 degree model was obtained by rotation and scanning four times with 90 degree intervals. In contrast to the datasets so far, Sch¨ops et al. (2017) provide scenes that are not carefully staged in a controlled laboratory environment and thus represent real world challenges. Sch¨ops et al. (2017) recorded high-resolution DSLR imagery as well as synchronized low-resolution stereo videos in a variety of indoor and outdoor scenes. A high-precision laser scanner allows to register all images with a robust method. The high-resolution images enable the evaluation of detailed 3D reconstruction while the low-resolution stereo images are provided to compare approaches for mobile devices.
- 立体与 3D 重建类数据集：由Scharstein＆Szeliski（2002）引入的Middlebury立体声基准测试仪提供了多个立体声数据集，用于比较立体匹配算法的性能。<p>
- 通过在分段平面场景中手工标记和重建平面构成获得像素级地面真值。Scharstein和Szeliski（2002）进一步提供立体声算法的分类法，允许通过比较设计决策和测试台来进行定量评估。<p>
- 使用均方误差以及估计值和地面真实视差图之间坏像素的百分比来评估提交给其基准网站的方法。<p>
- Scharstein & Szeliski (2003) 和 Scharstein et al. (2014)为Middlebury基准引入了一种新颖的数据集，这个数据及包含更多复杂的场景和普通的物体，比如椅子、桌子、植物等对象。在这两个工作中，均使用一个结构化的照明系统来创造地面实况。<p>
- 对于最新版本的Middlebury v3，Scharstein（2014）采用新颖的2D子像素对应搜索和相机自动校准技术以及投影机为高分辨率立体图像生成高精度的地面实况。与现有数据集相比，该新版本的差异和整改精度明显提高，可以进行更精确的评估。 Figure 1是来自数据集的示例深度图：
Seitz等人的Middlebury多视点立体声（MVS）基准测试（2006）是注册地面真相3D模型用于比较MVS方法一种校准的多视图图像数据集。基准测试在MVS方法的进步中发挥了关键作用，但只有两个场景，尺寸相对较小。相比之下，Jensen等人的TUD MVS数据集（2014年）提供了124个不同的场景，这些场景也被记录在受控实验室环境中。 参考数据通过组合来自每个摄像机位置的结构光扫描获得，并且所得到的扫描非常密集，平均每个包含13.4million个点。对于44个场景，通过以90度的间隔旋转和扫描四次获得完整的360度模型。 与迄今为止的数据集相比，Sch¨ops等人（2017年）提供了在受控实验室环境中未仔细分级的场景，从而代表了现实世界的挑战。Sch¨ops et al. (2017) 录制了高分辨率DSLR单反相机图像以及各种室内和室外场景中同步的低分辨率立体视频。 高精度激光扫描仪允许以强大的方法注册所有图像。高分辨率图像可以评估详细的3D重建，同时提供低分辨率立体图像来比较移动设备的方法。

- **Optical Flow**: The Middlebury flow benchmark13 by Baker et al. (2011) provides sequences with non-rigid motion, synthetic sequences and a subset of the Middlebury stereo benchmark sequences (static scenes) for the evaluation of optical flow methods. For all non-rigid sequences, ground truth flow is obtained by tracking hidden fluorescent textures sprayed onto the objects using a toothbrush. The dataset comprises eight different sequences with eight frames each. Ground truth is provided for one pair of frames per sequence.
- Besides the limited size, real world challenges like complex structures, lighting variation and shadows are missing as the dataset necessitates laboratory conditions which allow for manipulating the light source between individual captures. In addition, it only comprises very small motions of up to twelve pixels which do not admit the investigation of challenges provided by fast motions. Compared to other datasets, however, the Middlebury dataset allows to evaluate sub-pixel precision since it provides very accurate and dense ground truth. Performance is measured using the angular error (AEE) and the absolute end point error (EPE) between the estimated flow and the ground truth.
- Janai et al. (2017) present a novel optical flow dataset comprising of complex real world scenes in contrast to the laboratory setting in Middlebury. High-speed video cameras are used to create accurate reference data by tracking pixel through densely sampled space-time volumes. This method allows to acquire optical flow ground truth in challenging everyday scenes in an automatic fashion and to augment realistic effects such as motion blur to compare methods in varying conditions. Janai et al. (2017) provide 160 diverse real-world sequences of dynamic scenes with a significantly larger resolution (1280X1024 Pixels) than previous optical datasets and compare several state of-the-art optical techniques on this data.
光流类数据集：Baker等人的“Middlebury流量标准” （2011）提供了具有非刚性运动序列，合成序列和Middlebury立体声基准序列（静态场景）的子集的序列，用于评估光流方法。 对于所有非刚性序列，通过使用toothbrush牙刷追踪在物体上喷洒的隐藏的荧光纹理来获得地面真实流。 数据集包含八个不同的序列，每个序列具有八个帧。 每个序列提供一对帧的地面实况。
除了有限的大小之外，由于数据集需要实验室条件，允许在各个捕获之间操纵光源，所以缺少像复杂结构，照明变化和阴影这样的真实世界挑战。 此外，它只包含最多十二个像素的非常小的运动，不承认对快速运动提供的挑战的调查。 然而，与其他数据集相比，Middlebury数据集可以评估子像素精度，因为它提供了非常精确和密集的地面实例。 使用角度误差（AEE）和估计流量与地面实数之间的绝对终点误差（EPE）来测量性能。
Janai等人 （2017）提出了一个新颖的光流数据集，其中包括复杂的现实世界场景，与Middlebury的实验室设置相反。 高速视频摄像机用于通过密集采样的时空容量跟踪像素来创建精确的参考数据。 该方法允许以自动方式在挑战性的日常场景中获取光流场地真相，并且增加诸如运动模糊的现实效果以在不同条件下比较方法。  Janai等人 （2017年）提供了160个不同的现实世界动态场景序列，具有比以前的光学数据集显着更大的分辨率（1280x1024像素），并比较了这些数据的几种最先进的光学技术。

- **Object Recognition and Segmentation**: The availability of large-scale, publicly available datasets such as ImageNet (Denget al. (2009)), PASCAL VOC (Everingham et al. (2010)), Microsoft COCO (Lin et al.(2014)), Cityscapes (Cordts et al.(2016)) and TorontoCity (Wang et al. (2016)) have had a major impact on the success of deep learning in object classification, detection, and semantic segmentation tasks.
- The PASCAL Visual Object Classes (VOC) challenge14 by Everingham et al. (2010) is a benchmark for object classification, object detection, object segmentation and action recognition. It consists of challenging consumer photographs collected from Flickr with high quality annotations and contains large variability in pose, illumination and occlusion. Since its introduction, the VOC challenge has been very popular and was yearly updated and adapted to the needs of the community until the end of the program in 2012. Whereas the first challenge in 2005 had only 4 dierent classes, 20 dierent object classes
were introduced in 2007. Over the years, the benchmark grew in size reaching a total of 11,530 images with 27,450 ROI annotated objects in 2012.
- In 2014, Lin et al. (2014) introduced the Microsoft COCO dataset15 for the object detection, instance segmentation and contextual reasoning. They provide images of complex everyday scenes containing common objects in their natural context. The dataset comprises 91 object classes, 2.5 million annotated instances and 328k images in total. Microsoft COCO is significantly larger in the number of instances per class than the PASCAL VOC object segmentation benchmark. All objects are annotated with per-instance segmentations in an extensive crowd worker eort. Similar to PASCAL VOC, the intersection-overunion metric is used for evaluation.
对象识别与分割类数据集
大量的公开数据集，如ImageNet（Deng等人（2009）），PASCAL VOC（Everingham等（2010）），Microsoft COCO（Lin等人（2014）），Cityscapes（Cordts （2016））和TorontoCity（Wang等人（2016年））对物体分类，目标检测和语义分割任务中深入学习的成功产生了重大影响。
由Everingham等人（2010）提供的PASCAL视觉对象类（VOC）挑战是对象分类，物体检测，物体分割和动作识别的基准。它由具有高质量标注的Flickr收集的有挑战性的消费者照片组成，并且包含姿势，照明和遮挡的大变化。 自从介绍以来，VOC的挑战一直很受欢并且逐年更新并适应社区的需求直到2012年计划结束。而2005年的第一个挑战只有4个不同的类，2007年引入了20个不同的对象类。多年来，基准规模在2012年达到总共11,530张图像当中共有27,450张ROI注释物体。
2014年，Lin等 （2014）介绍了Microsoft COCO数据集，用于物体检测，实例分割和上下文推理。 它们在自然环境中提供包含常见对象的复杂日常场景的图像。 数据集总共包括91个对象类，250万个注释实例和328k个图像。 Microsoft COCO在PASCAL VOC对象分割基准测试中每个类的实例数显著增加。 所有物体都在广泛的人群工作人员的努力下对每个实例进行标注。 与PASCAL VOC类似，IOU度量用于评估。

- **Tracking**: Leal-Taix´e et al. (2015); Milan et al. (2016) present the MOTChallenge16 which addresses the lack of a centralized benchmark for multi object tracking. The benchmark contains 14 challenging video sequences in unconstrained environments filmed with static and moving cameras and subsumes many existing multi-object tracking benchmarks such as PETS (Ferryman & Shahrokni (2009)) and KITTI (Geiger et al.(2012b)). The annotations for three object classes are provided: moving or standing pedestrians, people that are not in an upright position and others. They use the two popular tracking measures, Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) introduced by Stiefelhagen et al. (2007) for the evaluation of the approaches. Detection ground truth provided by the authors allows to analyze the performance of tracking systems independent of a detection system. Methods using a detector and methods using the detection ground truth can be compared separately on their website. - Aerial Image Datasets: The ISPRS benchmark17 (Rottensteiner et al. (2013, 2014)) provides data acquired by airborne sensors for urban object detection and 3D building reconstruction and segmentation. It consists of two datasets: Vaihingen and Downtown Toronto. The object classes considered in the object detection task are building, road, tree, ground, and car. The Vaihingen dataset provides three areas with various object classes and a large test site for road detection algorithms. The Downtown Toronto dataset covers an area of about 1.45 km2 in the central area of Toronto, Canada. Similarly to Vaihingen, there are two smaller areas for object extraction and building reconstruction, as well as one large area for road detection. For each test area, aerial images with orientation parameters, digital surface model (DSM), orthophoto mosaic and airborne laser scans are provided. The quality of the approaches is assessed using several metrics for detection and reconstruction. In both cases completeness, correctness and quality is assessed on a per-area level and a per-object level.
追踪：Leal-Taix'e（2015），Milan（2016）提出了MOTChallenge16，解决了多对象跟踪缺乏集中的基准。该基准测试包含14个具有静态和移动摄像机拍摄的无约束环境的挑战性视频序列，并包含许多现有的多对象跟踪基准，如PETS（Ferryman＆Shahrokni（2009））和KITTI（Geiger等（2012b））。提供三个对象类的注释：移动或站立的行人，不在直立位置的人等。他们使用Stiefelhagen等人介绍的两个流行的跟踪措施，多目标跟踪精度（MOTA）和多对象跟踪精度（MOTP）。 （2007）评估方法。作者提供的检测基准真实性可以分析独立于检测系统的跟踪系统的性能。使用检测器的方法和使用检测基准的方法可以在其网站上单独进行比较。 - 空中图像数据集：ISPRS benchmark17（Rottensteiner等（2013，2014））提供了用于城市物体检测和3D建筑重建和分割的机载传感器获取的数据。它包括两个数据集：Vaihingen和多伦多市区。对象检测任务中考虑的对象类是建筑，道路，树木，地面和汽车。 Vaihingen数据集提供了三个不同对象类别的区域和一个用于道路检测算法的大型测试站点。多伦多市中心数据集在加拿大多伦多的中部地区面积约1.45平方公里。与Vaihingen类似，有两个较小的对象提取和建筑重建区域，以及一个大面积的道路检测。对于每个测试区域，提供具有取向参数，数字表面模型（DSM），正射影像马赛克和机载激光扫描的航空图像。使用检测和重建的几个度量来评估方法的质量。在这两种情况下，完整性，正确性和质量都在每个面积水平和每个物体水平上进行评估。

- **Autonomous Driving**: In 2012, Geiger et al. (2012b, 2013) have introduced the KITTI Vision Benchmark18 for stereo, optical flow, visual odometry/SLAM and 3D object detection (Figure). The dataset has been captured from an autonomous driving platform and comprises six hours of recordings using high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and high-precision GPS/IMU inertial navigation system. The stereo and optical flow benchmarks derived from this dataset comprise 194 training and 195 test image pairs at a resolution of 1280  376 pixels and sparse ground truth obtained by projecting accumulated 3D laser point clouds onto the image. Due to the limitations of the rotating laser scanner used as reference sensor, the stereo and optical flow benchmark is restricted to static scenes with camera motion.
- To provide ground truth motion fields for dynamic scenes, Menze & Geiger (2015) have annotated 400 dynamic scenes, fitting accurate 3D CAD models to all vehicles in motion in to order to obtain flow and stereo ground truth for these objects. The KITTI flow and stereo benchmarks use the percentage of erroneous (bad) pixels to assess the performance of the submitted methods. Additionally, Menze & Geiger (2015) combined the stereo and flow ground truth to form a novel 3D scene flow benchmark. For evaluating scene flow, they combine classical stereo and optical flow measures.
- The visual odometry / SLAM challenge consists of 22 stereo sequences, with a total length of 39.2 km. The ground truth pose is obtained using GPS/IMU localization unit which was fed with RTK correction signals. The translational and rotational error averaged over a particular trajectory length is considered for evaluation.
- For the KITTI object detection challenge, a special 3D labeling tool has been developed to annotate all 3D objects with 3D bounding boxes for 7481 training and 7518 test images. The benchmark for the object detection task was separated into a vehicle, pedestrian and cyclist detection tasks, allowing to focus the analysis on the most important problems in the context of autonomous vehicles. Following PASCAL VOC Everingham et al. (2010), the intersection-over-union (IOU) metric is used for evaluation. For an additional evaluation, this metric has been extended to capture both 2D detection and 3D orientation estimation performance. A true 3D evaluation is planned to be released shortly.
- The KITTI benchmark was extended by Fritsch et al. (2013) to the task of road/lane detection. In total, 600 diverse training and test images have been selected for manual annotation of road and lane areas. Mattyus et al. (2016) used aerial images to enhance the KITTI dataset with fine grained segmentation categories such as parking spots and sidewalk as well as the number and location of road lanes. The KITTI dataset has established itself as one of the standard benchmarks in all of the aforementioned tasks, in particular in the context of autonomous driving applications.
- 2012年，Geiger等（2012b，2013）推出了用于立体声，光流，视觉测距/ SLAM和3D物体检测的KITTI Vision Benchmark18（图）。数据集已从自主驾驶平台捕获，包括使用高分辨率彩色和灰度立体相机的六小时录音，Velodyne 3D激光扫描仪和高精度GPS / IMU惯性导航系统。从该数据集派生的立体声和光流基准测试包括194次训练和195个测试图像对，分辨率为1280？通过将累积的3D激光点云投影到图像上获得的376个像素和稀疏的地面真实。由于用作参考传感器的旋转激光扫描仪的局限性，立体声和光学流量基准仅限于具有摄像机运动的静态场景。
- 为了为动态场景提供地面真相运动场，Menze＆Geiger（2015）已​​经注明了400个动态场景，将精确的3D CAD模型适用于所有运动的车辆，以获得这些物体的流动和立体声地面实况。 KITTI流量和立体声基准使用错误（不良）像素的百分比来评估提交的方法的性能。此外，Menze＆Geiger（2015）结合了立体声和流动地面的真相，形成了一种新颖的3D场景流动基准。为了评估场景流，它们结合了古典立体声和光学流量测量。
- 视觉测距/ SLAM挑战包括22个立体声序列，总长39.2公里。使用馈送有RTK校正信号的GPS / IMU定位单元获得地面真实姿势。考虑在特定轨迹长度上平均的平移和旋转误差进行评估。
- 对于KITTI对象检测挑战，已经开发了一种特殊的3D标签工具，用于通过3D边界框注释所有3D对象，用于7481个训练和7518个测试图像。物体检测任务的基准被分为车辆，行人和骑车人员检测任务，允许将分析集中在自主车辆的上下文中最重要的问题。按照PASCAL VOC Everingham等（2010），交叉联合（IOU）度量用于评估。为了进一步评估，该指标已扩展到捕获2D检测和3D定向估计性能。计划即将发布真正的3D评估。
- 由Fritsch等人扩展了KITTI基准。 （2013年）到道路/车道检测任务。总共选择了600多种不同的训练和测试图像，用于手动注释道路和车道区域。 Mattyus等人（2016）使用航空图像来增强KITTI数据集，并提供诸如停车位和人行道之类的细粒度细分类别，以及道路的数​​量和位置。 KITTI数据集已经成为所有上述任务的标准基准之一，特别是在自主驾驶应用的上下文中。

- Complementary to other datasets, the HCI benchmark19 proposed in Kondermann et al. (2016) specifically includes realistic, systematically varied radiometric and geometric challenges. Overall, a total of 28,504 stereo pairs with stereo and flow ground truth is provided. In contrast to previous datasets, ground truth uncertainties have been estimated for all static regions. The uncertainty estimate is derived from pixel-wise error distributions for each frame which are computed based on Monte Carlo sampling. Dynamic regions are manually masked out and annotated with approximate ground truth for 3,500 image pairs.
- The major limitation of this dataset is that all sequences were recorded in a single street section, thus lacking diversity. On the other hand, this enabled better control over the content and environmental conditions. In contrast to the mobile laser scanning solution of KITTI, the static scene is scanned only once using a high-precision laser scanner in order to obtain a dense and highly accurate ground truth of all static parts. Besides the metrics used in KITTI and Middlebury, they use semantically meaningful performance metrics such as edge fattening and surface smoothness for evaluation Honauer et al. (2015). The HCI benchmark is rather new and not established yet but the controlled environment allows to simulate rarely occurring events such as accidents which are of great interest in the evaluation of autonomous driving systems.
- The Caltech Pedestrian Detection Benchmark20 proposed by Dollar et al. (2009) provides 250,000 frames of sequences recorded by a vehicle while driving through regular traffic in an urban environment. 350,000 bounding boxes and 2,300 unique pedestrians were annotated including temporal correspondence between bounding boxes and detailed occlusion labels. Methods are evaluated by plotting the miss rate against false positives and varying the threshold on detection confidence.
- The Cityscapes Dataset21 by Cordts et al. (2016) provides a benchmark and large-scale dataset for pixel-level and instancelevel semantic labeling that captures the complexity of realworld urban scenes. It consists of a large, diverse set of stereo video sequences recorded in streets of different cities. High quality pixel-level annotations are provided for 5,000 images while 20,000 additional images have been annotated with coarse labels obtained using a novel crowd sourcing platform. For two semantic granularities, i.e., classes and categories, they report mean performance scores and evaluate the intersection-overunion metric at instance-level to assess how well individual instances are represented in the labeling.
- The TorontoCity benchmark presented byWang et al. (2016) covers the greater Toronto area with 712 km2 of land, 8,439 km of road and around 400,000 buildings. The benchmark covers a large variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling and scene type classification. The dataset was captured from airplanes, drones, and cars driving around the city to provide different perspectives.
- 与其他数据集的补充，在Kondermann等人提出的HCI基准19。 （2016）具体包括现实的，有系统地变化的辐射和几何挑战。总的来说，共提供了28,504立体声和流动地面真相的立体声对。与以前的数据集相比，所有静态区域的地面真实不确定度已被估计。不确定性估计是根据基于蒙特卡洛取样计算的每个帧的像素误差分布得出的。手动屏蔽动态区域并用3,500个图像对的近似地面实例进行注释。
- 这个数据集的主要限制是所有序列记录在单个街区，因此缺乏多样性。另一方面，这能够更好地控制内容和环境条件。与KITTI的移动激光扫描解决方案相比，静态场景仅使用高精度激光扫描仪扫描一次，以获得所有静态部件的致密和高精度的地面实况。除了KITTI和Middlebury使用的指标之外，他们使用语义有意义的性能指标，如边缘育肥和表面平滑度评估Honauer等。 （2015年）。 HCI基准相当新，尚未建立，但受控环境允许模拟很少发生的事件，例如对自主驾驶系统的评估感兴趣的事故。
- 美国加州大学提出的Caltech行人检测基准20 （2009）提供了车辆记录的25万帧序列，同时在城市环境中经常进行交通。包括350,000个边界框和2,300个独特的行人，包括边界框和详细遮挡标签之间的时间对应关系。通过绘制误差率与误报率并在检测置信度上改变阈值来评估方法。
- 由Cordts等人的Cityscapes Dataset21 （2016）为像素级和实例级语义标注提供了基准和大型数据集，捕捉到现实城市场景的复杂性。它由不同城市的街道上记录的大型，多样化的立体视频序列组成。为5,000张图像提供了高质量的像素级注释，而使用新颖的人群采购平台获得的粗略标签已经注明了20,000张附加图像。对于两个语义粒度，即类别和类别，他们报告平均绩效评分，并评估实例级别的交叉点平均度量，以评估在标签中表示个体实例的程度。
- Wang等人提出的多伦多城市基准（2016年）覆盖多伦多地区，712平方公里的土地，8,439公里的道路和大约40万个建筑物。该基准涵盖了建筑高度估计（重建），道路中心线和路缘提取，建筑物实例分割，建筑轮廓提取，语义标注和场景类型分类等各种任务。数据集被从飞机，无人驾驶飞机和汽车驾驶在城市周围捕获，以提供不同的观点。

- **Long-Term Autonomy**: Several datasets such as KITTI or Cityscapes focus on the development of algorithmic competences for autonomous driving but do not address challenges of long-term autonomy, as for examples environmental changes over time. To address this problem, a novel dataset for autonomous driving has been presented by Maddern et al. (2016). They collected images, LiDAR and GPS data while traversing 1,000 km in central Oxford in the UK during one year. This allowed them to capture large variations in scene appearance due to illumination, weather and seasonal changes, dynamic objects, and constructions. Such long-term datasets allow for in-depth investigation of problems that detain the realization of autonomous vehicles such as localization in dierent times of the year.
长期自动：几个数据集，如KITTI或Cityscapes，着重于开发自主驾驶的算法能力，但不能解决长期自主的挑战，例如随着时间的推移环境变化。 为了解决这个问题，Maddern等人提出了一个用于自主驾驶的新型数据集。（2016）。 他们在一年内在英国牛津中心穿过1000公里的地方收集图像，LiDAR和GPS数据。 这允许他们捕获由于照明，天气和季节变化，动态对象和结构而导致的场景外观的大变化。 这些长期数据集允许深入调查在一年中的不同时期扣留自主车辆的实现问题，例如本地化。


- 2.2. Synthetic Data
- The generation of ground truth for real examples is very labor intensive and often not even possible at large scale when pixel-level annotations are required. On the other hand, pixel-level ground truth for large-scale synthetic datasets can be easily acquired. However, the creation of realistic virtual world is time-consuming. The popularity of movies and video games have led to an industry creating very realistic 3D content which nourishes the hope to replace real data completely using synthetic datasets. Consequently, several synthetic datasets have been proposed, recently, but it remains an open question whether the realism and variety attained is sufficient to replace real world datasets. Besides, creating realistic virtual content is a time consuming and expensive process itself and the trade-off between real and synthetic (or augmented) data is not clear yet.
为真实的例子生成地面真相是非常劳动密集型的，并且在需要像素级注释时通常甚至不可能大规模地实现。 另一方面，可以轻松获取大规模合成数据集的像素级地面实况。 然而，创造现实的虚拟世界是耗时的。 电影和视频游戏的普及导致了行业创造了非常逼真的3D内容，这些内容丰富了使用合成数据集完全替代实际数据的希望。 因此，最近已经提出了几个合成数据集，但是现实主义和品种是否足以替代现实世界数据集仍然是一个悬而未决的问题。 此外，创建逼真的虚拟内容是一个耗时且昂贵的过程本身，真实和合成（或增强）数据之间的权衡尚不清楚。

- **MPI Sintel**: The MPI Sintel Flow benchmark22 presented by Butler et al. (2012) takes advantage of the open source movie Sintel, a short animated film, to render scenes of varying complexity with optical flow ground truth. In total, Sintel comprises 1,628 frames. Different datasets obtained using different passes of the rendering pipeline vary in complexity shown in Figure 3. The albedo pass has roughly piecewise constant colors without illumination effects while the clean pass introduces illumination of various kinds. The final pass adds atmospheric effects, blur, color correction and vignetting. In addition to the average endpoint error, the benchmark website provides different rankings of the methods based on speed, occlusion boundaries, and disocclusions.
- **Flying Chairs and Flying Things**: The limited size of optical flow datasets hampered the training of deep high-capacity models. To train a convolutional neural network, Dosovitskiy et al.(2015) thus introduced a simple synthetic 2D dataset of flying chairs rendered on top of random background images from Flickr. As the limited realism and size of this dataset proved insufficient to learn highly accurate models, Mayer et al. (2016) presented another large-scale dataset consisting of three synthetic stereo video datasets: FlyingThings3D, Monkaa, Driving. FlyingThings3D provides everyday 3D objects flying along randomized 3D trajectories in a randomly created scene. Inspired by the KITTI dataset a driving dataset has been created which uses car models from the same pool as FlyingThings3D and additionally highly detailed tree and building models from 3D Warehouse. Monkaa is an animated short movie similar to Sintel used in the MPI Sintel benchmark.
- **Game Engines**: Unfortunately, data from animated movies is very limited since the content is hard to change and such movies are rarely open source. In contrast, game engines allow for creating an infinite amount of data. One way to create virtual worlds using a game engine is presented by Gaidon et al. (2016) which introduces the Virtual KITTI dataset23. They present an efficient real-to-virtual world cloning method to create realistic proxy worlds. A cloned virtual world allows to vary conditions such as weather or illumination and to use different camera settings. This way, the proxy world can be used for virtual data augmentation to train deep networks. Virtual KITTI contains 35 photo-realistic synthetic videos with a total of 17,000 high resolution frames. They provide ground truth for object detection, tracking, scene and instance segmentation, depth and optical flow.
MPI Sintel ：由Butler等人提出的MPI Sintel Flow benchmark22 （2012）利用开源电影Sintel（短片动画），以光流地面的真相呈现不同复杂度的场景。总共有Sintel包括1,628帧。使用不同渲染流程获得的不同数据集的复杂度如图3所示。反照率传递具有大致分段恒定颜色，无照明效果，而清洁通道则引入各种照明。最后的通行证增加了大气效果，模糊，颜色校正和渐晕。除了平均终点误差之外，基准网站还提供了基于速度，遮挡边界和不相关的方法的不同排名。
- 飞行椅和飞行事物：光流数据集的数量有限，妨碍了深层大容量模型的训练。为了训练卷积神经网络，Dosovitskiy等人（2015）引入了一个简单的合成2D数据集，它们呈现在Flickr的随机背景图像之上。由于该数据集的有限现实性和大小证明不足以学习高精度模型，Mayer等（2016）提出了另外一个由三个合成立体视频数据集组成的大型数据集：FlyingThings3D，Monkaa，Driving。 FlyingThings3D在随机创建的场景中提供随机3D轨迹飞行的每天3D对象。受KITTI数据集的启发，已经创建了一个驱动数据集，它使用与FlyingThings3D相同的池中的汽车模型，以及来自3D Warehouse的另外高度详细的树和建筑模型。 Monkaa是一个类似于Sintel的动画短片，用于MPI Sintel基准测试。
- 游戏引擎：不幸的是，动画电影的数据非常有限，因为内容很难改变，这样的电影很少是开源的。相比之下，游戏引擎允许创建无限量的数据。 Gaidon等人提出了使用游戏引擎创建虚拟世界的一种方式。 （2016）介绍了虚拟KITTI数据集23。他们提出了一种高效的实时虚拟世界克隆方法来创建现实的代理世界。克隆的虚拟世界允许改变诸如天气或照明的条件，并使用不同的相机设置。这样，代理世界可以用于虚拟数据扩充来训练深层网络。虚拟KITTI包含35张照片合成视频，总共17,000个高分辨率帧。它们为物体检测，跟踪，场景和实例分割，深度和光流提供了基础。

- In concurrent work, Ros et al. (2016) created SYNTHIA24, a synthetic collection of Imagery and Annotations of urban scenarios for semantic segmentation. They rendered a virtual city with the Unity Engine. The dataset consists of 13,400 randomly taken virtual images from the city and four video sequences with 200,000 frames in total. Pixel-level semantic annotations are provided for 13 classes.
- Richter et al. (2016) have extracted pixel-accurate semantic label maps for images from the commercial video game Grand Theft Auto V. Towards this goal, they developed a wrapper which operates between the game and the graphics hardware to obtain pixel-accurate object signatures across time and instances. The wrapper allows them to produce dense semantic annotations for 25 thousand images synthesized by the photorealistic open-world computer game with minimal human supervision. However, for legal reasons, the extracted 3D geometry can not be made publicly available. Similarly, Qiu & Yuille (2016) provide an open-source tool to create virtual worlds by accessing and modifying the internal data structure of Unreal Engine 4. They show how virtual worlds can be used to test deep learning algorithms by linking them with the deep learning framework Caffe Jia et al. (2014).
- 在并行工作中，Ros et al。 （2016）创建了SYNTHIA24，一种用于语义分割的城市场景图像和注释的综合集合。他们用Unity Engine渲染了一个虚拟的城市。该数据集由13,400个随机抽取的城市虚拟图像和四个视频序列组成，共20万帧。为13个类提供像素级语义注释。
- Richter et al。 （2016）已经为商业视频游戏“侠盗猎车手”V提取了图像的像素精确语义标签贴图。为了实现这一目标，他们开发了一种在游戏和图形硬件之间运行的包装器，以便跨越时间获得像素精确的对象签名，实例。包装器允许他们通过最小的人力监督来生成由真实感的开放世界电脑游戏合成的2.5万张图像的密集语义注释。然而，出于法律原因，提取的3D几何不能公开获得。同样，Qiu&Yuille（2016）通过访问和修改虚幻引擎4的内部数据结构，提供了一个开源工具来创建虚拟世界。他们展示了虚拟世界如何通过将深度学习算法与深层次学习框架Caffe Jia（2014）。

### 3. Cameras Models & Calibration 摄像头模块&校准
- 3.1. Calibration 校准
- Multiple sensors including odometry, range sensors, and different types of cameras such as perspective and fish-eye are widely used in automotive context. Calibration is the problem of estimating intrinsic and extrinsic parameters of these sensors to relate 2D image points to 3D world points and represent sensed information in a common coordinate system in case of multiple sensors. Fiducial markings on checkerboard patterns are the standard tool for calibration. Almost all systems use them either for initialization or for joint optimization to improve the intrinsics. Reprojection error which is the pixel distance between a projected point and a measured one, is used as a way of measuring accuracy quantitatively. Accuracy of calibration is a key issue in driver assistance applications requiring 3D reasoning, and consequently in the safety of autonomous vehicles. Besides accuracy, other desired qualities in a calibration system are speed, robustness to varying imaging conditions, full automation, minimum restrictions in terms of assumptions such as overlapping field of view or information required such as an initial guess of the parameters.
- Modern systems are equipped with multiple sensors for different purposes. Geiger et al. (2012c) use a setup involving two cameras and a single range sensor such as Kinect or Velodyne laser scanner. They present two algorithms for camera-tocamera and camera-to-range calibration using a single image per sensor. They assume a common field of view for the sensors which is particularly useful for applications such as generating stereo or scene flow ground truth. Heng et al. (2013) and Heng et al. (2015) tackle the automatic intrinsic and extrinsic calibration of a multi-camera rig system with four fish-eye cameras and odometry without assuming overlapping fields of view. Heng et al. (2015) propose an improved version of Heng et al. (2013). While Geiger et al. (2012c) require fiducial markings to re-calibrate the system before every run, they remove the requirement to modify infrastructure by using a map and natural features instead. They first build a map of the calibration area and then perform calibration by using this map and image-based geo-localization. In contrast to SLAM-based selfcalibration methods, image-based localization removes the burden of exhaustive feature matching between dierent cameras and bundle adjustment.
多个传感器，包括测距仪，量程传感器以及不同类型的摄像机，如透视和鱼眼，广泛应用于汽车领域。校准是估计这些传感器的内在和外在参数以将2D图像点与3D世界点相关并且在多个传感器的情况下在公共坐标系中表示感测信息的问题。棋盘图案上的基准标记是校准的标准工具。几乎所有系统都使用它们进行初始化或联合优化来改进内在函数。作为投影点和测量点之间的像素距离的重新投影误差被用作定量测量精度的一种方式。校准的准确性是驾驶员辅助应用中的关键问题，需要3D推理，因此在自主车辆的安全性方面。除了准确性之外，校准系统中的其他期望的质量是对变化的成像条件的速度，鲁棒性，全自动化，在诸如重叠视野或所需信息（诸如参数的初始猜测）的假设方面的最小限制。
- 现代系统配备多个传感器用于不同的目的。盖革等人（2012c）使用一个包含两个摄像头和单范围传感器（如Kinect或Velodyne激光扫描仪）的设置。它们使用每个传感器单个图像呈现两种相机摄像机和摄像机到距离校准的算法。它们假设传感器的共同视野，这对于诸如产生立体声或场景流场实况的应用特别有用。 Heng等（2013）和Heng等（2015）利用四个鱼眼摄像机和距离测量法来处理多摄像机钻机系统的自动内在和外在校准，而不考虑重叠的视野。 Heng等（2015）提出了Heng等人的改进版。 （2013年）。而Geiger等人（2012c）要求在每次运行之前重新校准系统的基准标记，它们通过使用地图和自然特征来消除修改基础设施的要求。他们首先构建校准区域的图，然后使用该地图和基于图像的地理定位进行校准。与基于SLAM的自动校准方法相比，基于图像的定位消除了不同相机与捆绑调整之间的穷举特征匹配的负担。

- 3.2. Omnidirectional Cameras
- A panoramic field of view is desirable in autonomous driving to gain maximum information about the surrounding area for safe navigation. An omnidirectional camera with a 360-degree field of view provides enhanced coverage by eliminating the need for more cameras or mechanically turnable cameras. There are dierent types of omnidirectional cameras with a visual field that covers a hemisphere or even approximately the entire sphere. Catadioptric cameras combine a standard camera with a shaped mirror, such as a parabolic, hyperbolic, or elliptical mirror while dioptric cameras use purely dioptric fisheye
lenses. Polydioptric cameras use multiple cameras with overlapping field of view to provide a full spherical field of view.
- One classification often used in the literature for omnidirectional cameras is based on the projection center: central and noncentral. In central cameras, the optical rays to the viewed objects intersect in a single point in 3D which is known as the single effective viewpoint property. This property allows the generation of geometrically correct perspective images from the images captured by omnidirectional cameras and consequently, application of epipolar geometry which holds for any central camera. Central catadioptric cameras are built by choosing the mirror shape and the distance between the camera and the mirror.
在自主驾驶中需要全景视野以获得关于周围区域的最大信息以用于安全导航。具有360度视野的全向摄像机可以通过消除对更多摄像机或机械可转换摄像机的需求而提供更高的覆盖范围。有不同类型的全方位摄像机，其视野覆盖半球或甚至大致整个球体。反折射相机将标准相机与成型镜相结合，例如抛物线，双曲线或椭圆镜，而折光相机使用纯粹的折射鱼眼
镜头。多折照相机使用多个具有重叠视野的相机，以提供完整的球面视野。
- 文献中常用于全向摄像机的一个分类是基于投影中心：中央和非中央。在中央相机中，到所观看的物体的光线在3D中的单个点相交，这被称为单个有效视点属性。该属性允许从由全向照相机拍摄的图像产生几何正确的透视图像，并因此产生适用于任何中央相机的对极几何形状。通过选择镜面形状和相机与镜子之间的距离来构建中央反折射相机。

- In contrast to pinhole cameras, calibration of omnidirectional cameras cannot be modeled by a linear projection due to very high distortion. The model should take into account the reflection of the mirror in the case of a catadioptric camera or the refraction caused by the lens in the case of a fisheye camera. Geyer & Daniilidis (2000) provide a unifying theory for all central catadioptric systems which is known as unified projection model in the literature and widely used by different calibration toolboxes (Mei & Rives (2007); Heng et al. (2013, 2015)). They prove that every projection, both standard perspective and
catadioptric using a hyperbolic, parabolic, or elliptical mirror, can be modeled with projective mappings from the sphere to a plane where projection center is on a sphere diameter and the plane perpendicular to it. Scaramuzza & Martinelli (2006) propose modeling the imaging function by a Taylor series expansion whose degree and the coecients are the parameters to be estimated. Polynomials of order three or four are able to model accurately all catadioptric cameras and many types of fisheye cameras. Mei & Rives (2007) improve upon unified projection model of Geyer & Daniilidis (2000) to account for real-world errors by modeling distortions with well identified parameters.
- As desirable as it is, the single viewpoint property is often violated in practice due to varifocal lenses and difficulty of precise alignment. However, non-central models as the alternative are computationally demanding, hence not suitable for real-time applications. Sch¨onbein et al. (2014) extend a noncentral approach in order to accurately obtain the viewing ray orientations, and then propose a fast central approximation with a mapping to match the obtained orientations. This kind of approach, tested on hypercatadioptric cameras, achieves a reprojection error lower than the central models (Geyer & Daniilidis(2000); Scaramuzza & Martinelli (2006); Mei & Rives (2007)) and comparable to non-central models while being much faster.
- 与针孔相机不同，由于非常高的失真，全向摄像机的校准不能由线性投影建模。在反折射相机的情况下，该模型应考虑到镜子的反射，或者在鱼眼相机的情况下应考虑镜头引起的折射。 Geyer＆Daniilidis（2000）为所有中心反射折射系统提供了统一的理论，在文献中被称为统一投影模型，并被不同的校准工具箱广泛使用（Mei＆Rives（2007）; Heng等（2013，2015）） 。他们证明了每一个投影，既有标准的视角又有
使用双曲线，抛物面或椭圆镜的反射折射可以用从球体到投影中心在球体直径和垂直于其的平面的平面进行投影映射来建模。 Scaramuzza＆Martinelli（2006）提出了通过泰勒级数展开来建模成像函数，其程度和系数是要估计的参数。三阶或四阶的多项式能够准确地模拟所有反射折射相机和许多类型的鱼眼相机。 Mei＆Rives（2007）改进了Geyer＆Daniilidis（2000）的统一投影模型，通过建立具有良好识别参数的失真模型来解释现实世界的错误。
- 根据需要，由于变焦透镜和精确对准的困难，实际上常常违反单一视点特性。然而，非中心模型作为替代方案在计算上要求苛刻，因此不适合实时应用。 Sch¨onbein等（2014）扩展了非中心的方法，以便准确地获得观察射线取向，然后提出一个具有映射的快速中心近似以匹配获得的方向。这种在过度反射摄影机上进行测试的方法实现了比中央模型（Geyer＆Daniilidis（2000）; Scaramuzza＆Martinelli（2006）; Mei＆Rives（2007））更低的重现误差，并且与非中心模型相当，快多了。

- **Applications**: Omnidirectional cameras are more and more used in autonomous driving. For feature based applications such as navigation, motion estimation and mapping, large field of view enables extraction and matching of interesting points from all around the car. For instance, omnidirectional feature matches improve the rotation estimate significantly when doing visual odometry or simultaneous localization and mapping(SLAM). Scaramuzza&Siegwart (2008) estimate the ego-motion of the vehicle relative to the road from a single, central omnidirectional camera by using a homography based tracker for the ground plane and an appearance-based tracker for the rotation of the vehicle. 3D perception also benefits from the unified view offered by omnidirectional sensors, despite the limited effective resolution which leads to noisy reconstructions. Laserbased solutions as an alternative provide only sparse point clouds without color, are extremely expensive and suffer from rolling shutter effects. Sch¨onbein & Geiger (2014) propose a method for 3D reconstruction through joint optimization of disparity estimates from two temporally and two spatially adjacent omnidirectional view in a unified omnidirectional space by using plane-based priors. H¨ane et al. (2014) extend the planesweeping stereo matching for fisheye cameras by incorporating unified projection model for fisheye cameras directly into the plane-sweeping stereo matching algorithm. This kind of approach allows producing dense depth maps directly from fisheye images in real time using GPUs and opens the way for dense 3D reconstruction with large a field of view in real time.
- 应用：全自动摄像机越来越多地用于自主驾驶。对于基于功能的应用程序，如导航，运动估计和映射，大视野可以提取和匹配来自汽车周围的有趣点。例如，全方位特征匹配在进行视觉测距或同时定位和映射（SLAM）时会显着改善旋转估计。 Scaramuzza＆Siegwart（2008）通过使用用于地平面的基于单应性的跟踪器和用于车辆旋转的基于外观的跟踪器来估计来自单个中央全向照相机的车辆相对于道路的自主运动。尽管有限分辨率导致噪声重建，3D感知也受益于全向传感器提供的统一视图。作为替代的基于激光的解决方案仅提供没有颜色的稀疏云，是非常昂贵的并且受到滚动快门效应的影响。 Sch¨onbein＆Geiger（2014）提出了一种通过使用基于平面的先验在统一的全向空间中从两个时间和两个空间相邻的全向视差联合优化差异估计的三维重建方法。 H¨ane等（2014年）通过将鱼眼摄影机的统一投影模型直接纳入平扫立体匹配算法，扩展了鱼眼摄像机的扫描立体匹配。这种方法允许使用GPU实时直接从鱼眼图像生成密集的深度图，并且以实时的大视野打开密集3D重建的方式。

- 3.3. Event Cameras 活动相机
- Contrary to conventional frame-based imagers at constant frame rates, event-based sensors have very recently been introduced. They produce a stream of asynchronous events at microsecond resolution in case of a brightness change surpassing a pre-defined threshold (Dynamic Vision Sensor) as shown in Figure 4. An event contains the location, sign, and precise timestamp of the change. This kind of data is sparse in nature, thus reducing redundancy in transmission and processing. Another advantage is high temporal resolution, allowing the design of highly reactive systems. These properties, namely low latency and low bandwidth requirement make event-based sensors interesting for autonomous driving. However, standard computer-vision algorithms cannot be applied directly to the output of event-based vision sensors which is fundamentally different from intensity images. Events occur at high frequency and each event doesn’t carry enough information by itself. A straightforward solution is to generate intensity images by accumulating
events over a fixed time interval, but this kind of event to-frame conversion introduces some latency and obstructs the efficiency which comes with the high temporal resolution.
- Instead, algorithms should ideally exploit the high rate at which events are generated. Consequently several methods have recently been introduced which exploit the high temporal resolution and the asynchronous nature of the sensor for different problems in autonomous vision. The design goal of such algorithms is that each incoming event can asynchronously change the estimated state, thus respecting the event-based nature of the sensor and allowing for perception and state estimation in highly dynamic scenarios. For trajectory estimation, Mueggler et al. (2015b) propose a continuous temporal model as a natural representation of the pose trajectory described by a smooth parametric model. Rebecq et al. (2016) propose an event-based 3D reconstruction algorithm to produce a parallel tracking and mapping pipeline that runs in real-time on the CPU. Eventbased SLAM does not suffer from motion blur due to high speed motions and very high dynamic range scenes which can be challenging for standard camera approaches.
- Lifetime Estimation: In addition to enabling novel solutions for existing problems where low latency and high frame rates are required, event-based sensors also give rise to new problems. One such problem is lifetime estimation of events by modeling the set of active events. An event is considered active as long as the brightness gradient causing the event is visible by the pixel. Explicit modeling of active events can be used to generate sharp gradient images at any point in time, or for clustering of events in tracking of multiple objects. For this task, Mueggler et al. (2015a) propose using event-based optical flow with optional regularization, independent of a temporal window.
- 与传统的基于帧的成像器在恒定帧速率相反，基于事件的传感器最近已经被引入。在亮度变化超过预定阈值（动态视觉传感器）的情况下，它们以微秒分辨率产生异步事件流，如图4所示。事件包含更改的位置，符号和精确时间戳。这种数据本质上是稀疏的，从而减少传输和处理的冗余。另一个优点是高时间分辨率，允许设计高反应性系统。这些属性，即低延迟和低带宽需求使得基于事件的传感器对于自主驾驶是有意义的。然而，标准计算机视觉算法不能直接应用于与强度图像基本不同的基于事件的视觉传感器的输出。事件发生在高频率，每个事件本身不携带足够的信息。一个简单的解决方案是通过积累产生强度图像
事件在固定的时间间隔内，但这种事件帧间转换引入了一些延迟并阻碍了高时间分辨率带来的效率。
相反，算法应该理想地利用生成事件的高速率。因此，最近已经引入了几种方法，其利用自主视觉中的不同问题的传感器的高时间分辨率和异步性质。这种算法的设计目标是每个传入事件可以异步地改变估计状态，从而满足传感器的基于事件的性质，并允许在高度动态的情况下的感知和状态估计。对于轨迹估计，Mueggler et al。 （2015b）提出了一种连续时间模型作为由平滑参数模型描述的姿势轨迹的自然表示。 Rebecq等（2016）提出了一种基于事件的3D重建算法，以产生在CPU上实时运行的并行跟踪和映射管道。基于事件的SLAM由于高速运动和非常高的动态范围场景而不受运动模糊的影响，这对于标准相机方法来说可能是具有挑战性的。
- 寿命估算：除了为需要低延迟和高帧速率的现有问题提供新颖的解决方案外，基于事件的传感器也会引起新的问题。一个这样的问题是通过对一组活动事件建模来对事件进行寿命估计。只要导致事件的亮度梯度被像素看到，事件被认为是活动的。活动事件的显式建模可用于在任何时间点生成锐利梯度图像，或用于在跟踪多个对象时对事件进行聚类。对于这个任务，Mueggler等人（2015a）提出了使用基于事件的光流与可选正则化，独立于时间窗口。

### 4. Representations 表现
- A wide variety of representations at different levels of granularity is used in the computer vision literature. Variables or parameters can be associated directly with 2D pixels in an image or describe high-level primitives in 3D space. In pixel-based representation each pixel is a separate entity, for example a random variable in a graphical model. Pixels are amongst the most fine-grained representations, but are harder to relate to physical properties of our 3D world. Furthermore, pixel-based representations increase complexity of inference algorithms due to the large number of variables in high resolution images. As a consequence, many approaches model only local interactions between pixels which do not capture the structure of our world sufficiently well to overcome all ambiguities in the ill-posed inverse
problems computer vision is trying to solve.
- 在计算机视觉文献中使用了不同粒度级别的各种表示。 变量或参数可以直接与图像中的2D像素相关联，或者描述3D空间中的高级原语。 在基于像素的表示中，每个像素是一个单独的实体，例如图形模型中的随机变量。 像素是最细粒度的表示，但更难与3D世界的物理属性相关。 此外，由于高分辨率图像中的大量变量，基于像素的表示增加了推理算法的复杂性。 因此，许多方法仅模拟像素之间的局部相互作用，这些相互作用不能很好地捕获我们世界的结构，以克服不正确的逆向中的所有模棱两可
计算机视觉问题正在试图解决。

- **Superpixels**: Consequently, compact representations based on grouping of pixels, i.e. superpixels, have gained popularity. Superpixel-based representations are obtained by a segmentation of the image into atomic regions which are ideally similar in color and texture, and respect image boundaries (Ren & Malik (2003); Achanta et al. (2012); Li & Chen (2015)). The implicit assumption each superpixel-based method makes is that certain properties of interest remain constant within a superpixel, e.g., the semantic class label or the slant of a surface. However, boundary adherence with respect to these properties is easily violated, especially for cluttered images when relying on standard segmentation algorithms which leverage color or intensity cues.
- If available, depth information can be leveraged as valuable feature for accurate superpixel extraction (Badino et al.(2009); Yamaguchi et al. (2014)). Superpixels are used as building blocks for various tasks such as stereo and flow estimation (Yamaguchi et al. (2012, 2013, 2014); G¨uney & Geiger (2015); Bai et al. (2016)), scene flow (Menze & Geiger (2015); Menze et al. (2015b); Lv et al. (2016)), semantic segmentation (Xiao & Quan (2009); Wegner et al. (2013)), scene understanding (Ess et al. (2009b); Liu et al. (2014)) and 3D reconstruction (Sch¨onbein et al. (2014)). In cases that include geometric reasoning
such as stereo estimation, superpixels often represent 3D planar segments. When the goal is to represent real-world scenes with independent object motion as in scene flow or optical flow, superpixels can be generalized to rigidly moving segments (Vogel et al. (2015); Menze & Geiger (2015)), or semantic segments (Bai et al. (2016); Sevilla-Lara et al. (2016)).
- 超像素：因此，基于像素分组（即，超像素）的紧凑表示已经受欢迎。基于超像素的表示是通过将图像分割成在颜色和纹理理想上相似的原子区域和尊重图像边界来获得的（Ren＆Malik（2003）; Achanta等人（2012）; Li＆Chen（2015） ）。每个基于超像素的方法所产生的隐含假设是在某个超像素（例如，语义类标签或表面的倾斜）中保持一定的感兴趣的特性。然而，相对于这些属性的边界附着容易受到侵害，特别是对依赖于利用颜色或强度提示的标准分割算法的混乱图像。
- 如果可用，深度信息可以用作准确的超像素提取的有价值的特征（Badino等（2009）; Yamaguchi等（2014））。超像素被用作立体声和流量估计等各种任务的构建块（Yamaguchi et al。（2012，2013，2014）;G¨uney＆Geiger（2015）; Bai et al。（2016）），场景流（Menze ＆Geiger（2015）; Menze等（2015b）; Lv et al。（2016）），语义分割（Xiao＆Quan（2009）; Wegner等（2013）），场景理解（Ess et al。 2009b）; Liu et al。（2014））和3D重建（Sch¨onbein等（2014））。在包括几何推理的情况下
例如立体声估计，超像素通常表示3D平面片段。当目标是在场景流或光流中表现具有独立物体运动的现实世界场景时，可以将超像素推广到刚性移动段（Vogel等人（2015）; Menze＆Geiger（2015））或语义段（Bai et al。（2016）; Sevilla-Lara et al。（2016））。

- **Stixels**: Stixels are presented as a medium level representation of 3D traffic scenes with the goal to bridge the gap between pixels and objects (Badino et al. (2009)). The so-called “Stixel World” representation originates from the observation that free space in front of the vehicle is mostly limited by vertical surfaces. Stixels are represented by a set of rectangular sticks standing vertically on the ground to approximate these surfaces. Assuming a constant width, each stixel is defined by its 3D position relative to the camera and its height. The main goal is to gain eciency through a compact, complete, stable, and robust representation. In addition, Stixel representations provide an encoding of the free space and the obstacles in the scene.
- Using depth maps from SGM Hirschm¨uller (2008) as input, Badino et al. (2009) use dynamic programming based on occupancy grids to compute free space (determining the Stixels’ lower positions) and foreground/background segmentation on the disparity map (to compute the Stixels’ height). Pfeiffer & Franke (2011) extend Badino et al. (2009) to a unified probabilistic scheme. They lift the constraint of Stixels to touch the ground and allow multiple stixels along an image column. This way objects can be located at multiple depths in a single image column (Figure 5).
- Stixels：Stixels被呈现为3D流量场景的中等级别表示，目的是弥补像素和对象之间的差距（Badino等人（2009））。所谓的“Stixel World”表现起源于观察车前面的自由空间主要受垂直面的限制。柱状物由垂直放置在地面上的一组矩形棒表示以近似这些表面。假设一个恒定的宽度，每个Stixel是通过它相对于相机的3D位置及其高度来定义的。主要目标是通过紧凑，完整，稳定和强大的表现来提高效率。此外，Stixel表示提供了现场的自由空间和障碍物的编码。
- 使用SGMHirschm¨uller（2008）的深度图作为输入，Badino et al。 （2009）使用基于占用网格的动态规划来计算自由空间（确定Stixels的较低位置）和视差图上的前景/背景分割（以计算Stixels的高度）。 Pfeiffer＆Franke（2011）扩展了Badino等人（2009）统一概率方案。它们提升Stixels的约束以触摸地面，并允许沿着图像列的多个Stixels。这样，对象可以位于单个图像列中的多个深度（图5）。

- Pfeiffer & Franke (2010) extend the Stixel world representation to dynamic scenes by tracking stixels using a 6D Kalman filter framework and optical flow as input. Erbs et al. (2012, 2013) propose a CRF framework for segmenting a traffic scene based on the Dynamic StixelWorld representation. G¨unyel et al. (2012) show that motion estimation for stixels can be reduced to a 1D problem and can be solved eciently via 2D dynamic programming by avoiding costly dense optical flow computation.
- Levi et al. (2015) propose to use a CNN called StixelNet for learning to extract the foot point of each Stixel from the image. Cordts et al. (2014) propose to incorporate top-down objectlevel cues into bottom-up Stixel representation in a probabilistic approach. In order to achieve that, they leverage probability images derived from the output of three different object detectors, namely pedestrian, vehicle, and guard rail. Schneider et al.(2016) propose a semantic Stixel representation to jointly infer semantic and geometric layout of the scene from a dense disparity map and a pixel-level semantic scene labeling.
- Pfeiffer＆Franke（2010）通过使用6D卡尔曼滤波器框架和光流作为输入，通过跟踪Stixels将Stixel世界表示扩展到动态场景。 Erbs等人（2012年，2013年）提出了一个基于Dynamic StixelWorld表示来分割交通场景的CRF框架。 G¨unel等人（2012）显示，Stixels的运动估计可以减少到1D问题，可以通过2D动态编程有效地解决，避免了昂贵的密集光流计算。
- Levi et al。 （2015）建议使用名为StixelNet的CNN学习从图像中提取每个Stixel的脚点。 Cordts et al。 （2014）提出将自顶向下的物体等级线索纳入自下而上的Stixel表示法中，以概率方法。为了实现这一点，它们利用从三个不同物体检测器（即行人，车辆和护栏）的输出得到的概率图像。 Schneider等人（2016）提出了一种语义Stixel表示，从密集视差图和像素级语义场景标记共同推断场景的语义和几何布局。

- **3D Primitives**: The use of 3D geometric primitives is very common in 3D reconstruction, particularly when reconstructing urban areas. Atomic regions which are geometrically meaningful allow the shape of urban objects to be better preserved. In addition, simplified geometric assumptions can provide significant speedups as well as a compact model. In Cornelis et al.(2008), 3D city models are composed of ruled surfaces for both the facades and the roads. Duan & Lafarge (2016) use polygons with elevation estimate for 3D city modeling from pairs of satellite images. de Oliveira et al. (2016) update a list of large-scale polygons over time for an incremental scene representation from 3D range measurements. Lafarge et al. (2010) use a library of 3D blocks for reconstructing buildings with different roof forms. Lafarge & Mallet (2012); Lafarge et al. (2013) use 3D-primitives such as planes, cylinders, spheres or cones for describing regular structures of the scene. Dub´e et al. (2016) segments point clouds into distinct elements for a loop-closure detection algorithm based on the matching of 3D segments.
- 3D原语：在3D重建中使用3D几何图元非常常见，特别是在重建城市地区时。几何有意义的原子区域可以更好地保存城市物体的形状。此外，简化的几何假设可以提供显着的加速以及紧凑的模型。在Cornelis等人（2008）中，3D城市模型由外墙和道路的统治曲面组成。 Duan＆Lafarge（2016）使用多边形，对卫星图像进行三维城市建模的高程估计。 de Oliveira et al。 （2016）根据3D范围测量更新一个随时间推移的大尺寸多边形的增量场景表示。拉法基等人（2010）使用3D块库来重建具有不同屋顶形式的建筑物。拉法基与马勒（2012）;拉法基等人（2013）使用3D原始图像，例如平面，圆柱体，球体或锥体来描述场景的常规结构。 Dub'e等人（2016）将云划分成基于3D段匹配的闭环检测算法的不同元素。

###5. Object Detection 目标检测
- Reliable detection of objects is a crucial requirement to realize autonomous driving. As the car is sharing the road with many traffic participants, particularly in urban areas, the awareness of other traffic participants or obstacles is necessary to avoid accidents that might be life threatening. The detection in urban areas is hard because of the wide variety of object appearances and occlusions caused by other objects or the object of interest itself. In addition, the resemblance of objects to each other or to the background and physical effects like cast shadows or reflections can make the distinction difficult.
对物体的可靠检测是实现自主驾驶的关键要求。 由于汽车与许多交通参与者，特别是在城市交通拥挤的道路上，其他交通参与者或障碍物的意识是必要的，以避免可能危及生命的事故。 由于其他物体或感兴趣的物体引起的物体外观和遮挡物的种类繁多，所以在城市地区的检测是困难的。 另外，对象之间的相似性或背景和物理效果（如投射阴影或反射）的区别可能使得区分变得困难。

- **Sensors**: The object detection task can be addressed with a variety of different of sensors. Cameras are the cheapest and most commonly used type of sensors for the detection of objects. The visible spectrum (VS) is typically used for daytime detections whereas the infrared spectrum can be used for nighttime detection. Thermal infrared (TIR) cameras capture relative temperature which allows to distinguish warm objects like pedestrians from cold objects like vegetation or the road. Active sensors, that emit signals and observe their reflection, like laser scanners can provide range information which is helpful for detecting an object and localizing it in 3D. Depending on the weather conditions or material properties it can be problematic to rely on a single type of sensor alone. VS cameras and laser scanners are affected by reflective or transparent surfaces while hot objects (like engines) or warm temperatures can influence TIR cameras. The combination of information from different sensors via sensor fusion (Enzweiler & Gavrila (2011); Chen et al. (2016b); Gonz´alez et al. (2016)) allows for the robust integration of this complementary information.
- 传感器：物体检测任务可以用各种不同的传感器来寻址。相机是用于检测物体的最便宜和最常用的传感器类型。可见光谱（VS）通常用于日间检测，而红外光谱可用于夜间检测。热红外（TIR）摄像机捕获相对温度，允许区分温暖的物体，如行人与植物或道路等寒冷物体。发射信号并观察其反射的主动传感器，如激光扫描仪，可提供范围信息，有助于检测物体并将其定位在3D中。根据天气条件或材料特性，依靠单一类型的传感器可能是有问题的。 VS相机和激光扫描仪受到反光或透明表面的影响，而热物体（如发动机）或温暖的温度可影响TIR相机。通过传感器融合的不同传感器信息的组合（Enzweiler＆Gavrila（2011）; Chen等（2016b）; Gonz'alez等（2016））允许这种补充信息的稳健整合。

- **Standard Pipeline**: A traditional detection pipeline consists of the following steps: preprocessing, region of interest extraction (ROI), object classification and verification/refinement. In the preprocessing step tasks such as exposure and gain adjustment, as well as camera calibration and image rectification are usually performed. Some approaches leverage temporal information with a joint detection and tracking system. We give a detailed overview of the tracking problem in Section 9.
- Regions of interest can be extracted using a sliding window approach which shifts a detector over the image at different scales. As exhaustive search is very expensive, several heuristics have been proposed for reducing the search space. Typically, the number of evaluations is reduced by assuming a certain ratio, size and position of candidate bounding boxes. Apart from that, image features, stereo or optical flow can be leveraged for focusing the search on the relevant regions. Broggi et al. (2000) filter pedestrian candidates using morphological characteristics (size, ratio and shape) and vertical symmetry of human shape. In addition, they exploit the distance information obtained from stereo vision in the ROI extraction and refinement steps of the algorithm. Selective Search (Uijlings et al.(2013)) is an alternative approach to generate regions of interest. They exploit segmentation for efficiently extracting approximate locations instead of performing an exhaustive search over the full image domain.
- In their survey on pedestrian detection systems from monocular images, Dollar et al. (2011) present an extensive evaluation focusing on the evaluation of sliding window approaches. They claim that these approaches are most promising for low to medium resolution detection but found that detection with low-resolution inputs and occlusions are still problematic for the considered approaches.
- 标准管道：传统的检测流程包括以下步骤：预处理，感兴趣区域提取（ROI），对象分类和验证/细化。在预处理步骤中，通常执行诸如曝光和增益调整之类的任务，以及相机校准和图像校正。一些方法利用联合检测和跟踪系统来利用时间信息。我们详细介绍第9节中的跟踪问题。
- 可以使用将检测器移动到不同尺度的图像上的滑动窗口方法来提取感兴趣区域。由于穷举搜索非常昂贵，因此已经提出了几种启发式方法来减少搜索空间。通常，通过假设候选边界框的一定比例，大小和位置来减少评估的数量。除此之外，可以利用图像特征，立体声或光流来将搜索集中在相关区域。 Broggi等人（2000）过滤行人候选人使用形态特征（尺寸，比例和形状）和人体形状的垂直对称性。此外，它们利用在算法的ROI提取和细化步骤中从立体视觉获得的距离信息。选择性搜索（Uijlings等人（2013））是产生感兴趣区域的替代方法。它们利用分割来有效地提取近似位置，而不是在完整图像域上进行详尽的搜索。
- 在他们对单眼图像行人检测系统的调查中，Dollar et al。 （2011）提出了广泛的评估，侧重于滑动窗口方法的评估。他们声称这些方法对于中低分辨率检测是最有希望的，但发现用低分辨率输入和遮挡的检测对于所考虑的方法仍然是有问题的。

- **Classification**: The classification of all candidates in an image using the sliding window approach can become quite costly due to the vast amount of image regions which need to be classified. Therefore, a fast decision is necessary which quickly discards candidates in the background region of the image. Viola et al. (2005) combine simple and efficient classifiers, learned using AdaBoost, in a cascade which allows to quickly discard false candidates while spending more time on promising regions. With the work of Dalal & Triggs (2005), linear Support Vector Machines (SVMs), that maximizes the margin of all samples from a linear decision boundary, in combination with Histogram of Orientation (HOG) features have become popular tools for classification. However, all previous methods rely on hand-crafted features that are difficult to design. With the renaissance of deep learning, convolutional neural networks have automated this task while significantly boosting performance. For example, Sermanet et al. (2013) introduced CNNs to the pedestrian detection problem using unsupervised convolutional sparse auto-encoders to pre-train features and end-to-end supervised training to train the classifier while fine-tuning the features. Today, all state of the art detection approaches are learned in an end-to-end fashion from large datasets as we will discuss in Section 5.1.
- 分类：使用滑动窗口方法对图像中所有候选人的分类可能会变得相当昂贵，因为需要分类的大量图像区域。因此，需要快速地决定是否在图像的背景区域中放弃候选。 Viola等人（2005）结合了简单而有效的分类器，在AdaBoost中学习使用了一个级联，可以在有希望的地区花费更多的时间来快速丢弃错误的候选人。通过Dalal＆Triggs（2005）的工作，线性支持向量机（SVM）将线性决策边界中的所有样本的边缘与定向（HOG）特征的组合结合起来，已经成为流行的分类工具。然而，以前的所有方法都依赖于难以设计的手工制作功能。随着深度学习的复兴，卷积神经网络已经自动化了这项任务，同时显着提升了性能。例如，Sermanet等人（2013）引入了CNN对行人检测问题，采用无监督卷积稀疏自动编码器预先训练特征和端对端监控训练，对微分器进行微调。今天，所有最先进的检测方法都是以大型数据集的端到端方式学习，我们将在5.1节中讨论。

- **Part-based Approaches**: Learning the appearance of articulated objects is difficult because all possible articulations need to be considered. The idea of part-based approaches is to split the complex appearance of non-rigidly moving objects like humans into simple parts and to represent any articulation using these parts. This provides greater flexibility and reduces the number of training examples required for learning the object appearance. The Deformable Part Model (DPM) by Felzenszwalb et al. (2008) attempts to break down the complex appearance of objects into easier parts for training SVMs with latent structure variables which represent the model configuration and need to be inferred at training time. They use a coarse global template covering the entire object and higher resolution part templates to model the appearance of each part as illustrated in Figure 6. All templates are represented using HOG features. In addition, they generalize SVMs to handle latent variables such as the part position location.
- An alternative to this representation is the Implicit Shape Model proposed by Leibe et al. (2008a) which learns a highly flexible representation of object shape. They extract local features around interest points and perform clustering to build up a codebook of local appearances that are characteristic for the particular object class under consideration. Based on this codebook, they learn where on the object the codebook entries may occur.
- While the part-based models presented so far have been very successful, they can not represent contextual information which is necessary for occlusion reasoning. Usually, a separate context model is learned to handle occlusions, see Hoiem et al. (2008); Tu & Bai (2010); Desai et al. (2011); Yang et al. (2012). And-Or models embed a grammar to represent large structural and appearance variations in a reconfigurable hierarchy. Wu et al. (2016a) propose to learn an And-Or model which takes into account structural and appearance variations at multi-car, single-car and part-levels jointly to represent both context and occlusions.
基于部分的方法：学习表达对象的外观是困难的，因为需要考虑所有可能的表述。基于部分的方法的想法是将非刚性移动物体（如人类）的复杂外观分解成简单的部分，并使用这些部分来表示任何发音。这提供了更大的灵活性，并减少了学习对象外观所需的训练示例的数量。 Felzenszwalb等人的变形部分模型（DPM） （2008）尝试将对象的复杂外观分解为更容易的部分，用于训练具有潜在结构变量的SVM，这些变量表示模型配置，需要在训练时推断出。它们使用覆盖整个对象的粗糙全局模板和更高分辨率部件模板来对每个部件的外观进行建模，如图6所示。所有模板都使用HOG功能表示。此外，它们推广SVM来处理潜在变量，如零件位置位置。
- 这种表示的替代方法是Leibe等人提出的隐式形状模型。 （2008a），学习对象形状的高度灵活的表示。他们提取利益点附近的局部特征，并执行聚类，以建立对于正在考虑的特定对象类特征的局部外观的码本。基于这本代码本，他们将学习在物体上可能发生的代码簿条目。
- 尽管目前提出的基于零件的模型已经非常成功，但它们不能表示阻塞推理所必需的上下文信息。通常，学习单独的上下文模型来处理闭塞，参见Hoiem等人（2008）;涂和（2010）; Desai et al。 （2011）;杨等（2012年）。和 - 或者模型嵌入语法来表示可重构层次结构中的大型结构和外观变化。吴等（2016a）建议学习一个兼并多车型，单车和部分级别的结构和外观变化的And-Or模型，共同表示上下文和闭塞。

- 5.1. 2D Object Detection 二维目标侦测
- KITTI Geiger et al. (2012b) is among the most popular benchmarks for object detection systems in the autonomous car context. A similar popularity for the pedestrian detection task has the Caltech-USA dataset (Doll´ar et al. (2012)). In this work we would like to focus our attention on the KITTI benchmark since it allows us to compare object and pedestrian detection systems on the same data. We refer the interested reader to the survey papers (Benenson et al. (2014); Zhang et al. (2016b)) for an in-depth comparison of pedestrian detection systems on Caltech-USA. In Table 1 we show the state-of-the-art on the KITTI benchmark for object, pedestrian and cyclist detection from images. Note that for all result tables in this paper, we list only public methods which have a paper associated with them as the details for the anonymous entries cannot be discussed yet. The performance is assessed for three level of difficulties using PASCAL VOC intersection-over-union (IOU) (Everingham et al. (2010)). Easy examples have a minimum bounding box height of 40 px and are fully visible, whereas moderate examples have a minimum height of 25 px including partial occlusion and hard examples have the same minimum height but includes the maximum occlusion level. In Table 2 the estimation of the object’s orientation is evaluated using the average orientation similarity (AOS) proposed in Geiger et al. (2012b).
KITTI Geiger等人（2012b）是自主汽车背景下对象检测系统最受欢迎的基准之一。行人检测任务的类似知名度有Caltech-USA数据集（Doll'ar et al。（2012））。在这项工作中，我们希望将注意力集中在KITTI基准上，因为它允许我们在同一数据上比较物体和行人检测系统。我们将有兴趣的读者参考调查文件（Benenson等（2014）; Zhang等（2016b）），对Caltech-USA的行人检测系统进行了深入的比较。在表1中，我们展示了从图像中的对象，行人和骑自行车者检测的KITTI基准测试的最新技术。请注意，对于本文中的所有结果表，我们仅列出与它们相关联的公共方法，因为匿名条目的详细信息不能讨论。使用PASCAL VOC交叉联盟（IOU）（Everingham等人（2010））评估了表现的三个难度。简单的例子具有40像素的最小边界框高度，并且是完全可见的，而适度示例具有25像素的最小高度，包括部分遮挡，硬实例具有相同的最小高度，但包括最大遮挡水平。在表2中，使用Geiger等人提出的平均取向相似度（AOS）来评估对象的取向的估计。 （2012B）。

- Convolutional Neural Networks allowed a significant improvement in the performance of object detectors. In the beginning, CNNs were integrated in sliding-window approaches (Sermanet et al. (2013)). However, the precise localization of objects is challenging because of the large receptive fields and strides. Girshick et al. (2014), on the other hand, propose RCNNs to solve the CNN localization problem with a “recognition using regions” paradigm. They generate many region proposals using selective search (Uijlings et al. (2013)), extract a fixed-length feature vector for each proposal using a CNN and classify each region with a linear SVM. Region-based CNNs are computationally expensive but several improvements have been proposed to reduce the computational burden (He et al.(2014); Girshick (2015)). He et al. (2014) use spatial pyramid pooling which allows to compute a convolutional feature map for the entire image with only one run of the CNN in contrast to R-CNN that needs to be applied on many image regions. Girshick (2015) further improve with a single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations. Even though these region-based networks have proven to be very successful on the PASCAL VOC benchmark, they could not achieve similar performance on KITTI. The main reason for this is that the KITTI dataset contains objects at many different scales and small objects
which are often heavily occluded or truncated. These objects are hard to detect using the region-based networks. Therefore, several methods for obtaining better object proposals have been proposed (Ren et al. (2015); Chen et al. (2016b,a); Yang et al. (2016); Cai et al. (2016)).
- 卷积神经网络可以显着改善物体探测器的性能。一开始，CNN被纳入滑窗方法（Sermanet等（2013））。然而，物体的精确定位是具有挑战性的，因为大的接收场和步幅。 Girshick等人（2014），另一方面，提出RCNNs以“使用区域认知”模式解决CNN本地化问题。他们使用选择性搜索生成许多区域提案（Uijlings等人（2013）），使用CNN提取每个提案的固定长度特征向量，并使用线性SVM对每个区域进行分类。基于区域的CNN在计算上是昂贵的，但是已经提出了几种改进来减少计算负担（He等人（2014）; Girshick（2015））。他等（2014）使用空间金字塔池，其允许计算整个图像的卷积特征图，只有CNN的一次运行与需要应用于许多图像区域的R-CNN相反。 Girshick（2015）通过单阶段训练算法进一步改进，共同学习对对象建议进行分类并优化其空间位置。尽管这些基于区域的网络已被证明在PASCAL VOC基准上非常成功，但是它们在KITTI上无法达到类似的表现。其主要原因是KITTI数据集包含许多不同尺度和小对象的对象
它们经常被遮蔽或截断。这些对象很难使用基于区域的网络进行检测。因此，提出了几种获得更好的对象提案的方法（Ren et al。（2015）; Chen et al。（2016b，a）; Yang et al。（2016）; Cai et al。（2016））。

- Ren et al. (2015) have introduced Region Proposal Networks (RPN) in which the region proposal network shares full-image convolutional features with the detection network and thus doesn’t increase computational costs. RPNs are trained end-to-end to generate high quality region proposals which are classified using the Fast R-CNN detector (Girshick (2015)). Chen et al. (2015c) use 3D information estimated from a stereo camera pair to extract better bounding box proposals. They place 3D candidate boxes on the ground plane and score them using 3D point cloud features. Finally, a CNN exploiting contextual information and using a multi-task loss jointly regresses the object’s coordinates and orientation. Inspired by this approach, Chen et al. (2016a) learn to generate class-specific 3D object proposals for monocular images, exploiting contextual models as well as semantics. They generate proposals by exhaustively placing 3D bounding boxes on the ground plane and scoring them with a standard CNN pipeline (Chen et al. (2015c)). Both methods Chen et al. (2015c) and Chen et al. (2016a) achieve comparable results to the best performing method in all detection task while outperforming all other methods on easy examples of KITTI car (Table 1a). In addition, they are among the best performing methods for the orientation estimation (Table 2).
- ren（2015）引入了区域提案网络（RPN），区域提案网络与检测网络共享全图像卷积特征，从而不增加计算成本。 RPN端对端进行培训，以生成使用Fast R-CNN检测器分类的高质量区域提案（Girshick（2015））。陈等（2015c）使用从立体相机对估计的3D信息来提取更好的边框提案。他们将3D候选箱放置在地面上，并使用3D点云特征对其进行评分。最后，利用上下文信息和使用多任务丢失的CNN联合回归对象的坐标和方向。灵感来自于这种方法，Chen et al。 （2016a）学习为单眼图像生成类特定的3D对象提案，利用上下文模型以及语义。他们通过在地平面上彻底放置3D边界框并用标准的CNN流水线（Chen et al。（2015c））得出建议。 Chen等人（2015c）和陈等人（2016a）在所有检测任务中实现与最佳性能方法相当的结果，同时在KITTI车的简单示例上表现超过所有其他方法（表1a）。此外，它们是方位估计中表现最好的方法之一（表2）。

- An alternative approach is presented by Yang et al. (2016). In case of small objects a strong activation of convolutional neurons is more likely to occur in earlier layers. Therefore, Yang et al. (2016) use scale-dependent pooling which allows to represent a candidate bounding box using the convolutional features from the corresponding scale. In addition, they propose layer-wise cascaded rejection classifiers, treating convolutional features in early layers as weak classifiers, to efficiently eliminate negative object proposals. The proposed scale-dependent pooling approach is one of the best-performing methods in all tasks ( Table 1 ).
- Cai et al. (2016) propose a multi-scale CNN consisting of a proposal sub-network and a detection sub-network. The proposal network, illustrated in Figure 7, performs detection at multiple output layers and these complementary scale-specific detectors are combined to produce a strong multi-scale object detector. Their multi-scale CNN outperforms all other methods on KITTI pedestrian and cyclist (Tables 1b,1c) while ranking second on KITTI car (Table 1a). Xiang et al. (2016) propose a region proposal network that uses subcategory information obtained from 3DVP (Xiang et al. (2015b)), to guide the proposal generating process, and a detection network for joint detection and subcategory classification. Object subcategories are defined for objects with similar properties or attributes such as appearance, pose or shape. The subcategory information allows them to outperform all other methods for the detection task on KITTI cars (Table 1a) and to achieve the best performance in the orientation estimation (Table 2).
- Yang等人提出了一种替代方法（2016）。在小物体的情况下，卷积神经元的强烈活化更可能发生在较早的层中。因此，Yang等（2016）使用规模依赖池，其允许使用来自相应量表的卷积特征来表示候选边界框。此外，他们提出层次级联拒绝分类器，处理早期层次中的卷积特征作为弱分类器，以有效消除负面对象提案。所提出的规模依赖性池化方法是所有任务中表现最好的方法之一（表1）。
- cai（2016）提出了由提案子网和检测子网组成的多尺度CNN。如图7所示的提议网络在多个输出层执行检测，并且将这些互补的比例特异性检测器组合以产生强大的多尺度对象检测器。他们的多尺度CNN优于KITTI行人和骑自行车的其他方法（表1b，1c），同时在KITTI车上排名第二（表1a）。 Xiang et al。 （2016）提出了一个使用从3DVP（Xiang et al。（2015b）获得的子类别信息）的区域提案网络，指导建议生成过程和联合检测和子类别分类的检测网络。对象子类别定义为具有类似属性或属性（如外观，姿态或形状）的对象。子类别信息使他们能够胜过KITTI车辆检测任务的所有其他方法（表1a），并在方位估计中达到最佳性能（表2）。

- 5.2. 3D Object Detection from 2D Images 用二维图像完成检测
- Geometric 3D representations of object classes can recover far more details than just 2D or 3D bounding boxes, however most of today’s object detectors are focused on robust 2D matching. Zia et al. (2013) exploit the fact that high-quality 3D CAD models are available for many important classes. From these models, they obtain coarse 3D wireframe models using principal components analysis and train detectors for the vertices of the wireframe. At test time, they generate evidence for vertices by densely applying the detectors. Zia et al. (2015) extend this work by directly using detailed 3D CAD models in their formulation, combining them with explicit representations of likely occlusion patterns. Further, a ground plane is jointly estimated to stabilize the pose estimation process. This extension outperforms the pseudo-3D model of Zia et al. (2013) and shows the benefits of reasoning in true metric 3D space.
- While these 3D representations provide more faithful descriptions of objects they can not yet compete with state-of-theart detectors using 2D bounding boxes. To overcome this problem, Pepik et al. (2015) propose a 3D extension of the powerful deformable part model (Felzenszwalb et al. (2008)), which combines the 3D geometric representation with robust matching to real-world images. They further add 3D CAD information of the object class of interest as geometry cue to enrich the appearance model.
- 对象类的几何3D表示可以恢复比2D或3D边界框更多的细节，然而，今天的大多数对象检测器都集中在鲁棒的2D匹配上。 Zia等人（2013）利用高质量3D CAD模型可用于许多重要课程。从这些模型中，他们使用主成分分析和线框检测器来获取粗略的3D线框模型。在测试时间，它们通过密集地应用检测器产生顶点的证据。 Zia等人（2015）通过在制定中直接使用详细的3D CAD模型来扩展这项工作，将它们与可能的闭塞模式的显式表示相结合。此外，共同估计地平面以稳定姿态估计过程。该扩展优于Zia等人的伪3D模型。 （2013），并显示了在真正的度量3D空间推理的好处。
- 虽然这些3D表示提供了对物体的更忠实的描述，但是它们还不能与使用2D边界框的状态检测器竞争。为了克服这个问题，Pepik（2015）提出了强大的可变形部分模型的3D扩展（Felzenszwalb等人（2008）），其将3D几何表示与实际图像的鲁棒匹配相结合。他们进一步添加感兴趣的对象类的3D CAD信息作为几何提示，以丰富外观模型。

- 5.3. 3D Object Detection from 3D Point Clouds 用三维点群做三维检测
- The KITTI dataset Geiger et al. (2012b) provides synchronized camera and LiDAR frames and allows the comparison of image-based and LiDAR-based approaches on the same data. In contrast to cameras, LiDAR laser range sensors directly provide accurate 3D information which simplifies the extraction of object candidates and can be helpful for the classification task as it provides 3D shape information. However, 3D data from laser scanners is typically sparse and its spatial resolution is limited. Therefore, the state-of-the-art relying only on laser range data can not reach the performance of camera-based detection systems, yet. In Table 3 we show the LiDAR-based state-of-the-art on the KITTI benchmark for object, pedestrian and cyclist detection. The performance is assessed similar to the image-based approaches using the PASCAL intersection over- union by projecting the 3D bounding boxes into the image plane.
- Wang & Posner (2015) propose an efficient scheme to apply the common 2D sliding window detection approach to 3D data. More specifically, they exploit the sparse nature of the problem using a voting scheme to search all possible object locations and orientations. Li et al. (2016b) improve upon these results by exploiting a fully convolutional neural network for detecting vehicles from range data. They represent the data in a 2D point map, and predict an objectness confidence and a bounding box simultaneously using a single 2D CNN. The encoding used to represent the data allows them to predict the full 3D bounding box of the vehicles. Engelcke et al. (2016) leverage a featurecentric voting scheme to implement a novel convolutional layer which exploits the sparsity of the point cloud. Additionally, they propose to use the L1 penalty for regularization.
- Relying on laser range data alone makes the detection task challenging due to the limited density of the laser scans. Thus, existing LiDAR-based approaches perform weaker compared to their image-based counterparts on the KITTI datasets. Chen et al. (2016c) combine LiDAR laser range data with RGB images for object detection. In their approach, the sparse point cloud is encoded using a compact multi-view representation and a proposal generation network utilizes the bird’s eye view representation of the point cloud to generate 3D candidates. Finally, they combine region-wise features from multiple views with a deep fusion scheme as illustration in Figure 8. This approach outperforms the other LiDAR-based approaches by a significant margin and achieves state-of-the-art performance in the KITTI car benchmarks (Tables 1a,3a).
- KITTI数据集Geiger等（2012b）提供了同步的摄像机和LiDAR帧，并允许在相同的数据上比较基于图像和基于LiDAR的方法。与摄像机相反，LiDAR激光测距传感器直接提供精确的3D信息，简化了对象候选的提取，可以为分类任务提供3D形状信息。然而，来自激光扫描仪的3D数据通常很稀疏，其空间分辨率有限。因此，依靠激光测距数据的最先进技术还不能达到基于摄像机的检测系统的性能。在表3中，我们展示了基于LiDAR的最先进的KITTI基准测试对象，行人和骑车人检测。通过将3D边界框投影到图像平面中，性能被评估与使用PASCAL交叉联合的基于图像的方法相似。
- Wang＆Posner（2015）提出了一种将常用2D滑动窗口检测方法应用于3D数据的有效方案。更具体地说，它们使用投票方案来利用问题的稀疏性来搜索所有可能的对象位置和方向。 Li et al。 （2016b）通过利用完全卷积神经网络来检测来自范围数据的车辆来改善这些结果。它们表示2D点图中的数据，并使用单个2D CNN同时预测物体信心和边界框。用于表示数据的编码允许他们预测车辆的完整3D边界框。 Engelcke等人（2016）利用特征中心投票方案来实现利用点云稀疏性的新颖卷积层。此外，他们建议将L1惩罚用于正规化。
- 依靠激光测距数据，由于激光扫描的密度有限，使得检测任务具有挑战性。因此，与基于KITTI数据集的基于图像的对手相比，现有的基于LiDAR的方法表现较弱。chen（2016c）将LiDAR激光测距数据与用于物体检测的RGB图像相结合。在他们的方法中，使用紧凑的多视图表示对稀疏点云进行编码，并且提案生成网络利用点云的鸟瞰图表示来生成3D候选。最后，他们将来自多个视图的区域特征与深度融合方案结合在一起，如图8所示。该方法在显着优势下胜过其他基于LiDAR的方法，并在KITTI汽车基准测试中达到最先进的性能（表1a，3a）。

- 5.4. Person Detection 人物检测
- While so far we have discussed general object detection algorithms, we now focus on specific approaches to person or pedestrian detection which are of high relevance to any autonomous system interacting with a real environment. As human behavior is less predictable than the behavior of a car, reliable person detection is necessary to drive safely in the proximity of pedestrians. The detection of people is particularly difficult because of the large variety of appearances due to different clothing and articulated poses. Furthermore, the articulation and interaction of pedestrians can strongly affect the appearance of pedestrians in case of partial occlusion.
- **Pedestrian Protection Systems**: This problem has been deeply investigated for advanced driver assistance systems to increase road safety. Pedestrian protection systems (PPS) detect the presence of stationary and moving people around a moving vehicle in order to warn the driver against dangerous situations. Even though missed detections of a PPS can still be handled by the driver, the pedestrian detection of an autonomous car needs to be flawless. The pedestrian detection system needs to be robust against all weather conditions and efficient for real-time detection. Geronimo et al. (2010) survey pedestrian detection for Advanced Driver Assistance Systems.
- **Surveys**: Enzweiler&Gavrila (2009) give a very broad overview of different architectures for monocular pedestrian detection. They make the observation that the HOG/SVM combination as proposed by Dalal & Triggs (2005) works well at higher resolutions with higher processing time whereas AdaBoost cascade approaches are superior at lower resolutions, achieving near real-time performance. In their survey, Benenson et al. (2014) found no clear evidence that a certain type of classifier (e.g., SVM or decision forests) is better suited than others. In particular, Wojek & Schiele (2008b) show that AdaBoost and linear SVM perform roughly the same if enough features are given. Moreover, Benenson et al. (2014) observe that part based models like (Felzenszwalb et al. (2008)) improve results only slightly compared to the much simpler approach of Dalal & Triggs (2005). They conclude that the number and diversity of features is clearly an important factor for the performance of classifiers since the classification problem becomes easier with higher dimensional representations. Consequently, today all state-of-the-art pedestrian detection systems use convolutional neural networks and learn feature representations in an end-to-end fashion (Cai et al. (2016); Xiang et al. (2016); Zhu et al. (2016); Yang et al. (2016); Chen et al. (2015c); Ren et al. (2015)).
- 虽然到目前为止，我们已经讨论了一般对象检测算法，但是我们现在关注与人或行人检测的具体方法，这些方法与任何与真实环境相互作用的自治系统都具有高度的相关性。由于人的行为比汽车的行为不太可预测，所以需要可靠的人员检测来安全行驶在行人附近。人的检测是特别困难的，因为由于不同的衣服和关节姿势，出现了各种各样的外观。此外，在部分闭塞的情况下，行人的关节和相互作用可能会严重影响行人的出现。
- 行人保护系统：这个问题已经深入调查了先进的驾驶员辅助系统，以增加道路安全。行人保护系统（PPS）检测移动车辆周围的固定和移动人员的存在，以警告驾驶员处于危险状况。虽然驾驶员仍然可以处理PPS的错误检测，但自行车的行人检测需要完美无缺。行人检测系统需要对所有天气状况都很强大，对于实时检测是有效的。 Geronimo等人（2010年）高级驾驶员辅助系统行人检测调查。
- 调查：Enzweiler＆Gavrila（2009）给出了单眼行人检测的不同架构的非常广泛的概述。他们提出，Dalal＆Triggs（2005）提出的HOG / SVM组合在更高的分辨率下处理时间更长，而AdaBoost级联方法在较低分辨率下更好，实现近乎实时的性能。在他们的调查中，Benenson等（2014）发现没有明确的证据表明某种类型的分类器（例如SVM或决策树）比其他分类器更适合。特别地，Wojek＆Schiele（2008b）表明，如果给出足够的特征，AdaBoost和线性SVM执行大致相同。此外，Benenson等（2014）观察到，像Felzenszwalb等人（2008）这样的部分模型比Dalal＆Triggs（2005）更简单的方法略有提高。他们得出结论，特征的数量和多样性显然是分类器性能的重要因素，因为分类问题变得更容易，具有更高的维度表示。因此，今天所有最先进的行人检测系统都使用卷积神经网络，并以端到端的方式学习特征表征（Cai et al。（2016）; Xiang et al。（2016）; Zhu et al （2016）; Yang et al。（2016）; Chen et al。（2015c）; Ren et al。（2015））。

- **Temporal Cues**: Similarly, Shashua et al. (2004) point out the importance of good features for the person detection task. They noted that the integration of additional cues measured over time (dynamic gait, motion parallax) and situation specific features (such as leg positions at certain poses) are key for reliable detection. Wojek et al. (2009) notice that most pedestrian detection systems rely only on a single image as input and do not exploit the available temporal information of objects in video sequences. They show significant improvement in detection performance by incorporating motion cues and combining different complementary feature types.
- **Scarcity of Target Class**: The enlargement of training data allows to train sophisticated models for the detection problem. However, the generation of examples belonging to the target class is usually time consuming because of manual labeling while many negative examples can be easily obtained. Enzweiler & Gavrila (2008) address the bottleneck caused by the scarcity of samples of the target class. They create synthesized virtual samples with a learned generative model to enhance a discriminative model. The generative model captures prior knowledge about the pedestrian class and allows significant improvement in the classification performance.
- **Real-time Pedestrian Detection**: In case of a potential collision with pedestrians a fast detection allows early intervention of the autonomous system. Benenson et al. (2012) provide fast and high quality pedestrian detections based on better handling of scales and exploiting depth extracted from stereo. Instead of resizing the images, they scale HOG features similar to Viola & Jones (2004). The Stixel World representation (Badino et al. (2009)) provides depth information which allows to significantly reduce the search space and detect pedestrians at 80 Hz in a parallel framework.
- 时间线：同样，Shashua等（2004）指出了人员检测任务的良好功能的重要性。他们指出，随时间推移的额外提示的集成（动态步态，运动视差）和情况特定特征（例如某些姿势下的腿部位置）是可靠检测的关键。 Wojek（2009）指出，大多数行人检测系统仅依赖于单个图像作为输入，并且不会利用视频序列中对象的可用时间信息。它们通过结合运动线索和组合不同的互补特征类型，显着提高了检测性能。
- 目标类的稀缺：扩大训练数据可以训练出复杂的模型用于检测问题。然而，属于目标类的示例的生成通常是耗时的，因为手工标记，而许多负面示例可以容易地获得。 Enzweiler＆Gavrila（2008）解决了目标课程样本稀缺所造成的瓶颈。他们使用学习的生成模型创建合成的虚拟样本，以增强歧视性模型。生成模型捕获有关行人阶级的先前知识，并且可以显着提高分类性能。
- 实时行人检测：如果与行人有潜在的碰撞，快速检测可以让自主系统的早期干预。 Benenson（2012）基于更好地处理尺度和利用从立体声提取的深度提供快速和高质量的行人检测。他们不是调整图像大小，而是缩放与“Viola＆Jones”相似的HOG功能（2004）。 Stixel World表示（Badino等人（2009））提供了深度信息，允许在并行框架中显着减少搜索空间并检测80Hz的行人。

- 5.5. Human Pose Estimation 人物姿态估计
- The pose and gaze of a person provides important information to the autonomous vehicle about the behavior and intention of the person. However, the pose estimation problem is challenging since the pose space is very large and typically people can only be observed on low resolutions, because of their size and distance to the vehicle. Several approaches have been proposed to jointly estimate the pose and body parts of a person. Traditionally, a two-staged approach was used by first detecting body parts and then estimating the pose as in (Pishchulin et al.(2012); Gkioxari et al. (2014); Sun & Savarese (2011)). This is problematic in cases when people are in proximity of each other because body-parts can be wrongly assigned to different instances.
- Pishchulin et al. (2016) present DeepCut, a model which jointly estimates the poses of all people in an image. The formulation is based on partitioning and labeling a set of bodypart hypotheses obtained from a CNN-based part detector. The model jointly infers the number of people, their poses, spatial proximity and part level occlusions. Bogo et al. (2016) use DeepCut to estimate the 3D pose and 3D shape of a human body from a single unconstrained image. SMPL, a 3D body shape model proposed by Loper et al. (2015), is fit to predictions of the 2D body joint locations from DeepCut. SMPL captures correlations in human shape across the population which allows to robustly fit human poses even in the presence of weak observations.
- 一个人的姿势和目光向自治车辆提供关于该人的行为和意图的重要信息。然而，姿势估计问题是具有挑战性的，因为姿态空间非常大，并且通常人们只能以低分辨率观察，因为它们的大小和与车辆的距离。已经提出了几种方法来共同估计一个人的姿势和身体部位。传统上，通过首先检测身体部位然后估计姿势（Pishchulin等人（2012）; Gkioxari等人（2014）; Sun＆Savarese（2011））采用了两阶段方法。在人们彼此接近的情况下，这是有问题的，因为身体部位可能被错误地分配给不同的实例。
- Pishchulin等（2016）目前DeepCut是一个共同估计图像中所有人的姿势的模型。该公式基于从基于CNN的部件检测器获得的一组bodypart假设的划分和标记。该模型共同推断了人数，姿势，空间接近度和部分等级闭塞。 Bogo等人（2016）使用DeepCut从单个无约束图像估计人体的3D姿态和3D形状。 SMPL，由Loper等人提出的3D体形模型（2015）适用于DeepCut的2D身体关节位置的预测。 SMPL捕获人口形态的相关性，即使在存在弱观察的情况下，也可以强制适应人类的姿势。

- 5.6. Discussion 讨论
- Object detection works already quite well in case of high resolution with little occlusions. For the easy and moderate cases of the car detection task (Table 1a) many methods show impressive performance. The pedestrian and cyclist detection task (Tables 1b,1c) is more challenging and thus weaker overall performance can be observed. One reason for this is the limited number of training examples and the possibility of confusing cyclists and pedestrians which differ only via their context and semantics. Remaining major problems across tasks are detection of small objects and highly occluded objects. In the leaderboards this manifests in a significant drop in performance when comparing easy, moderate and hard examples. Qualitatively, this can be observed in Figures 9, 10,11 where we show typical estimation errors of the best performing methods on the KITTI dataset. A major source of errors are crowds of pedestrians, groups of cyclists and lines of cars that cause many occlusions and lead to missing detections for all methods. Furthermore, a large amount of distant objects needs to be detected in some cases which is still a challenging task for modern methods since the amount of information provided by these objects is very low.
- 在高分辨率的情况下，对象检测工作已经很好了，几乎没有遮挡。对于容易和适度的汽车检测任务（表1a），许多方法表现出令人印象深刻的性能。行人和骑车人员检测任务（表1b，1c）更具挑战性，因此可以观察到整体性能较差。这样做的一个原因是训练样本数量有限，以及混淆骑自行车者和行人的可能性，只有通过上下文和语义不同。任务中仍然存在的主要问题是检测小物体和高遮蔽物体。在排行榜中，当比较简单，中等和难度的例子时，这表现出显着的性能下降。定性地，这可以在图9,10,11中进行观察，其中我们显示KITTI数据集上最佳性能方法的典型估计误差。错误的主要原因是行人群，骑自行车的人群和汽车线，导致许多障碍，导致所有方法的检测失踪。此外，在某些情况下需要检测大量的遥远物体，这对现代方法来说仍然是一项具有挑战性的任务，因为这些物体所提供的信息量非常低。


### 6. Semantic Segmentation
- Semantic segmentation, is a fundamental topic in computer vision. The goal of semantic segmentation is to assign each pixel in the image a label from a predefined set of categories. The task is illustrated in Figure 12 with all pixel of a certain category colorized in as specific color in a scene of the Cityscapes dataset by Cordts et al. (2016) recorded in Zurich. Segmentation of images into semantic regions usually found in street scenes, such as cars, pedestrians, or road affords a comprehensive understanding of the surrounding which is essential to autonomous navigation. Challenges of semantic segmentation arise from the complexity of the scene and the size of the label space.
- 语义分割是计算机视觉中的一个基本课题。 语义分割的目标是为图像中的每个像素分配来自预定义类别集合的标签。 该任务在图12中示出，Cordts等人在Cityscapes数据集的场景中以特定颜色的所有像素着色。 （2016年）记录在苏黎世。 将图像分割成通常在街道场景中发现的语义区域，例如汽车，行人或道路，可以全面了解对自主导航至关重要的周边环境。 语义分割的挑战源于场景的复杂性和标签空间的大小。

- **Formulation**: Traditionally, the semantic segmentation problem was posed as maximum a posteriori (MAP) inference in a conditional random field (CRF), defined over pixels or super pixels (He et al. (2004, 2006)). However, these early formulations were not efficient and could only handle only datasets of limited size and a small number of classes. Furthermore, only very simple features such as color, edge and texture information have been exploited. Shotton et al. (2009) observed that more powerful features can significantly boost performance and proposed an approach based on a novel type of features called texture-layout filter that exploits the textural appearance of objects, its layout and textural context. They combine texturelayout filters with lower-level image features in a CRF to obtain pixel-level segmentations. Randomized boosting and piecewise training techniques are exploited to efficiently train the model.
- Hierarchical and long-range connectivity, as well as higherorder potentials defined on image regions were considered to tackle the limited ability of CRFs to model long-range interactions within the image. However, methods based on image regions (He et al. (2004); Kumar & Hebert (2005); He et al. (2006); Kohli et al. (2009); Ladicky et al. (2009, 2014)) are restricted by the accuracy of the image segmentations used as input. In contrast, Krahenbuhl & Koltun (2011) propose a highly efficient inference algorithm for fully connected CRF models which models pairwise potentials between all pairs of pixels in the image.
- 制定：传统上，语义分割问题被构成为在像素或超像素上定义的条件随机场（CRF）中的最大后验（MAP）推理（He et al。（2004，2006））。 然而，这些早期方法并不有效，只能处理有限大小和少数类的数据集。 此外，只有非常简单的特征，如颜色，边缘和纹理信息已被利用。 Shotton等人 （2009）观察到，更强大的功能可以显着提高性能，并提出了一种基于一种新颖的功能类型的方法，称为纹理布局过滤器，利用了对象的纹理外观，其布局和纹理上下文。 它们将纹理布局过滤器与CRF中的较低级别图像特征相结合，以获得像素级分割。 利用随机提升和分段训练技术有效地训练模型。
- 被认为分层和远距离连接以及图像区域定义的更高阶电位可以解决CRF在图像中建立长距离相互作用的有限能力。 然而，基于图像区域的方法（He et al。（2004）; Kumar＆Hebert（2005）; He et al。（2006）; Kohli et al。（2009）; Ladicky et al。（2009，2014）） 受用作输入的图像分割的准确性的限制。 相比之下，Krahenbuhl＆Koltun（2011）为完全连接的CRF模型提出了一种高效的推理算法，它们在图像中的所有像素对之间建模成对的电位。

- The methods so far consider each object class independently while the co-occurrence of object classes can be an important clue for semantic segmentation, for example cars are more likely to occur in a street scene than in an office. Consequently, Ladicky et al. (2010) propose to incorporate object class cooccurrence as global potentials in a CRF. They show how these potentials can be efficiently optimized using a graph cut algorithm and demonstrate improvements over simpler pairwise models.
- The success of deep convolutional neural networks for image classification and object detection has sparked interest in leveraging their power for solving the pixel-wise semantic segmentation task. The fully convolutional neural network (Long et al. (2015)) is one of the earliest works which applies CNNs to the image segmentation problem. However, while modern convolutional neural networks for image classification combine multi-scale contextual information by consecutive pooling and subsampling layers that lower the resolution, semantic segmentation requires multi-scale contextual reasoning together with full-resolution dense prediction. In the following we will review recent approaches which address this problem.
- We focus the comparison of different semantic segmentation approaches on the Cityscapes dataset by Cordts et al. (2016) described in Section 2 because of the autonomous driving context. Table 4a shows the leaderboard of Cityscapes for the pixel-level semantic labeling task. The intersection-over union metric is provided for two semantic granularities, i.e., classes and categories, and additionally the instance-weighted IoU is reported for both granularities to penalize methods ignoring small instances.
- 迄今为止，这些方法独立地考虑每个对象类，而对象类的共现可能是语义分割的重要线索，例如，汽车在街景场景中比在办公室中更可能发生。 因此，Ladicky et al。 （2010）提出将对象类并发作为全球潜力纳入CRF。 他们展示了如何使用图形切割算法有效地优化这些潜力，并通过简单的成对模型来证明改进。
- 用于图像分类和物体检测的深卷积神经网络的成功引发了兴趣，利用他们的力量来解决像素方面的语义分割任务。 完全卷积神经网络（Long et al。（2015））是将CNN应用于图像分割问题的最早的作品之一。 然而，虽然用于图像分类的现代卷积神经网络通过连续的合并和降低分辨率的子采样层组合多尺度上下文信息，语义分割需要多尺度上下文推理以及全分辨率密集预测。 在下文中，我们将回顾最近解决这个问题的方法。
- Cordts等人对Cityscapes数据集的不同语义分割方法进行了比较。 （2016）由于自主驾驶环境而在第2节中描述。 表4a显示了Cityscapes针对像素级语义标注任务的排行榜。 提供了交叉联合度量用于两个语义粒度，即类和类别，另外为两个粒度报告了实例加权的IoU，以惩罚忽略小实例的方法。

- **Structured CNNs**: Recently, several methods have been proposed to tackle the opposing needs of multi-scale inference and full-resolution prediction output. Dilated convolutions have been proposed (Chen et al. (2015b); Yu & Koltun (2016)) to enlarge the receptive field of neural networks without loss of resolution. Their operation corresponds to regular convolution with dilated filters which allows for efficient multi-scale reasoning while limiting the increase in the number of model parameters.
- In the SegNet model, Badrinarayanan et al. (2015) have replaced the traditional decoder in a deep architecture with a network which consists of a hierarchy of decoders one corresponding to each encoder. Each decoder maps a low resolution feature map of an encoder (max-pooling layer) to a higher resolution feature map. In particular, the decoder in their model takes advantage of the pooling indices computed in the max-pooling step of the corresponding encoder to implement the upsampling process. This eliminates the need to learn the upsampling and thus results in a smaller number of parameters. Furthermore, sharper segmentation boundaries have been demonstrated using this approach.
- 结构化CNN：最近，已经提出了几种方法来解决多尺度推理和全分辨率预测输出的相反需求。 已经提出了扩张卷积（Chen et al。（2015b）; Yu＆Koltun（2016））来扩大神经网络的接受场而不损失分辨率。 它们的操作对应于具有扩张过滤器的常规卷积，其允许有效的多尺度推理，同时限制模型参数的数量的增加。
- 在SegNet模型中，Badrinarayanan等 （2015）已经用深层架构代替了传统的解码器，网络由一个对应于每个编码器的解码器层次组成。 每个解码器将编码器的低分辨率特征图（最大池）映射到更高分辨率的特征图。 特别地，其模型中的解码器利用在相应编码器的最大汇集步骤中计算的汇集指数来实现上采样过程。 这消除了学习上采样的需要，从而导致较少数量的参数。 此外，已经使用这种方法证明了更清晰的分割边界。

- While activation maps at lower-levels of the CNN hierarchy lack object category specificity, they do contain higher spatial resolution information. Ghiasi & Fowlkes (2016) leverage this assumption and propose to construct a Laplacian pyramid based on a fully convolutional network. Aggregating information at multiple scales allows them to successively refine the boundary reconstructed from lower-resolution layers. They achieve this by using skip connections from higher resolution feature maps and multiplicative confidence gating, penalizing noisy high-resolution outputs in regions where the low-resolution predictions have high confidence. With this approach Ghiasi&Fowlkes (2016) achieve competitive results on Cityscapes Table 4a.
- One of the best performing methods on Cityscapes was proposed by Zhao et al. (2016) using a pyramid scene parsing network, illustrated in Figure 13, to incorporate global context information into the pixel-level prediction task. Specifically, they apply a pyramid parsing module to the last convolutional layer of a CNN which fuses features of several pyramid scales to combine local and global context information. The resulting representation is fed into a convolution layer to obtain final per-pixel predictions.
- 虽然CNN层次较低层的激活映射缺少对象类别的特异性，但它们确实包含较高的空间分辨率信息。 Ghiasi＆Fowlkes（2016）利用这一假设，并提出构建基于完全卷积网络的拉普拉斯金字塔。 在多个尺度上聚合信息使得它们可以连续细化从较低分辨率层重建的边界。 他们通过使用来自较高分辨率特征图的跳跃连接和乘法置信门控来实现这一点，在低分辨率预测具有高置信度的地区惩罚嘈杂的高分辨率输出。 通过这种方式，Ghiasi＆Fowlkes（2016）在“城市风景”表4a中获得了竞争优势。
- Zhao等人提出了Cityscapes最佳表现方法之一。 （2016），使用如图13所示的金字塔场景解析网络，以将全局上下文信息并入到像素级预测任务中。 具体来说，它们将金字塔解析模块应用于CNN的最后卷积层，该融合层融合了几个金字塔尺度的特征以组合局部和全局上下文信息。 所得到的表示被馈送到卷积层以获得最终的每像素预测。

- Simonyan & Zisserman (2015) and Szegedy et al. (2015) have shown that the depth of a CNN is crucial to represent rich features. However, increasing the depth of a network lead to the saturation and degradation of the accuracy. He et al. (2016) propose deep residual learning framework (ResNet) to address this problem. They let each stacked layer learn a residual mapping instead of the original, unreferenced mapping. This allows them to train deeper networks with improving accuracy while plain networks (simply stacked networks) exhibited higher training errors. Pohlen et al. (2016) present a ResNet-like architecture that provides strong recognition performance while preserving high-resolution information throughout the entire network by combining two different processing streams. One stream passes through a sequence of pooling layers, whereas the other stream processes feature maps at full image resolution. The two processing streams are combined at the full image resolution using residuals. Wu et al. (2016b) have proposed a more efficient ResNet architecture by analyzing the effective depths of residual units. They point out that ResNets behave as linear ensembles of shallow networks. Based on this understanding they design a group of relatively shallow convolutional networks for the task of semantic image segmentation. While Pohlen et al. (2016) achieve competitive results on Cityscapes (Table 4a), Wu et al. (2016b) outperform all others in all measures besides the instance-weighted class-level IoU.
- Simonyan＆Zisserman（2015）和Szegedy等人（2015）表明，CNN的深度对于表现丰富的特征至关重要。然而，增加网络的深度导致饱和度和精度的降低。他等（2016）提出了深层次的残留学习框架（ResNet）来解决这个问题。它们使每个堆叠层学习残差映射，而不是原始的未引用映射。这允许他们在提高准确性的同时训练更深入的网络，而简单的网络（简单的堆叠网络）展示更高的训练误差。 Pohlen等人（2016）提出了一种类似ResNet的架构，通过组合两种不同的处理流，可以在整个网络中保持高分辨率信息，提供强大的识别性能。一个流通过一系列池池，而另一个流以完整图像分辨率处理特征图。两个处理流以全图像分辨率使用残差进行组合。吴等（2016b）通过分析剩余单位的有效深度，提出了一种更有效的ResNet架构。他们指出ResNets表现为浅网络的线性集合。基于这一理解，他们为语义图像分割任务设计了一组相对浅的卷积网络。而Pohlen等人（2016）在“城市景观”中获得了竞争优势（表4a），Wu et al。 （2016b）除了实例加权的类水平IoU之外的所有措施都胜过所有其他。

- **Conditional Random Fields**: A different way to address the needs of multi-scale inference and full resolution prediction is the combination of CNNs with CRF models. Chen et al.(2015b) propose to refine the label map obtained using a convolutional neural network using a fully connected CRF model (Krahenbuhl & Koltun (2011)). The CRF allows to capture fine details based on the raw RGB input which are missing in the CNN output due to the limited spatial accuracy of the CNN model. In similar spirit, Jampani et al. (2016) generalize bilateral filters and unroll the CRF program which allows for end-to-end training of the (generalized) filter parameters from data. This effiectively allows for reasoning over larger spatial regions within one convolutional layer by leveraging input features as a guiding signal.
- Inspired by higher order CRFs for semantic segmentation, Gadde et al. (2016a) propose a new Bilateral Inception module for CNN architectures as an alternative to structured CNNs and CRF techniques. They use the assumption that pixels which are spatially and photometrically similar are more likely to have the same label. This allows them to directly learn long-range interactions, thereby removing the need for post-processing using CRF models. Specifically, the proposed modules propagate edge-aware information between distant pixels based on their spatial and color similarity, incorporating the spatial layout of superpixels. Propagation of information is achieved by applying bilateral filters with Gaussian kernels at various scales.
- 条件随机场：解决多尺度推理和全分辨率预测需求的不同方法是将CNN与CRF模型相结合。 Chen等人（2015b）提出使用完全连接的CRF模型（Krahenbuhl＆Koltun（2011））来改进使用卷积神经网络获得的标签图。 由于CNN模型的空间精度有限，CRF可以根据CNN输出中缺少的原始RGB输入来捕获细节。 类似的精神，Jampani等人 （2016）推广双边筛选器并展开CRF程序，允许对来自数据的（广义）过滤器参数进行端对端培训。 这有效地允许通过利用输入特征作为引导信号来推理在一个卷积层内的较大空间区域。
- 灵感来自语义分割的高阶CRF，Gadde et al。 （2016a）提出了一种用于CNN架构的新的双边入局模块，作为结构化CNN和CRF技术的替代方案。 他们使用这样的假设：空间和光度相似的像素更可能具有相同的标签。 这允许他们直接学习远程交互，从而消除了使用CRF模型进行后处理的需要。 具体地说，所提出的模块基于它们的空间和颜色相似性，在远距离像素之间传播边缘感知信息，并结合超像素的空间布局。 通过在各种尺度上应用具有高斯核的双边滤波器来实现信息的传播。

- **Discussion**: The focus on multi-scale inference of recent methods led to impressive results in pixel-level semantic segmentation on Cityscapes. Today, the top methods in Cityscapes Table 4b reach an impressive IoU of almost 81% over classes and 91% over categories. In contrast, the instance-weighted IoU is always below 58% over classes and 80% over categories. This indicates that semantic segmentation works well with instances covering large image areas but is still problematic with instances covering small regions. Similarly to the detection in low resolutions discussed in Section 5.6, small regions provide only little information to assign the correct label. Furthermore segmenting out small, and possibly occluded objects is a challenging task which might require novel approaches to jointly perform depth estimation and depth-adaptive recognition.
- 讨论：对近期方法的多尺度推论的关注导致了Cityscapes中像素级语义分割的令人印象深刻的结果。 今天，“城市风景”表4b中的顶级方法达到令人印象深刻的几率，达到81％，超过类别91％。 相比之下，实例加权IoU总是低于58％，超过类别80％。 这表明语义分割对于覆盖大图像区域的实例很好，但是对于覆盖小区域的实例仍然是有问题的。 类似于5.6节中讨论的低分辨率检测，小区域只提供很少的信息来分配正确的标签。 此外，分割出小的可能闭塞的物体是一项具有挑战性的任务，可能需要新颖的方法来共同执行深度估计和深度自适应识别。

- 6.1. Semantic Instance Segmentation  语义实例分割
- The goal of semantic instance segmentation is simultaneous detection, segmentation and classification of every individual object in an image. Unlike semantic segmentation, it provides information about the position, semantics, shape and count of individual objects, and therefore has many applications in autonomous driving. For the task of semantic instance segmentation, there exist two major lines of research: Proposal-based and proposal-free instance segmentation.
- In Table 4b we show the leaderboard of semantic instance segmentation methods on the Cityscapes dataset. The performance is assessed with the average precision on the region level averaged across a range of overlap thresholds (AP), for an overlap value of 50 % (AP 50%) and for objects within 100 m and 50 m (AP 100m, AP 50m).
- 语义实例分割的目标是对图像中每个单独对象进行同时检测，分割和分类。 与语义分割不同，它提供了关于单个对象的位置，语义，形状和数量的信息，因此在自主驾驶中具有许多应用。 对于语义实例分割的任务，存在两个主要研究领域：基于提案和无提议的实例分割。
- 在表4b中，我们在Cityscapes数据集上显示语义实例分割方法的排行榜。 对于重叠值为50％（AP 50％）和100米和50米范围内的物体（AP 100米，AP 50米），区域水平平均精度在平均重叠阈值（AP）范围内进行评估）。

- **Proposal-based Instance Segmentation**: Proposal-based instance segmentation methods extract class-agnostic proposals which are classified as an instance of a certain semantic class in order to obtain pixel-level instance masks. Region proposals like Multiscale Combinatorial Grouping (Arbel´aez et al. (2014)) can be directly used as instance segments. Coarser representations such as bounding boxes need further refinement to obtain the instance mask. Unfortunately, proposal-based algorithms are slow at inference time due to the computationally expensive proposal generation step. To avoid this bottleneck, Dai et al. (2016) propose a fully convolutional network with three stages. They extract box proposals, use shared features to refine these to segments, and finally classify them into semantic categories. The causal relations between the outputs of the stages complicate training of the multi-task cascade. However, the authors show how these difficulties can be overcome using a differentiable layer which allows for training the whole model in an end-to-end fashion.
- Proposal-based instance segmentation methods that use proposals in the form of bounding boxes to predict a binary segmentation mask are sensitive to errors in the proposal generation process including wrongly scaled or shifted bounding boxes. To tackle this problem, Hayder et al. (2016) present a new object representation. More specifically, they propose a shape aware object mask network that predicts a binary mask for each bounding box proposal, potentially extending beyond the box itself. They integrate the object mask network into the Multitask Network Cascade framework of Dai et al. (2016) by replacing the original mask prediction stage. The shape aware approach is the second best performing method on Cityscapes(Table 4b).
- 基于提案的实例分段：基于提案的实例分段方法提取类别不可知的提案，被分类为某个语义类的实例，以获得像素级实例掩码。区域建议如多尺度组合分组（Arbel'aez et al。（2014））可直接用作实例。较粗糙的表示，例如边界框需要进一步细化以获得实例掩码。不幸的是，由于计算昂贵的建议生成步骤，基于提议的算法在推理时间较慢。为了避免这个瓶颈，Dai等（2016）提出了一个具有三个阶段的完全卷积网络。他们提取框提案，使用共享功能将这些细分为细分，最后将它们分类为语义类别。这些阶段的产出之间的因果关系使多任务级联的训练复杂化。然而，作者展示了如何使用可微分层来克服这些难题，从而可以以端对端的方式对整个模型进行训练。
- 使用基于边界框形式的提案以预测二进制分割掩码的基于投标的实例分割方法对提案生成过程中的错误（包括错误缩放或移位的边界框）敏感。 为了解决这个问题，Hayder等 （2016）提出了一个新的对象表示。 更具体地说，它们提出了一种形状感知对象掩模网络，其预测每个边界框提案的二进制掩码，潜在地延伸超出框本身。 他们将对象掩码网络集成到Dai等人的多任务网络级联框架中。 （2016）通过替换原始的掩模预测阶段。 形状感知方法是Cityscapes第二好的表现方法（表4b）。

- Proposal-free Instance Segmentation: Recently, a number of alternative methods to proposal-based instance segmentation have been proposed in the literature. These methods jointly infer the segmentation and the semantic category of individual instances by casting instance segmentation directly as a pixel labeling task.
- Zhang et al. (2015, 2016c) train a fully convolutional neural networks (FCN) to directly predict pixel-level instance segmentation while the instance ID encodes a depth ordering. They improve the predictions and enforce consistency with a subsequent Markov Random Field. Uhrig et al. (2016) propose a method based on FCN to jointly predict semantic segmentation as well as depth and an instance-based direction relative to the centroid of each instance. The instance segmentation pipeline is illustrated in Figure 14. However, they require ground-truth depth data for training their model. Kirillov et al. (2016) present a proposal-free method which combines semantic segmentation and object boundary detection via global reasoning in a multi-cut formulation to infer semantic instance segmentation. Bai & Urtasun (2016) combine intuitions from classical watershed transform and deep learning to create an energy map where the basins corresponds to object instances. This allows them to cut at a single energy level to obtain an pixel-level instance segmentation. Kirillov et al. (2016) and Bai & Urtasun (2016) both achieve competitive results on Cityscapes (Table 4b). However, Arnab & Torr (2017) outperform all others by feeding an initial semantic segmentation into an instance subnetwork. Specifically, the initial category-level segmentation is used along cues from the output of an object detector within an end-to-end CRF to predict pixel-level instances.
- 无提议实例分割：最近，文献中提出了一些基于提案的实例分割的替代方法。 这些方法通过直接将实例分割作为像素标注任务来共同推断单个实例的分割和语义类别。
- Zhang et al（2015，2016c）训练完全卷积神经网络（FCN）直接预测像素级实例分割，而实例ID编码深度排序。他们改进了预测，并强制与随后的马尔可夫随机场一致。 Uhrig等人（2016）提出了一种基于FCN的方法来共同预测语义分割以及相对于每个实例的重心的深度和基于实例的方向。实例分割管线如图14所示。但是，它们需要用于训练其模型的地面真相深度数据。基里洛夫等人（2016）提出了一种无提议方法，通过全局推理将语义分割和对象边界检测结合在一个多切分公式中，以推断语义实例分割。 Bai＆Urtasun（2016）结合了经典流域变换和深度学习的直觉，创建了一个能量图，其中盆地对应于对象实例。这允许他们在单个能级切割以获得像素级的实例分割。基里洛夫等人（2016年）和Bai＆Urtasun（2016年）都在Cityscapes上取得了竞争力的结果（表4b）。然而，Arnab＆Torr（2017）通过将初始语义分割提供给实例子网络来胜过所有其他方式。具体来说，初始类别级别的分段被用于端到端CRF内的对象检测器输出的提示，以预测像素级实例。

- **Discussion**: The instance segmentation task is much more difficult than the semantic segmentation task. Each instance need to be carefully annotated separately whereas in semantic segmentation groups of one semantic class can be annotated together when they occur next to each other. In addition, the number of instance varies greatly between different images. In the autonomous driving context often a wide view is present. Therefore, a large number of instances that appear are rather small in the image making them challenging to detect. In contrast to bounding boxes discussed in Section 5.6, the exact shape of each object instance needs to be inferred in this task. For these reasons, the state-of-the-art is still struggling with the Cityscape dataset (Table 4b) reaching an average precision of 20% or less.
- 讨论：实例分割任务比语义分割任务困难得多。 每个实例需要分别仔细注释，而在语义分割中，一个语义类的组可以在彼此相邻发生时一起注释。 另外，实例的数量在不同的图像之间变化很大。 在自主驾驶环境中，通常有广泛的观点。 因此，出现的大量实例在图像中相当小，使得它们具有挑战性。 与第5.6节讨论的边界框相反，每个对象实例的确切形状需要在此任务中推断出来。 由于这些原因，最先进的技术仍然在与Cityscape数据集（表4b）挣扎，达到20％以下的平均精度。...

- 6.2. Label Propagation 标签传播
- Creating large-scale image datasets with highly accurate pixel-level annotations is labor intensive, and thus very expensive to obtain the desired degree of quality. Semi-supervised methods for annotation of video sequences can help to reduce this cost. Compared to annotating individual images, video sequences offer the advantage of temporal consistency between consecutive frames. Label propagation techniques take advantage of this fact by propagating annotations from a small set of annotated keyframes to all unlabeled frames based on color
information and motion estimates.
- 创建具有高精度像素级注解的大规模图像数据集是劳动密集型的，因此获得所需的质量程度非常昂贵。 用于注释视频序列的半监督方法可以帮助降低成本。 与注释单个图像相比，视频序列提供连续帧之间的时间一致性的优点。 标签传播技术通过将注释从一小部分注释关键帧传播到基于颜色的所有未标记的帧来利用这一事实
信息和运动估计。

- Towards this goal, Badrinarayanan et al. (2010) propose a coupled Bayesian network for joint modeling of the image sequence and pixel-wise labels. Specifically, they employ a propagation scheme based on correspondences obtained from image patch based similarities and semantically consistent regions to transfer label information to unlabeled frames between annotated keyframes. Budvytis et al. (2010) extend this approach by proposing a hybrid model of the generative propagation introduced in Badrinarayanan et al. (2010) as well as a discriminative classification stage which tackles occlusions and dis-occlusions, and allows to propagate over larger time frames. To correct erroneous label propagation, Badrinarayanan et al. (2014) propose a superpixel based mixture-of-tree model for temporal correlation. Vijayanarasimhan & Grauman (2012) tackle the
problem of selecting the most promising keyframes for manual labeling such that the expected propagation error is minimized.
- 为实现这一目标，Badrinarayanan等 （2010）提出了一种耦合贝叶斯网络，用于图像序列和像素方向标签的联合建模。 具体地说，它们采用基于从基于图像块的相似性和语义上一致的区域获得的对应的传播方案来将标签信息转移到带标注的关键帧之间的未标记的帧。 Budvytis等人 （2010）通过提出在Badrinarayanan等人引入的生成繁殖的混合模型来扩展这种方法。 （2010）以及解决闭塞和闭塞的歧视性分类阶段，并允许在更大的时间范围内传播。 为了纠正错误的标签传播，Badrinarayanan等 （2014）提出了一种用于时间相关的基于超像素的树混合模型。 Vijayanarasimhan＆Grauman（2012）解决了
选择用于手动标签的最有希望的关键帧的问题，使得期望的传播误差最小化。

- While the aforementioned methods transfer annotations in 2D, Chen et al. (2014); Xie et al. (2016) propose to annotate directly in 3D and then transfer these annotations into the image domain. Given a source of 3D information (e.g., stereo, laser), these approaches are able to produce improved semantic accuracy and time coherent labels while limiting annotation costs. Towards this goal, Chen et al. (2014) use annotations from KITTI (Geiger et al. (2013)) and leverage 3D car CAD models to infer separate figure-ground segmentations for all cars in the image. In contrast, Xie et al. (2016) reason jointly about all objects in the scene and also handle categories for which CAD models or 3D point measurements are unavailable. To this end, they propose a non-local CRF model which reasons jointly about semantic and instance labels of all 3D points and pixels in the image.
- 虽然上述方法在2D中传输注释，但Chen等（2014）; 谢等人 （2016）建议直接在3D中注释，然后将这些注释传输到图像域中。 给定3D信息的来源（例如，立体声，激光），这些方法能够产生改进的语义准确度和时间相干标签，同时限制注释成本。 为了实现这一目标，Chen et al。 （2014）使用KITTI（Geiger等人（2013））的注释，并利用3D汽车CAD模型推测图像中所有汽车的独立图形分割。 相比之下，谢等人 （2016）共同理解场景中的所有对象，并处理CAD模型或3D点测量不可用的类别。 为此，他们提出了一种非本地CRF模型，它们共同对图像中所有3D点和像素的语义和实例标签进行了理解。

- 6.3. Semantic Segmentation with Multiple Frames 多帧语义分割
- Semantic segmentation from movable platforms such as autonomous vehicles has become an active area of research due to the need of autonomous systems for recognizing their surrounding environment. As such systems are typically equipped with video cameras, temporal correlation between adjacent frames can be exploited to improve segmentation accuracy, efficiency and robustness.
- Towards this goal, Floros & Leibe (2012) propose graphical models operating on video sequences in order to enforce temporal consistency between frames. Specifically, they have proposed a CRF where temporal consistency between consecutive video frames is ensured by linking corresponding image pixels to the inferred 3D scene points obtained by Structure from Motion (SfM). Compared to an image-only baseline they achieve an improved segmentation performance and observe a good generalization to varying image conditions.
- 自动车辆等移动平台的语义分割已经成为一个活跃的研究领域，由于需要自主系统来识别周围的环境。 由于这样的系统通常配备有摄像机，因此可以利用相邻帧之间的时间相关性来提高分割精度，效率和鲁棒性。
- 为了实现这一目标，Floros＆Leibe（2012）提出了对视频序列进行操作的图形模型，以便强化帧之间的时间一致性。 具体来说，他们提出了一种CRF，其中通过将对应的图像像素与通过运动结构（SfM）获得的推断的3D场景点相关联来确保连续视频帧之间的时间一致性。 与仅基于图像的基线相比，它们实现了改进的分割性能，并观察到不同图像条件的良好泛化。

- 3D reconstruction works relatively well for static scenes but is still an open problem in dynamic scenes. Feature-sensitive CRF models have been very successful in semantic image segmentation but the considered distance measure does not appropriately model spatiotemporal correspondences. The presence of both scene and camera motion makes temporal association in videos a challenging task. Because of the possibility of significant optical flow due to such motions, Euclidean distance in the space-time volume is not a good surrogate for correspondence. To tackle this problem, Kundu et al. (2016) propose a method for optimizing the feature space of a dense CRF for spatiotemporal regularization. Specifically, the feature space is optimized such that distances between features associated with corresponding points are minimized using correspondences from optical flow. The resulting mapping is exploited by the CRF to achieve long-range regularization over the entire video volume.
- 3D重建对于静态场景效果较好，但在动态场景中仍然是一个开放的问题。 功能敏感的CRF模型在语义图像分割中取得了非常成功，但考虑的距离度量并不能适当地模拟时空对应关系。 场景和相机运动的存在使视频中的时间关联成为一项具有挑战性的任务。 由于这种运动的可能性很大，所以在时空体积中的欧几里德距离不是很好的对应关系。 为了解决这个问题，Kundu et al。 （2016）提出了一种用于优化密集CRF的时空正则化特征空间的方法。 具体地，特征空间被优化，使得使用来自光流的对应来最小化与对应点相关联的特征之间的距离。 所得到的映射由CRF利用，以在整个视频卷上实现远程正则化。

- 6.4. Semantic Segmentation of 3D Data 3D数据的语义分割
- Autonomous systems need to recognize their surroundings to identify and interact with objects of interest. While the problem of semantic object labeling has been studied extensively, most of these algorithms work in the 2D image domain where each pixel in the image is labeled with a semantic category such as car, road or pavement. However, 2D images lack important information such as the 3D shape and scale of objects which are strong cues for object class segmentation and facilitate the detection and separation of individual object instances.
- Sengupta et al. (2012) present an approach to generate a semantic overhead map of an urban scene from street level images. They formulate the problem using two CRFs. The first is used for semantic image segmentation of the street view images treating each image independently. Each street view image is then related by a geometrical function that back projects a region from the image into the overhead map. The outputs of this phase are then aggregated over many images to form the input for a second CRF producing a labeling of the ground plane. However, their method does not go beyond the flat world assumption to deliver dense semantic reconstruction using multiple street view images.
- 自治系统需要识别他们的周围环境，以识别和与感兴趣的对象进行交互。 虽然已经广泛研究了语义对象标注的问题，但是大多数这些算法在2D图像域中工作，其中图像中的每个像素被标记有诸如汽车，道路或路面的语义类别。 然而，2D图像缺少诸如对象类别分割的强线索的对象的3D形状和尺度等重要信息，并且便于各个对象实例的检测和分离。
- Sengupta（2012）提出了一种从街道图像生成城市场景的语义开销图的方法。 他们使用两个CRF制定问题。 第一个用于独立处理每个图像的街景图像的语义图像分割。 然后，每个街景图像通过几何函数相关联，后者将区域从图像投影到架空地图中。 然后将该相的输出聚集在许多图像上以形成用于产生接地平面的标记的第二CRF的输入。 然而，他们的方法不超越平面世界的假设，使用多个街景图像提供密集的语义重建。

- Towards this goal, Sengupta et al. (2013) propose an approach illustrated in Figure 15 where a dense semantic 3D reconstruction is generated using multiple street view images. They use visual odometry for ego-motion estimation according to which depth-maps generated from input stereo image pairs are fused. This allows them to generate a volumetric 3D representation of the scene. In parallel, input images are semantically classified using a CRF model. The results of segmentation are then aggregated across the sequence to generate the final 3D semantic model. However, the object labeling is performed in the image domain and then projected onto the model. As a result, these methods fail to fully exploit all structural constraints present in road scenes.
- Valentin et al. (2013) tackle the problem of semantic scene reconstruction in 3D space by combining both structural and appearance cues. They use input depth estimates to generate a triangulated mesh representation of the scene and apply a cascaded classifier to learn geometric cues from the mesh and appearance cues from images. Subsequently, they solve for the labeling in 3D by defining a CRF over the scene mesh. However, they approach requires inference on the whole mesh an does not allow for incrementally adding information in an online setting as common in the autonomous driving context.
- Hackel et al. (2016) propose a fast semantic segmentation approach for 3D point clouds with strongly varying densities. They construct approximate multi-scale neighborhoods by downsampling the entire point cloud, to generate a multi-scale pyramid with decreasing density, and searching for the nearest neighbors per scale. This scheme allows to extract rich feature representation, that captures the geometry in a point’s local neighborhood such as roughness, surface orientation, height over ground and others, in very little time. A random forest classifier finally predicts the class-conditional probabilities. The proposed method can process point clouds with many million of points in a matter of minutes.
- 为了实现这一目标，Sengupta等 （2013）提出了图15所示的方法，其中使用多个街景图像生成密集的语义3D重建。 根据从输入立体图像对生成的深度图被融合，他们使用视觉测距法进行自我运动估计。 这允许他们生成场景的体积3D表示。 并行地，输入图像使用CRF模型进行语义分类。 然后，分割结果跨序列进行聚合，以生成最终的3D语义模型。 然而，对象标注在图像域中执行，然后投影到模型上。 因此，这些方法无法充分利用道路场景中存在的所有结构性限制。
- 瓦伦丁等人 （2013）通过结合结构和外观线索来解决3D空间中语义场景重构的问题。 他们使用输入深度估计来生成场景的三角网格表示，并应用级联分类器从网格和图像中的外观线索学习几何提示。 随后，他们通过在场景网格上定义CRF来解决3D中的标签问题。 然而，他们的方法需要对整个网格进行推理，不允许在自主驾驶环境中常见的在线设置中逐渐添加信息。
- Hackel等人 （2016）提出了一种具有强烈变化密度的3D点云的快速语义分割方法。 它们通过对整个点云进行下采样来构造近似的多尺度邻域，以生成具有减小密度的多尺度金字塔，并且每个尺度搜索最近的邻居。 该方案允许提取丰富的特征表示，其在非常少的时间内捕获点的本地邻域中的几何，例如粗糙度，表面取向，地面高度等。 随机森林分类器最终预测了类条件概率。 所提出的方法可以在几分钟内处理具有数百万点的点云。

- **Online Methods**: Vineet et al. (2015) propose an end-to-end system which processes data incrementally and performs realtime dense stereo reconstruction and semantic segmentation of outdoor environments. They achieve this using voxel hashing (Nießner et al. (2013)), a hash-table-driven 3D volumetric representation that ignores unoccupied space in the target environment. Furthermore, they employ an online volumetric mean-field inference technique that incrementally refines the voxel labeling. They are able to achieve semantic reconstruction at real-time rates by harnessing the processing power of modern GPUs.
- McCormac et al. (2016) propose a pipeline for dense 3D semantic mapping designed to work online by fusing semantic predictions of a CNN with the geometric information from a SLAM system (ElasticFusion by Whelan et al. (2015)). Specifically, ElasticFusion provides correspondences between 2D frames and a globally consistent map of surfels. Furthermore, they use a Bayesian update scheme which computes the class probabilities for each surfel based on the CNN’s predictions. The advantage of using surfel-based surface representations is their ability to fuse long-range information, for instance after a loop closure has been detected and the poses have been corrected accordingly.
- 在线方法：Vineet et al。 （2015）提出了一种端到端系统，可以逐步处理数据，并进行户外环境的实时密集立体重构和语义分割。 他们使用体素散列（Nießneret al。（2013））实现了这一点，这是一种散列表驱动的3D体积表示，忽略了目标环境中未占用的空间。 此外，他们采用在线体积平均场推理技术，逐步优化体素标签。 通过利用现代GPU的处理能力，他们能够以实时速率实现语义重建。
- McCormac等人 （2016）提出了一种用于密集3D语义映射的流水线，旨在通过将CNN的语义预测与SLAM系统的几何信息（Whelan等人（2015）的ElasticFusion）融合在线工作）。 具体来说，ElasticFusion可以提供2D帧与全局一致的冲浪图之间的对应关系。 此外，他们使用贝叶斯更新方案，其基于CNN的预测计算每个冲浪的类概率。 使用基于浮标的表面表示的优点是它们能够融合长距离信息，例如在检测到闭环并且相应地纠正姿态之后。

- **3D CNN**: While convolutional networks have proven very successful segmenting 2D images semantically, there exists relatively little work on labeling 3D data using convolutional networks. Huang & You (2016) propose a framework for labeling 3D point cloud data using a 3D Convolutional Neural Network (3D-CNN). Specifically, they compute 3D occupancy grids of size 203 centered at a set of randomly generated key points. The occupancy and the labels form the input to a 3D CNN, which is composed of convolutional layers, max-pooling layers, a fully connected layer and a logistic regression layer. Due to the dense voxel representation, 3D CNNs are only able to process voxel grids of very coarse resolution considering the memory limitations of modern GPUs.
- To alleviate this problem, Riegler et al. (2017) propose Oct-Nets, a 3D convolutional network, that allows for training deep architectures at significantly higher resolutions. They build on the observation that 3D data (e.g., point clouds, meshes) is often sparse in nature. The proposed OctNet exploits this sparsity property by hierarchically partitioning the 3D space into a set of octrees and applying pooling in a data-adaptive fashion. This leads to a reduction in computational and memory requirements as the convolutional network operations are defined on the structure of these trees and thus can dynamically allocate resources depending on the structure of the input.
- 3D CNN：虽然卷积网络已经证明非常成功地在语义上分割2D图像，但是使用卷积网络标记3D数据的工作相对较少。 Huang＆You（2016）提出了一个使用3D卷积神经网络（3D-CNN）标记3D点云数据的框架。 具体地说，它们以一组随机产生的关键点为中心计算大小为203的3D占用网格。 占有率和标签形成3D CNN的输入，该三维CNN由卷积层，最大池层，完全连接层和逻辑回归层组成。 考虑到现代GPU的内存限制，由于密集的体素表示，3D CNN只能处理非常粗略的分辨率的体素网格。
- 为了减轻这个问题，Riegler等 （2017）提出了一个3D卷积网络Oct-Nets，它允许以更高的分辨率训练深层架构。 它们建立在3D数据（例如，点云，网格）本质上经常稀疏的观察的基础上。 提出的OctNet通过将3D空间分层划分成一组八叉树并以数据自适应方式应用池来利用此稀疏属性。 这导致计算和存储器需求的减少，因为卷积网络操作在这些树的结构上被定义，并且因此可以根据输入的结构而动态地分配资源。

- 6.5. Semantic Segmentation of Street Side Views 街景视图的语义分割
- One important application of semantic segmentation for autonomous vehicles is to segment street-side images (i.e., building facades) into its components (wall, door, window, vegetation, balcony, store, mailbox etc.). Such semantic segmentations are useful for accurate 3D reconstruction, memory-efficient 3D mapping, robust localization as well as path planning.
- Xiao & Quan (2009) propose a multi-view semantic segmentation framework for images captured by a camera mounted on a car driving along the street. Specifically, they define a pairwise MRF across superpixels in multiple views, where the unary terms are based on 2D and 3D features. Furthermore, they minimize color differences for spatial smoothness and use dense correspondences to enforce smoothness across different views. Existing approaches for multi-view semantic segmentation typically require labeling all pixels in all images used for the 3D model which, depending on the semantic segmentation algorithm, can be prohibitively slow. To increase efficiency, Riemenschneider et al. (2014) exploit the inherent redundancy in the labeling of all overlapping images used for the 3D model. They propose an approach that exploits the geometry of a 3D mesh model obtained from multi-view reconstruction to predict the best view for each face of the mesh before performing the actual semantic image labeling. This allows them to accelerate the pipeline by two orders of magnitude.
- 用于自主车辆的语义分割的一个重要应用是将街道侧图像（即，建筑物立面）分割成其部件（墙壁，门，窗，植被，阳台，商店，邮箱等）。 这样的语义分割对于精确的3D重建，高效的内存记录，三维映射，鲁棒的本地化以及路径规划都是有用的。
- Xiao＆Quan（2009）提出了一种用于安装在沿着街道行驶的汽车上拍摄的照相机拍摄的图像的多视图语义分割框架。具体来说，它们在多个视图中定义跨越多个像素的成对MRF，其中一元项基于2D和3D特征。此外，它们使颜色差异最小化，使空间平滑度和密集的对应关系在不同视图之间实现平滑度。用于多视图语义分割的现有方法通常需要标注用于3D模型的所有图像中的所有像素，其取决于语义分割算法可能会非常缓慢。为了提高效率，Riemenschneider等（2014）利用3D模型中所有重叠图像的标签固有冗余。他们提出了一种方法，利用从多视图重建获得的3D网格模型的几何，以便在执行实际的语义图像标记之前预测网格的每个面的最佳视图。这允许他们加速管道两个数量级。

- Gadde et al. (2016b) describe a system for segmentation of 2D images and 3D point clouds of building facades that is fast at inference time and is easily adaptable to new datasets. In contrast to existing methods which exploit the structure of facade images by imposing strong priors, they implement a sequence of boosted decision tree classifiers, that are stacked using auto-context features and learn all correlations from data.
- Xiao et al. (2009) propose another method to generate streetside 3D photo-realistic models from images captured at ground level. In particular, they segment each image into semantically meaningful areas, such as building, sky, ground, vegetation or car. Then, they partition buildings into independent blocks and employ a regularization term by exploiting architectural priors in the orthographic view for inference. This allows them to cope with noisy and missing reconstructed 3D data and produces visually compelling results.
- Gadde等 （2016b）描述了一种在推理时间快的建筑立面的2D图像和3D点云的分割系统，并且易于适应新的数据集。 与通过强加优先权利用立面图像结构的现有方法相比，它们实现了一系列增强的决策树分类器，它们使用自动上下文特征进行堆叠，并从数据中学习所有相关性。
- Xiao等 （2009）提出了另一种方法，从地面捕获的图像生成街头3D逼真模型。 特别是，它们将每个图像分割成语义有意义的领域，例如建筑，天空，地面，植被或汽车。 然后，他们将建筑分成独立的块，并采用正则化术语，通过在正交视图中利用建筑先验来推断。 这使得它们能够应对嘈杂和缺失的重建3D数据，并产生视觉上令人信服的结果。

- Mathias et al. (2016) propose a flexible 3-layered method for segmentation of building facades which avoids the need for explicitly specifying a grammar. First, the facade is segmented into semantic classes which are combined with the output of detectors for architectural elements such as windows and door. Finally, weak architectural priors such as alignment, symmetry, co-occurrence are proposed which encourage the reconstruction to be architecturally consistent. The complete pipeline is illustrated in Figure 16. In contrast to the majority of semantic facade modeling approaches that treat facades as planar surfaces, Martinovi´c et al. (2015) propose an approach for facade modeling which operates directly in 3D. As their approach avoids time-consuming conversions between 2D and 3D representations, they obtain substantially shorter runtime. Specifically, they reconstruct a semi-dense 3D point cloud using SfM and classify each point using a Random Forest classifier trained on 3D features. Afterwards, they separate individual facades based on their semantic structure and impose weak architectural priors.
- Mathias等人（2016）提出了一种灵活的3层分层建筑立面分割方法，避免了明确指定语法的需要。首先，门面被分割成语义类，它们与建筑元素如窗户和门的检测器的输出相结合。最后，提出了弱化的建筑先验，如对齐，对称，同现，鼓励重建在架构上保持一致。完整的流水线如图16所示。与将正面视为平面曲面的大多数语义外观建模方法相反，Martinovi'c et al。 （2015）提出了一种立体建模方法，可直接在3D中进行操作。由于他们的方法避免了2D和3D表示之间耗时的转换，所以它们获得了更短的运行时间。具体来说，他们使用SfM重建半密度3D点云，并使用经过3D特征训练的随机森林分类器对每个点进行分类。之后，他们根据自己的语义结构分开了各个立面，强化了体系结构先验。

- 6.6. Semantic Segmentation of Aerial Images 空中图像的语义分割
- The aim of aerial image parsing is the automated extraction of urban objects from data acquired by airborne sensors. The need for accurate and detailed information for urban objects such as roads is rapidly increasing because of its applications in navigation of autonomous driving systems. For example, aerial image parses can be used to automatically build road maps (even in remote areas) and keep them up-to-date. Furthermore, information from aerial images can be used for localization. However, the problem is challenging because of the heterogeneous appearance of objects like buildings, streets, trees and cars which results in high intra-class variance but low inter-class variance. Furthermore, the complex structure of the prior complicates inference. For instance, roads must form a connected network of thin segments with slowly changing curvatures which meet at junctions. This type of prior knowledge is more challenging to formalize and integrate into a structured prediction formulation than standard smoothness assumptions.
- 空中图像解析的目的是通过机载传感器获取的数据自动提取城市对象。由于其在自主驾驶系统的导航中的应用，对诸如道路等城市对象的准确详细信息的需求正在迅速增长。例如，航空图像解析可以用于自动构建路线图（甚至在偏远地区），并保持最新。此外，来自航空图像的信息可以用于定位。然而，这个问题是具有挑战性的，因为像建筑物，街道，树木和汽车这样的物体的外观异乎寻常，导致班内差异很大，班级间差异较小。此外，复杂的结构先前复杂化推理。例如，道路必须形成具有缓慢变化的曲率的薄段的连接网络，其在交叉点处相遇。这种类型的先验知识比标准平滑度假设更具挑战性，将其形式化并结合到结构化预测公式中。

- Wegner et al. (2013) propose a CRF formulation for road labeling in which the prior is represented by cliques that connect sets of superpixels along straight line segments. Specifically, they formulate the constraints as high-order cliques with asymmetric $P^N$-potentials which express a preference to assign all rather than just some of their constituent superpixels to the road class. This allows the road likelihood to be amplified for thin chains while still being amenable to efficient inference using graph cuts. Wegner et al. (2015) also model the road network using a CRF with long-range, higher-order cliques. However, unlike Wegner et al. (2013), they allow for arbitrarily shaped segments which adapt to more complex road shapes by searching for putative roads with minimum cost paths based on local features. Montoya et al. (2015) extend this formulation to multi-label classification of aerial images with class-specific priors for buildings and roads. In addition to the road network prior of Wegner et al. (2015), they introduce a second higher order potential for cliques specific to buildings.
- Wegner et al。 （2013）提出了一种用于道路标记的CRF公式，其中先前由通过沿直线段连接超像素组的分支来表示。具体来说，他们将约束作为具有不对称的PN-totentials的高阶组合，它们表示偏好将所有而不仅仅是他们的组成超像素的一些分配给道路类。这允许对于细链可以扩大道路的可能性，同时仍然可以使用图形切割进行有效的推断。 Wegner et al。 （2015年）还使用具有远程，高阶组合的CRF对道路网进行建模。然而，与Wegner等人不同（2013），它们允许通过基于局部特征搜索具有最小成本路径的推定道路来适应更复杂的道路形状的任意形状的段。 Montoya等人（2015年）将此方案扩展到具有类别特色的建筑物和道路先进的航空图像多标签分类。除了Wegner等人之前的道路网络之外（2015年），它们为特定建筑物的集团引入了第二高的潜力。

- In contrast to other methods, Verdie & Lafarge (2014) propose the application of Markov point processes for recovering specific structures from images, including road networks. Markov point processes are a generalization of traditional MRFs which can address object recognition problems by directly manipulating parametric entities such as line segments, whereas MRFs are restricted to labeling problems. Importantly, they implicitly solve the model-selection problem, i.e., they allow for an arbitrary number of variables in the MRF which can be associated with the parameters of the objects of interest. Specifically for road segmentation, the parametric representation of road segments is chosen as a point at the center of mass of the segment and two additional parameters modeling the length and orientation of the road segment.
- 与其他方法相反，Verdie＆Lafarge（2014）提出了马尔科夫点处理从图像恢复特定结构的应用，包括道路网络。 马可夫点过程是传统MRF的概括，可以通过直接操纵诸如线段的参数实体来解决对象识别问题，而MRF仅限于标签问题。 重要的是，它们隐含地解决了模型选择问题，即它们允许可以与感兴趣对象的参数相关联的MRF中的任意数量的变量。 特别针对道路分割，道路段的参数化表示被选择为段的质心点，以及两个额外的参数来建模道路段的长度和方位。

- **Aerial Image Parsing using Maps**: Instead of framing the problem of detecting topologically correct road network as a semantic segmentation problem, Mattyus et al. (2015) exploit map information from OpenStreetMap (OSM)27. OSM is a collection of roads, trails, caf´es, railway stations and much more all over the world contributed and maintained by a community of mappers. It provides freely available maps of the road topology in the form of piece-wise linear road segments. Given a road map from OSM, Mattyus et al. (2015) propose an MRF which reasons about the location of the road centerline and its width for each road segment in OSM. In addition, they incorporate smoothness between consecutive line segments by encouraging their widths to be similar. This formulation has the advantage that it enables efficient inference while restricting the road topology to the OSM map.
- 使用地图的空中图像分析：Mattyus等人，不是将拓扑正确的道路网络检测为语义分割问题的问题。 （2015）利用OpenStreetMap（OSM）27的地图信息。 OSM是一个道路，小径，咖啡馆，火车站的集合，还有世界各地由映射者社区贡献和维护的。 它以分段线性路段的形式提供道路拓扑的免费地图。 给出了OSM的路线图，Mattyus等人 （2015）提出一个MRF，说明道路中心线的位置及其在OSM中每个路段的宽度的原因。 此外，它们通过鼓励它们的宽度相似而在连续线段之间融合平滑度。 该公式的优点是能够有效地推理，同时将道路拓扑限制到OSM地图。

- **Fine-grained Image Parsing with Aerial-to-ground Reasoning**: While aerial images provide full coverage of a significant portion of the world, they are of much lower resolution than ground images. In aerial imagery the resolution relates to the ground area covered by one pixel. Whereas 1 meter resolution is already a high resolution for satellite imagery, the standard resolution for most image databases (e.g. Google Earth) is 12 inch. Resolutions of 6 to 1 inch are considered high resolutions for aerial imagery and are usually not publicly available. This makes fine grained segmentation from aerial images a challenging problem. On the other hand, ground images provide additional information which enables fine-grained semantic segmentation. Motivated by the complementary nature of these cues, several methods for fine grained segmentation have been recently proposed which jointly reason about co-located aerial and ground image pairs.
- 具有空中对地推理的细粒度图像解析：虽然空中图像提供了全世界相当大部分的全面覆盖，但它们的分辨率远低于地面图像。 在空中影像中，分辨率与一个像素覆盖的地面相关。 虽然1米分辨率已经是卫星图像的高分辨率，但大多数图像数据库（例如Google Earth）的标准分辨率为12英寸。 6至1英寸的分辨率被认为是高分辨率的航空影像，通常不是公开的。 这使得空中图像的细粒度分割成为一个具有挑战性的问题。 另一方面，地面图像提供了能够进行细粒度语义分割的附加信息。 由于这些线索的互补性质，最近提出了几种细粒度分割方法，这些方法共同地说明了共同定位的天线和地面图像对。

- Mattyus et al. (2016) extend the approach of Mattyus et al. (2015) by introducing a formulation that reasons about fine-grained road semantics such as lanes and sidewalks. To infer this information, they jointly consider monocular aerial images and high-resolution stereo images captured from ground vehicles. Specifically, they formulate the problem as energy minimization in an MRF, inferring the number and location of the lanes for each road segment, all parking spots and sidewalks along with the alignment between the ground and aerial images. Towards this goal, they exploit deep learning to estimate semantics from aerial and ground images and define potentials exploiting both cues. In addition, they define potentials which model road constraints like relationships between parallel roads and the smoothness along roads.
- In a related work, Wegner et al. (2016) build a map of trees for urban planning applications from aerial images, street view images and semantic map data. They train CNN based object detection algorithms on human-annotated data. Furthermore, they combine the CNN predictions from multiple street view images and aerial images with map data in a CRF formulation to achieve a geolocated fine-grained catalog.
- Mattyus等人 （2016）扩展了Mattyus等人的方法。 （2015）通过介绍道路和人行道等细粒度道路语义的原因。 为了推断这些信息，他们共同考虑从地面车辆捕获的单目空间图像和高分辨率立体图像。 具体来说，他们将MRF中的能量最小化问题制定出来，推断每个路段的车道数量和位置，所有停车点和人行道以及地面和航空图像之间的对齐。 为实现这一目标，他们利用深度学习来估计空中和地面图像的语义，并定义利用两种线索的潜力。 此外，他们界定了模拟道路约束的潜力，如平行道路之间的关系以及道路的平滑度。
- 在相关的工作中，Wegner et al。 （2016）从航空图像，街景图像和语义地图数据构建城市规划应用的树木地图。 他们在人工数据上训练基于CNN的对象检测算法。 此外，它们将来自多个街景图像和航空图像的CNN预测与CRF公式中的地图数据相结合，以实现地理位置的细粒度目录。

- 6.6.1. ISPRS Segmentation Challenge ISPRS分段挑战
- The focus of the ISPRS segmentation challenge (Rottensteiner et al. (2013, 2014)) is detailed 2D semantic segmentation of data acquired by airborne sensors as shown in Figure17. More specifically, the task is to assign labels to multiple urban object categories. The challenge comprises two airborne image datasets, Vaihingen and Potsdam, which have been manually annotated by the six most common land cover classes, namely impervious surfaces, building, vegetation, tree, car, clutter/background. Both areas cover urban scenes. The leaderboards of the datasets Potsdam and Vaihingen are provided in the Table 5. The performance of the approaches is assessed with the F1 scores for the six classes and overall
- ISPRS分段挑战的重点（Rottensteiner等（2013，2014））详细描述了机载传感器获取的数据的2D语义分割，如图17所示。 更具体地说，任务是将标签分配给多个城市对象类别。 挑战包括两个机载图像数据集，Vaihingen和波茨坦，这些数据已经被六个最常见的土地覆盖类别手工注释，即不透水表面，建筑物，植被，树木，汽车，杂波/背景。 这两个区域都是城市场景。 数据集Potsdam和Vaihingen的排行榜在表5中提供。该方法的表现用六个等级和总体的F1分数进行评估。

- Paisitkriangkrai et al. (2015) is one of the best-performing methods in the ISPRS segmentation challenge. They propose a semantic pixel labeling method which combines CNN features with hand-crafted features in a pixel-wise CRF formulation to infer a globally consistent labeling that is locally smooth except at edges. Sherrah (2016) propose to use fully-convolutional networks without any downsampling layers to preserve the resolution of the output. In order to make use of elevation data, they propose a hybrid network that combines the pre-trained image features with features based on available digital surface models (DSM) which capture the Earth’s surface. Sherrah (2016) achieve the best performance on the ISPRS Potsdam (Table 5a) and competitive results on Vaihingen in Table 5b.
- Paisitkriangkrai等 （2015）是ISPRS分段挑战中最好的方法之一。 他们提出了一种语义像素标记方法，其将CNN特征与以像素为单位的CRF公式中的手工特征相结合，以推断除了边缘之外局部平滑的全局一致的标记。 Sherrah（2016）建议使用完全卷积网络，而不需要任何下采样层来保持输出的分辨率。 为了利用高程数据，他们提出了一种混合网络，其将预先训练的图像特征与基于捕获地球表面的可用数字表面模型（DSM）的特征相结合。 Sherrah（2016）在表5b中的“Vaihingen”上获得了最佳表现（表5a）和竞争结果。

- Maggiori et al. (2016) introduce a model which extracts spatial features at multiple resolutions and learns how to combine them in order to integrate local and global information. Audebert et al. (2016) further improved the state-of-the-art for dense scene labeling of aerial images by exploiting the encoder-decoder architecture of SegNet (Badrinarayanan et al. (2015)). In addition, they introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales and perform data fusion from heterogeneous sensors using a residual correction network. Marmanis et al. (2016a) demonstrate the best performance on the ISPRS Vaihingen challenge in Table 5b. They use their previous work Marmanis et al. (2016b) which uses an ensemble of fully convolutional networks to obtain pixel-wise classification at full resolution of aerial images. Marmanis et al. (2016a) propose to compensate the loss of spatial resolution due to the pooling layers by combining semantic segmentation with edge detection.
- Maggiori等人（2016）介绍了一种以多种分辨率提取空间特征的模型，并学习如何组合它们，以便整合本地和全球信息。 Audebert等（2016年）通过利用SegNet编码器 - 解码器架构（Badrinarayanan等（2015））进一步改进了航空图像密集场景标记的最新技术。此外，它们引入了一个多内核卷积层，用于在多个尺度上快速聚合预测，并使用残差校正网络执行异构传感器的数据融合。 Marmanis等人（2016a）在表5b中证明了ISPRS Vaihingen挑战的最佳性能。他们使用他们以前的工作Marmanis et al。 （2016b），其使用完整卷积网络的集合来获得全分辨率的空间图像的像素分类。 Marmanis等人（2016a）提出通过将语义分割与边缘检测相结合来补偿由于汇集层造成的空间分辨率的损失。

- 6.7. Road Segmentation 道路分割
- Segmentation of road scenes is a crucial problem in computer vision for applications such as autonomous driving and pedestrian detection. For instance, in order to navigate, an autonomous vehicle needs to determine the drivable free space ahead and determine its own position on the road with respect to the lane markings. However, the problem is challenging due to the presence of a variety of differently shaped objects such as cars and people, different road types and varying illumination and weather conditions.
- Munoz et al. (2010) propose an alternative to standard inference in graphical models for semantic labeling of scenes. In particular, they train a sequence of inference models in a hierarchical procedure that captures the context over large regions. This allows them to bypass the difficulties of training structured prediction models when exact inference is intractable and leads to a very efficient and accurate scene labeling algorithm.
- Kuehnl et al. (2012) propose a method that aims to improve appearance-based classification by incorporating the spatial layout of the scene. Specifically, they propose a two-stage approach for road segmentation. First, they represent the road surface and delimiting elements such as curbstones and lane-markings using confidence maps based on local visual features. From these confidence maps, they extract SPatial RAY (SPRAY) features that incorporate global properties of the scene and train a classifier on those features. Their evaluation shows that spatial layout helps especially for the cases where there is a clear structural correspondence between properties at different spatial locations.
- 道路场景的分割是自动驾驶和行人检测等应用的计算机视觉中的关键问题。 例如，为了导航，自主车辆需要确定前方的可驾驶自由空间，并且相对于车道标记确定其在道路上的位置。 然而，由于存在各种不同形状的物体，例如汽车和人，不同的道路类型和不同的照明和天气条件，问题是具有挑战性的。
- Munoz等人 （2010）提出了场景语义标注图形模型中标准推理的替代方法。 特别地，他们在分层程序中训练一系列推理模型，以捕获大区域上下文。 这样就可以避免训练结构化预测模型的困难，当精确推理是棘手的，并导致非常有效和准确的场景标记算法
- Kuehnl et al。 （2012）提出了一种旨在通过结合场景的空间布局来改进基于外观的分类的方法。 具体来说，他们提出了一个两阶段的道路分割方法。 首先，它们使用基于局部视觉特征的置信图来代表路面和划线元素，例如路缘石和车道标记。 从这些置信图中，他们提取了包含场景全局属性的空间RAY（SPRAY）功能，并对这些特征进行了分类。 他们的评估表明，空间布局特别适用于在不同空间位置的属性之间存在明确的结构对应关系的情况

- Alvarez et al. (2010) propose a Bayesian framework to classify road sequences by combining low-level appearance cues with contextual 3D road cues such as horizon lines, vanishing points, 3D scene layout and 3D road stages. In addition, they extract temporal cues for temporal smoothing of the results. In a follow-up work, A´ lvarez & Lo´pez (2011) convert the image into an illuminant invariant feature space to make their method robust to shadows and then apply a classifier to assign a semantic label to each pixel. Mansinghka et al. (2013) propose an inverse-graphics inspired method employing generative probabilistic graphics programs (GPGP) to infer roads in images taken from vehicle-mounted cameras. GPGPs consist of a stochastic scene generator for generating random samples from a road scene prior, a graphics renderer for rendering the image segmentation for each sample and a stochastic likelihood model linking the renderer’s output and the data.
- CNN-based Methods: Almost all existing algorithms for labeling road scenes are based on machine learning where the parameters of the model are estimated from large annotated datasets. To alleviate the burden of annotating large datasets manually, A´ lvarez et al. (2012) propose a method for road segmentation where noisy training labels for road images are generated using a convolutional neural network trained on a general image database. They further propose a texture descriptor which is based on learning a linear combination of color planes to reduce variability in road texture.
- Alvarez et al（2010）提出了一个贝叶斯框架，通过结合低级别的外观线索与上下文3D路线线索（如地平线，消失点，3D场景布局和3D路段）来分类道路序列。 此外，它们提取时间线索以便对结果进行时间平滑。 在后续工作中，A'lvarez＆Lo'pez（2011）将图像转换为光源不变特征空间，使其方法对阴影有效，然后应用分类器为每个像素分配语义标签。 曼辛哈卡等人 （2013）提出了一种利用生成概率图形程序（GPGP）来推导从车载摄像机拍摄的图像中的道路的逆图形启发方法。 GPGP由随机场景发生器组成，用于从道路场景生成随机样本，用于渲染每个样本的图像分割的图形渲染器以及链接渲染器的输出和数据的随机似然模型。
- 基于CNN的方法：几乎所有现有的用于标记道路场景的算法都是基于机器学习，其中模型的参数是从大的注释数据集估计的。 为了减轻手动注释大数据集的负担，A'lvarez et al。 （2012）提出了一种用于道路分割的方法，其中使用在一般图像数据库上训练的卷积神经网络来生成道路图像的噪声训练标签。 他们进一步提出了一种纹理描述符，其基于学习颜色的线性组合以减少道路纹理的变异性。

- Mohan (2014) propose a scene parsing system using deconvolutional layers in combination with traditional CNNs. Deconvolutional layers learn features that capture mid-level cues such as edge intersections, parallelism and symmetry in image data and thus obtain a more robust representation than regular CNNs. Oliveira et al. (2016) investigate the trade-off between segmentation quality and runtime using U-Nets by Ronneberger et al. (2015). Specifically, they introduce a new mapping between classes and filters at the up-convolutional part of the network to reduce the runtime. They further segment the whole image with a single forward pass, which makes the approach more efficient than patch-based approaches.
- To mitigate the difficulties in acquiring human annotations, Laddha et al. (2016) propose a map-supervised deep learning pipeline which does not require human annotations for training a road segmentation algorithm. Instead, they obtain ground truth labels based on OpenStreetMap information projected into the image domain using the vehicle pose given by the GPS sensor.
- Mohan（2014）提出了一种使用去卷积层与传统CNN相结合的场景解析系统。 解卷积层学习捕获中级线索的特征，例如图像数据中的边缘交点，并行度和对称性，从而获得比常规CNN更强的表示。 Oliveira等人 （2016）研究了Ronneberger等人使用U-Nets进行分割质量和运行时间之间的权衡。（2015年）。 具体来说，它们在网络的上卷积部分引入类和过滤器之间的新映射，以减少运行时间。 他们进一步将整个图像分割成一个单一的向前传递，这使得该方法比基于补丁的方法更有效率。
- 为了缓解人类注解的缺陷，Laddha等 （2016）提出了一种地图监督的深层学习管道，不需要人为注释来训练道路分割算法。 相反，他们使用GPS传感器给出的车辆姿态，基于投影到图像域中的OpenStreetMap信息获得地面真实标签。

- 6.7.1. Free Space Estimation 自由空间估计
- Accurate and reliable estimation of free space and detection of obstacles are core problems that need to be solved to enable autonomous driving. Free space is defined by the available space on the ground surface where navigation of vehicle is guaranteed without collision. Obstacles refer to structures that block the path of the vehicle by sticking out of the ground surface. In contrast to road segmentation approaches, methods for estimating the free-space in front of a vehicle often rely on geometric features as derived from a depth map computed from stereo sensors. However, both approaches can be advantageously combined.
- Badino et al. (2007) propose a method for free space estimation by computing stochastic occupancy grids based on stereo information, where cells in a stochastic occupancy grid carry information about the likelihood of occupancy. Stereo information is integrated over time in order to reduce depth uncertainty. The boundary between free space and occupied space is robustly obtained using dynamic programming on the occupancy grid. This work laid the foundations for the Stixel representation, see Section 4 for an in-depth discussion. While the original method of Badino et al. (2007) makes the assumption of a planar road surface, this assumption is often violated in practice. To tackle more complicated road surfaces, Wedel et al. (2009) propose an algorithm which models non-planar road surfaces using B-splines. The surface parameters are estimated from stereo measurements and tracked over time using a Kalman filter.
- 准确可靠的自由空间估计和障碍物的检测是需要解决的核心问题，以实现自主驾驶。 自由空间由地面上的可用空间定义，车辆的导航保证没有碰撞。 障碍是指通过从地面伸出而阻挡车辆路径的结构。 与道路分割方法相反，用于估计车辆前方的自由空间的方法通常依赖于从立体声传感器计算的深度图导出的几何特征。 然而，可以有利地组合这两种方法。
- Badino等人 （2007）提出了一种基于立体声信息计算随机占用网格的自由空间估计方法，其中随机占用网格中的单元携带有关占用可能性的信息。 随着时间的推移，立体声信息被整合，以减少深度不确定性。 使用占用网格上的动态规划可以获得自由空间和占用空间之间的边界。 这项工作为Stixel代表奠定了基础，参见第4节进行了深入的讨论。 虽然Badino等人的原始方法 （2007）假设一个平面的路面，这个假设在实践中经常被违反。 为了解决更复杂的路面，Wedel等 （2009）提出了一种使用B样条模拟非平面路面的算法。 表面参数从立体测量估计，并使用卡尔曼滤波器随时间跟踪。

- Suleymanov et al. (2016) propose an online system to detect and drive on collision-free traversable paths, based on stereo estimation using a variational approach. In addition to free space detection, their approach also establishes a semantic segmentation of the scene, where labels include ground, sky, obstacles and vegetation. Fisheye cameras provide a wider field of view compared to regular cameras and allow for detection of obstacles closer to the car. H¨ane et al. (2015) propose a method for obstacle detection using monocular fisheye cameras. In order to reduce runtime, they avoid using visual odometry systems to provide accurate vehicle poses and instead rely on less accurate pose estimates from the wheel odometry.
- Long Range Obstacle Detection: The accuracy of obstacle detection methods at long range is a crucial factor for timely obstacle localization when the observer (i.e., the ego-vehicle) moves at high speed. Unfortunately, the error of stereo vision system increases quadratically with depth in contrast to laser range sensors or radar which do not suffer from this problem. To tackle this problem, Pinggera et al. (2015, 2016) propose long-range obstacle detection algorithms using stereo vision by exploiting geometric constraints on camera motion and planarity to formulate obstacle detection as a statistical hypothesis testing problem. Specifically, independent hypothesis tests are performed on small local patches distributed across the input images where free-space and obstacles are represented by the null and alternative hypothesis respectively. The detection results for an exemplary scene from their novel dataset is illustrated in Figure 18.
- Suleymanov等人 （2016）提出了一种基于使用变分方法的立体声估计的在无系统的无碰撞路径上进行检测和驱动的在线系统。 除了自由空间检测，他们的方法还建立了场景的语义分割，其中标签包括地面，天空，障碍物和植被。 与普通相机相比，鱼眼相机提供了更广阔的视野，并且能够检测靠近汽车的障碍物。 H¨ane等 （2015）提出了一种使用单眼鱼眼相机进行障碍物检测的方法。 为了减少运行时间，他们避免使用视觉测距系统来提供精确的车辆姿势，而是依靠轮距离测量的精确姿态估计
- 长距障碍检测：当观察者（即，自我 - 车辆）高速移动时，长距离障碍物检测方法的准确性是及时障碍物定位的关键因素。 不幸的是，立体视觉系统的误差随着深度的增加而增加，与激光距离传感器或雷达不同，该雷达不会受到此问题的影响。 为了解决这个问题，Pinggera等 （2015年，2016年）提出了使用立体视觉的远程障碍物检测算法，通过利用相机运动和平面度的几何约束来制定障碍物检测作为统计假设检验问题。 具体来说，对分布在输入图像上的小局部补丁执行独立假设检验，其中空间和障碍分别由空和替代假设表示。 图18中示出了来自其新颖数据集的示例场景的检测结果。

### 7. Reconstruction 再构建
- 7.1. Stereo 立体
- Stereo estimation is the process of extracting 3D information from 2D images captured by stereo cameras, without need for special range measurement devices. In particular, stereo algorithms estimate depth information by finding correspondences in two images taken at the same point in time, typically by two cameras mounted next to each other on a fixed rig. These correspondences are projections of the same physical surface in the 3D world. Depth information is crucial for applications in autonomous driving or driver assistance systems. Accurate estimation of dense depth maps is a necessary step for 3D reconstruction, and many other problems such as obstacle detection, free space analysis, and tracking benefit from the availability of depth estimates.
- 立体声估计是从立体相机拍摄的2D图像中提取3D信息的过程，而不需要特殊的距离测量装置。 特别地，立体声算法通过在同一时间点上拍摄的两个图像中的对应关系来估计深度信息，通常通过在固定的钻机上彼此相邻地安装两个相机。 这些对应是3D世界中相同物理表面的投影。 深度信息对于自主驾驶或驾驶员辅助系统中的应用至关重要。 精确估计密集深度图是3D重建的必要步骤，诸如障碍物检测，自由空间分析和跟踪等许多其他问题可从深度估计的可用性中获益。

- **Taxonomies**: Multiple taxonomies for stereo matching have been proposed in the literature. Guided by the computational restrictions, the earliest one is based on the density of the output (Franke & Joos (2000)). Feature-based methods provide only sparse depth maps based on edges while area-based methods, such as block matching, generate dense outputs at the expense of computation time. A more recent and commonly referred taxonomy of stereo algorithms is based on the optimization as local and global. Local methods compute the disparity by simply selecting the lowest matching cost which is known as the winner takes all (WTA) solution. Global methods formulate disparity computation as an energy-minimization framework based on the smoothness assumption between neighboring pixels or regions. There are various ways of finding the minimum of a global energy function, including variational approaches in continuous domain and discrete approaches using dynamic programming, Graph Cuts, and Belief Propagation.
- 分类：文献中已经提出了立体匹配的多重分类法。在计算限制的基础上，最早的是基于输出密度（Franke＆Joos（2000））。基于特征的方法仅提供基于边缘的稀疏深度图，而基于区域的方法（例如块匹配）以牺牲计算时间为代价产生密集输出。立体声算法的更新和普遍引用的分类法是基于本地和全局的优化。本地方法通过简单地选择被称为获胜者获得所有（WTA）解的最低匹配成本来计算差异。全局方法基于相邻像素或区域之间的平滑度假设，将视差计算作为能量最小化框架。有各种各样的方法可以找到全局能量函数的最小值，包括使用动态规划，图表切换和信念传播的连续域和离散方法的变分方法。

- Matching Cost Function: Stereo matching is a correspondence problem where the goal is to identify the matching points between left and right image based on a cost function. The algorithms usually assume images are rectified, and the search space is reduced to a horizontal line where the correspondence between a left and right point is encoded by the distance on this line, which is defined as disparity. The matching cost computation is the process of computing a cost function at each pixel for all possible disparities which takes its minimal value at the true disparity. However, it is hard to design such a cost function in practice, therefore stereo algorithms make the assumption of constant appearance between matching points. This assumption is often violated in real-world situations, such as cameras with slightly dierent settings causing exposure changes, vignetting, image noise, non-Lambertian surfaces, illumination changes, etc. Hirschm¨uller & Scharstein (2007) call these changes radiometric differences and systematically investigate their effect on commonly used matching cost functions, namely absolute differences, filter-based costs (LoG, Rank and Mean), hierarchical mutual information (HMI), and normalized crosscorrelation. They found that the performance of a cost function depends on the stereo method that uses it. On images with simulated and real radiometric differences, rank filter performed best for correlation-based methods. For global methods, in tests with global radiometric changes or noise, HMI performed best, while in the presence of local radiometric variations, Rank and LoG filters performed better than HMI. Qualitative results show that filter-based cost cause blurred object boundaries when used with global methods. None of the matching costs evaluated could succeed at handling strong lighting changes.
- 匹配成本函数：立体匹配是一个对应问题，其目的是基于成本函数来识别左右图像之间的匹配点。算法通常假设图像被整流，并且搜索空间减少到水平线，其中左和右点之间的对应被该行上的距离编码，其被定义为视差。匹配成本计算是对于所有可能的差异，在每个像素处计算成本函数的过程，其在真实差异下采用其最小值。然而，在实践中很难设计这样的成本函数，因此立体声算法使匹配点之间的外观不变。这种假设在现实世界的情况下经常遭到侵犯，例如具有轻微不同设置的摄影机，会引起曝光变化，渐晕，图像噪声，非朗伯表面，照明变化等。Hirschmüller＆Scharstein（2007）称这些变化辐射度差异，并系统地研究其对常用匹配成本函数的影响，即绝对差异，基于过滤器的成本（LoG，Rank和Mean），分层互信息（HMI）和归一化互相关。他们发现成本函数的性能取决于使用它的立体声方法。在具有模拟和实际辐射度差异的图像上，等级滤波器对于基于相关的方法表现最佳。对于全局方法，在具有全局辐射度变化或噪声的测试中，HMI性能最好，而在存在局部辐射度变化的情况下，Rank和LoG滤波器的性能优于HMI。定性结果表明，当与全局方法一起使用时，基于过滤器的成本导致对象边界模糊。评估的匹配成本中没有一个可以成功处理强烈的照明变化。

- **SGM**: Semi-Global Matching (SGM) (Hirschm¨uller (2008)) has become very influential due to its speed and high accuracy as evidenced in various benchmarks such as Middlebury (Scharstein & Szeliski (2002)) or KITTI (Geiger et al. (2012b)). SGM is also recently used on top of CNN features, since simply outputting the most likely configuration for every pixel is not competitive with modern stereo algorithms (Zbontar & LeCun (2016); Luo et al. (2016)). The energy function has two levels of penalization for small and large disparity differences with a weighting based on the local intensity gradient for the latter one. The energy is calculated by summing costs along 1D paths from multiple directions towards each pixel using dynamic programming and the result is determined by WTA. There are a couple of follow-up works investigating the practical and theoretical sides of SGM. Gehrig et al. (2009) propose a real-time, low-power implementation of the SGM with algorithmic extensions for automotive applications on a reconfigurable hardware platform. Drory et al. (2014) offer a principled explanation for the success of SGM by clarifying its relation to belief propagation and tree-reweighted message passing with an uncertainty measure as an outcome.
- The performance of SGMs can be further improved by incorporating confidences of the stereo estimation. Seki & Pollefeys (2016) leverage CNNs to predict the confidences for stereo estimations. Taking into account ideas from conventional confidences features, that neighboring pixel which are consistent are more likely to be correct and the disparity estimated from the other image should correspond, they design a two-channel disparity patch which is used as input for the CNN. In order to acquire dense disparity, the confidences are incorporated into SGM by weighting each pixel according to the estimated confidence.
- SGM：半全球配对（Hirschmuller（2008））由于其速度和准确性而变得非常有影响力，如米德伯勒（Scharstein＆Szeliski（2002））或KITTI（Geiger （2012b））。 SGM也最近被用于CNN功能之上，因为简单地输出每个像素的最可能的配置与现代立体声算法（Zbontar＆LeCun（2016）; Luo等（2016））不具有竞争力。能量函数对于小和大的差异差异具有两个级别的惩罚，基于对后者的局部强度梯度的加权。通过使用动态规划将来自多个方向的1D路径的成本与每个像素相加来计算能量，并且结果由WTA确定。有几项后续工作调查了上海证券交易所的实际和理论方面。 Gehrig等人（2009）提出了在可重配置硬件平台上实现汽车应用的算法扩展的SGM的实时低功耗实现。 Drory等（2014年）通过澄清其与信仰传播的关系和树重加权的消息传递以不确定性度量作为结果，为SGM的成功提供了原则性的解释。
- 通过引入立体声估计的信心，可以进一步提高SGM的性能。 Seki＆Pollefeys（2016）利用CNN来预测立体声估计的信心。考虑到常规信号特征的想法，一致的相邻像素更可能是正确的，并且从另一个图像估计的视差应该对应，它们设计一个双通道视差补丁，用作CNN的输入。为了获得密集的差异，通过根据估计的置信度对每个像素进行加权，将信息并入SGM。

- **Variable Baseline/Resolution**: Stereo estimates can be fused to yield a more complete reconstruction of the static parts of the three-dimensional scene. However, assuming fixed baseline, focal length, field of view might not always be the best strategy. Gallup et al. (2008) point out two problems with traditional stereo methods: dropping accuracy in the far range and unnecessary computation time spent in the near range. Given that choice of views for stereo is quite flexible in many applications such as structure from motion, Gallup et al. (2008) propose to dynamically select the best cameras with the appropriate baseline for accurate estimation in the far range from a set of possible cameras recording images at the same time. Further, they reduce the resolution to speed up the computation in the near range. In contrast to traditional fixed-baseline stereo, the proposed variable baseline/resolution stereo algorithm achieves constant accuracy over the reconstructed volume by evenly spreading the computation throughout the volume.
- **Planarity**: The inherent ambiguity in appearance based matching costs can be overcome by regularization, i.e., by introducing prior knowledge about the expected disparity map into the stereo estimation process. The simplest prior favors neighboring pixels to take on the same disparity value. However, such generic smoothness priors fail to reconstruct poorly-textured and slanted surfaces, as they favor fronto-parallel planes. A more generic approach to handle arbitrary smoothness priors is using higher-order connections beyond pairwise. Higherorder priors are able to express more realistic assumptions about depth images, but usually at additional computational cost. One very common way to deal with slanted surfaces in the literature is to assume piecewise planarity. Geiger et al. (2010) build a prior over the disparity space by forming a triangulation on a set of robustly matched correspondences, called support points. This reduces matching ambiguities and results in an efficient algorithm by restricting the search to plausible regions. Gallup et al. (2010) first train a classifier to segment an image into piecewise planar and non-planar regions and then enforce a piecewise planarity prior only for planar regions. Non-planar regions are modeled by the output of a standard multi-view stereo algorithm.
- 可变基准/分辨率：可以融合立体声估计，以产生三维场景的静态部分的更完整的重建。然而，假设固定基线，焦距，视野可能不总是最好的策略。盖洛普等人（2008）指出了传统立体声方法的两个问题：在远程范围内降低精度，在近距离内消耗不必要的计算时间。鉴于立体声的选择在许多应用中是非常灵活的，例如来自运动的结构，Gallup等人（2008）提出动态选择具有适当基准的最佳相机，以便在远距离范围内进行精确估计，从一组可能的摄像机同时记录图像。此外，它们降低了分辨率，以加速近似范围内的计算。与传统的固定基线立体声相比，所提出的可变基线/分辨率立体声算法通过在整个音量上均匀地展开计算，在重建体积上实现了恒定的精度。
- 平面：基于外观的匹配成本的固有歧义可以通过正则化来克服，即通过将关于预期视差图的先前知识引入到立体声估计过程中。最简单的先前有利于相邻像素具有相同的差异值。然而，这种通用平滑度先验不能重构不良纹理和倾斜的表面，因为它们有利于前平行平面。处理任意平滑先验的更通用的方法是使用成对以外的高阶连接。高阶先验能够对深度图像表达更真实的假设，但通常以额外的计算成本。在文献中处理倾斜表面的一个很常见的方法是假设分段平面。盖革等人（2010）通过在称为支持点的一组鲁棒匹配的对应上形成三角测量来构建差异空间的前一个。这减少了匹配模糊度，并通过将搜索限制到合理的区域来产生有效的算法。盖洛普等人（2010）首先训练分类器将图像分割成分段平面和非平面区域，然后仅在平面区域之前实施分段平面度。非平面区域由标准多视点立体声算法的输出建模。

- **Variational Approaches**: Similarly, in variational approaches, commonly used smoothness prior, Total Variation (TV) does not produce convincing results in the presence of weak and ambiguous observations, since it encourages piecewise constant regions leading to stair-casing artifacts. Haene et al. (2012) introduce patch-based priors into a TV framework in the form of small, piecewise planar dictionaries. Total Generalized Variation (TGV) (Bredies et al. (2010)) is argued to be a better prior than TV, since it does not penalize piecewise affine solutions. However, it is restricted to convex data terms in contrast to TV, where global solutions can be computed even in the presence of non-convex data terms. Coarse-to-fine approaches as an approximation to non-convex problem of stereo matching often end up with loss of details. To preserve fine details, Kuschk & Cremers (2013) integrate an adaptive regularization weight into the TGV framework by using edge detection and report improved results compared to a coarse-to-fine approach. Ranftl et al. (2013) obtain even better results by proposing a decomposition of the non-convex functional into two subproblems which can be solved globally where one is convex, and the other can be made convex by lifting the functional to a higher dimensional space.
- **State-of-the-art**: In Table 6 we show the ranking of stereo methods on the KITTI stereo 2015 benchmark. The KITTI benchmark reports the percentage of erroneous (bad) pixels over background regions (D1-bg), foreground regions (D1-fg) and over all regions (D1-all). The best performing method G¨uney & Geiger (2015) use object knowledge to compensate for the weak data term on the reflecting and textureless surfaces. Seki &Pollefeys (2016) achieve the best performance on background regions with the prediction of stereo correspondence confidences and integration into SGM. Recently, deep learning approaches (Zbontar&LeCun (2016); Luo et al. (2016); Mayer et al. (2016)) were proposed achieving state-of-the-art performance. The deep learning approach presented by Mayer et al. (2016) is one of the fastest approaches.
- 变异方法：类似地，在变分方法中，普遍使用的平滑度之前，总变异（TV）不会产生令人信服的结果存在弱和模糊的观察，因为它鼓励分段恒定区域导致楼梯套管伪影。 Haene等人（2012）将补丁为基础的先验引入电视框架，形式为小型，分段平面字典。总广义变异（TGV）（Bredies等人（2010））被认为比电视更好，因为它不惩罚分段仿射解决方案。然而，与TV相反，它仅限于凸数据项，即使在存在非凸数据项的情况下，也可以计算全局解。粗略到精细的方法作为立体匹配的非凸问题的近似通常最终导致细节的损失。为了保持细节，Kuschk＆Cremers（2013）通过使用边缘检测将自适应正则化权重整合到TGV框架中，并报告与粗略到粗略方法相比的改进结果。 Ranftl等人（2013）通过提出将非凸函数分解成两个子问题来获得更好的结果，这两个子问题可以在一个凸起的情况下被全局求解，另一个可以通过将功能提升到更高维度的空间来制作凸起。
- 最先进的：在表6中，我们展示了KITTI立体声2015基准的立体声方法的排名。 KITTI基准测试报告了背景区域（D1-bg），前景区域（D1-fg）和所有区域（D1-all）之间的错误（差）像素的百分比。最好的表现方法G¨uney＆Geiger（2015）使用对象知识来补偿反射和无纹理表面上的弱数据项。 Seki＆Pollefeys（2016）在背景区域实现了最佳性能，预测了立体声通信的信心并融入了SGM。最近，提出了深入学习的方法（Zbontar＆LeCun（2016）; Luo等（2016）; Mayer等（2016））提出了最先进的表现。 Mayer等人提出的深度学习方法（2016）是最快捷的方法之一。

- **Superpixels**: An alternative way of modeling piecewise planarity is to explicitly partition the image into superpixels and modeling the surface at each superpixel as a slanted plane (Yamaguchi et al. (2012); G¨uney & Geiger (2015)). However, care must be taken that the super-pixelization is indeed an over-segmentation of the image with respect to planarity, i.e., that no superpixel contains two surfaces which are not co-planar. Yamaguchi et al. (2012) jointly reason about occlusion boundaries and depth in a hybrid MRF composed of both continuous and discrete random variables. Guney & Geiger (2015) use a similar framework to incorporate object-category specific 3D shape proposals which regularize over larger distances. By leveraging semantic segmentation and 3D CAD models, they resolve ambiguities in reflective and textureless regions originating from highly specular surface of cars in the scene as shown in Figure 19.
- **Deep Learning**: In the last years, deep learning approaches (Mayer et al. (2016); Zbontar&LeCun (2016); Luo et al. (2016)) gained popularity in stereo estimation. Mayer et al. (2016) adapt the encoder-decoder architecture proposed by Dosovitskiy et al. (2015) that was used for optical flow estimation (see Section 8.1). The encoder computes abstract features while the decoder reestablishes the original resolution with additional crosslinks between the contracting and expanding network parts. In contrast to the encoder-decoder architecture, ˇ Zbontar & LeCun (2016); Luo et al. (2016) use Siamese network which consists of two sub-networks with shared weights and a final score computation layer. The idea is to train the network for computing the matching cost by learning a similarity measure on small image patches. Zbontar & LeCun (2016) define positive/negative examples as matching and non-matching patches and use a margin loss to train either a fast architecture with a simple dot-product layer in the end or a slow but more accurate architecture which learns score computation with a set of fully connected layers. Luo et al. (2016) use a similar architecture, but formulate the problem as multi-class classification over all possible disparities to capture correlations between different disparities implicitly as visualized in Figure 20.
- 超像素：分段平面建模的另一种方法是将图像明确地划分为超像素，并将每个超像素的表面作为倾斜平面进行建模（Yamaguchi et al。（2012）;G¨uney＆Geiger（2015）） 。然而，必须注意，超像素化确实是相对于平面度的图像的过度分割，即，没有超像素包含不共面的两个表面。山口等（2012）共同理解了由连续和离散随机变量组成的混合MRF中的闭塞边界和深度。 Guney＆Geiger（2015）使用类似的框架来整合在更大距离上规则化的对象​​类特定3D形状提案。通过利用语义分割和3D CAD模型，它们解决了源自现场汽车高度镜面的反射和无纹理区域的模糊性，如图19所示。
- 深度学习：在过去几年中，深度学习方法（Mayer等（2016）; Zbontar＆LeCun（2016）; Luo等（2016））在立体声估计中得到普及。 Mayer等人（2016）适应Dosovitskiy等人提出的编码器 - 解码器架构。 （2015年），用于光流估计（见第8.1节）。编码器计算抽象特征，而解码器重新建立原始分辨率，并在合同和扩展网络部分之间附加交叉链接。与编码器 - 解码器架构相比，Zbontar＆LeCun（2016）;罗等人（2016）使用由具有共享权重的两个子网络和最终分数计算层组成的暹罗网络。这个想法是通过学习小图像补丁的相似性度量来训练网络来计算匹配成本。 Zbontar＆LeCun（2016）将正/负的例子定义为匹配和非匹配补丁，并使用边际损失来训练最终的简单点阵产品层的快速架构，或者学习分数计算的慢但更准确的架构与一套完全连接的层。罗等人（2016）使用了类似的架构，但是将所有可能的差异作为多类别分类来形成问题，以便在图20中隐含地隐藏不同差异之间的相关性。


- Discussion: Stereo estimation has shown great progress in the last years both in terms of accuracy and efficiency. However, some inherent problems refrain it from being marked as solved. Stereo matching is ultimately searching for correspondences in two images based on the assumption of constant appearance. However, appearance frequently changes by cues different than geometry, furthermore occluded regions or pixels leaving the frame cannot be matched. Therefore, failure in those cases is inevitable for methods that solely rely on appearance matching without any other prior assumptions about the geometry. We show accumulated errors of top 15 methods on KITTI stereo benchmark Geiger et al. (2012b) in Figure 21. The most common example of failure case in the autonomous driving context are car surfaces due to shiny and reflective regions. G¨uney & Geiger (2015) specifically address this problem by integrating prior knowledge on possible car shapes. Similarly, windows that are reflective and transparent cannot be matched reliably. As concluded by Hirschm¨uller & Scharstein (2007), strong illumination changes constitute another common source of error such as inside a tunnel or over-exposure on road surfaces. Pixels leaving the frame and occlusions often cause errors for many methods and both require reasoning beyond matching and local interactions. Other specific examples of problematic regions include thin structures like traffic signs, or repetitive ones like fences.
- 讨论：立体声估计在过去几年中在精度和效率方面都取得了很大进展。然而，一些固有的问题避免被标记为解决。基于恒定外观的假设，立体匹配最终搜索两个图像中的对应关系。然而，外观通常由与几何不同的线索改变，而离开框架的遮挡区域或像素也不能匹配。因此，在这些情况下的失败对于仅依赖于外观匹配的方法来说是不可避免的，而对于几何形状没有任何其他先前的假设。我们在KITTI立体声基准Geiger等人显示了前15种方法的累积误差。 （2012b）。自主驾驶环境中故障案例的最常见例子是汽车表面，由于光线和反射区域。 G¨uney＆Geiger（2015）专门针对这个问题，整合了可能的汽车形状的先前的知识。类似地，反射和透明的窗口不能可靠地匹配。如Hirschmuller＆Scharstein（2007）所做的那样，强烈的照明变化构成了隧道内的另一个常见的错误来源，或者在路面上过度曝光。离开帧和遮挡的像素通常会导致许多方法的错误，并且都需要超出匹配和本地交互的推理。有问题的地区的其他具体例子包括像交通标志这样的薄结构，或像栅栏这样的重复的结构。

- 7.2. Multi-view 3D Reconstruction 多视角3D重构
- The goal of multi-view 3D reconstruction is to model the underlying 3D geometry by inverting the image formation process often under certain prior or smoothness assumptions. In contrast to two-view stereo, multi-view reconstruction algorithms in particular address the problems of varying viewpoints and the complete reconstruction of 3D scenes from more than two and potentially a very large number of images. If the camera parameters are known, solving for the 3D geometry of the scene is equivalent to solving the correspondence problem, based on a photo-consistency function which measures the agreement between different viewpoints.
- Taxonomies: Several categorizations of multi-view reconstruction algorithms have been proposed in the literature, typically considering the form of the photo-consistency function, the scene representation, visibility computation, priors, and initialization requirements as in Seitz et al. (2006). From an application perspective, the scene representation is a common way of classifying multi-view reconstruction approaches into depth map, point cloud, mesh, and volumetric.
- 多视图3D重建的目标是通过在某些先前或平滑假设下经常反转图像形成过程来对底层3D几何进行建模。与双视图立体声相比，多视图重建算法特别解决了来自两个以上且潜在的大量图像的变化视点和3D场景的完整重建的问题。如果相机参数是已知的，则解决场景的3D几何相当于基于测量不同视点之间的一致性的照片一致性函数来解决对应问题。
- 分类：文献中已经提出了多视图重建算法的几个分类，通常考虑到照片一致性函数的形式，场景表示，可视性计算，先验和初始化要求，如Seitz等人。 （2006年）。从应用的角度来看，场景表示是将多视图重建方法分为深度图，点云，网格和体积的常见方式。

- **Representations: Depth Map**: The depth map representation typically consists of a depth map for each input view estimated with a 3D modeling pipeline which starts with image matching followed by pose estimation and dense stereo. This representation is usually preferred in scene analysis due to its flexibility and scalability to large scenes. One strategy which is particularly effiective for urban scenes is Plane Sweeping Stereo algorithm (Collins (1996)). It sweeps a family of parallel planes in a scene, projects images onto a plane via planar homographies, then evaluates photo-consistency values on each plane. In large scenes, one of the challenges is to handle massive amount of data in real-time. Pollefeys (2008) propose a large scale, realtime 3D reconstruction system based on depth map representation. The real-time performance is achieved by incorporating a set of components which are particularly efficient on typical urban scenes such as a 2D feature tracker with automatic gain adaptation for handling large dynamic range in natural scenes, and parallel implementations of plane sweeping stereo and depth map fusion on GPU.
- **Representations: Point-cloud**: In contrast to a partial depth map for each view, point-cloud or patch based surface representations reconstruct a single 3D point-cloud model using all the input images. Under spatial consistency assumptions, the pointcloud on the surface of the scene can grow or expand which provides easy model manipulation such as merging and splitting. The representative work for these kind of approaches is Patch-based Multi-View Stereo (PMVS) by Furukawa & Ponce (2010). PMVS starts with a feature matching step to generate a sparse set of patches and then iterate between a greedy expansion step and a filtering step to make patches dense and remove erroneous matches.
- 表示：深度图：深度图表示通常由用3D建模流水线估算的每个输入视图的深度图组成，3D建模流水线以图像匹配开始，随后是姿态估计和密集立体。由于场景分析的灵活性和可扩展性，因此这种表示在场景分析中通常是首选的。一种对城市场景特别有效的策略是平面扫描立体声算法（Collins（1996））。它在场景中扫描一系列平行平面，通过平面同图将图像投影到平面上，然后评估每个平面上的照片一致性值。在大型场景中，挑战之一是实时处理大量数据。 Pollefeys（2008）提出了一种基于深度图表示的大规模实时三维重建系统。实时性能是通过结合一套在典型城市场景上特别有效的组件来实现的，例如具有自动增益适应的2D特征跟踪器，用于处理自然场景中的大动态范围，以及平面扫描立体声和深度图的并行实现融合在GPU上
- 表示：点云：与每个视图的部分深度图相反，点云或基于贴片的表面表示使用所有输入图像重建单个3D点云模型。在空间一致性假设下，场景表面上的点云可以增长或扩展，这提供了容易的模型操作，如合并和分割。这些方法的代表性工作是Furukawa＆Ponce（2010）的基于Patch的多视点立体声（PMVS）。 PMVS从特征匹配步骤开始，以生成一组稀疏的补丁，然后在贪心扩张步骤和过滤步骤之间进行迭代，以使补丁密集并删除错误的匹配项。

- **Representations: Volumetric**: Volumetric approaches represent geometry on a regularly sampled 3D grid, i.e. volume, either as a discrete occupancy function (Kutulakos&Seitz (2000)) or a function encoding distance to the closest surface (level-set) (Faugeras & Keriven (1998)). More recent approaches use a probability map defined at regular voxel locations to encode the probability of occupancy (Bhotika et al. (2002); Pollard & Mundy (2007); Ulusoy et al. (2015)). The amount of memory required is the main limitation for volumetric approaches. There is a variety of methods for dealing with this problem such as voxel hashing (Nießner et al. (2013)) or a data adaptive discretization of the space in the form of a Delaunay triangulation (Labatut et al. (2007)). One effective solution is an octree data structure which is essentially an adaptive voxel grid to allocate high resolution cells only near the surfaces.
- **Representations: Mesh or Surface**: The final representation in reconstruction is typically triangular mesh-based surfaces. Volumetric surface extraction fuses 3D information from an intermediate representation such as depth maps, point clouds, volumes or scans into a single, clean mesh model. Seminal work by Curless & Levoy (1996) proposes an algorithm to accumulate surface evidence into a voxel grid using signed distance functions. The surface is implicitly represented as the zero crossing of the aggregated signed distance functions. It can be extracted using the Marching Cube algorithm Lorensen & Cline (1987) or using volumetric graph cuts to label each voxel as interior or exterior. There are approaches which directly start from images and refine a mesh model using an energy function composed of a data term based on photo-consistency function and a regularization term for smoothness. In these approaches, the energy is usually optimized using gradient descent, where the movement of each vertex is determined by the gradient of the objective function.
- 表示：体积：体积方法表示定期采样的3D网格上的几何，即体积，作为离散占用函数（Kutulakos＆Seitz（2000））或函数编码距离最接近的表面（水平集）的距离（Faugeras ＆Keriven（1998））。最近的方法使用在常规体素位置定义的概率图来编码占用概率（Bhotika等（2002）; Pollard＆Mundy（2007）; Ulusoy等（2015））。所需的内存量是体积方法的主要限制。处理这个问题的方法有很多种，如体素散列（Nießneret al。（2013））或以Delaunay三角剖分形式对空间进行数据自适应离散化（Labatut et al。（2007））。一个有效的解决方案是八叉树数据结构，它本质上是一个适应性体素网格，仅在表面附近分配高分辨率单元。
- 表示：网格或曲面：重建中的最终表示通常是三角形网格的曲面。体积表面提取将3D信息从诸如深度图，点云，体积或扫描的中间表示融合到单个，干净的网格模型中。 Curless＆Levoy（1996）的精神工作提出了一种使用带符号距离函数将表面证据积累到体素网格中的算法。表面隐含地表示为聚合有符号距离函数的过零点。它可以使用Marching Cube算法Lorensen＆Cline（1987）提取，或使用体积图切割将每个体素标记为内部或外部。存在直接从图像开始的方法，并且使用由基于光一致性函数的数据项组成的能量函数和用于平滑度的正则化项来细化网格模型。在这些方法中，通常使用梯度下降优化能量，其中每个顶点的移动由目标函数的梯度确定。

- **Urban Reconstruction**: In this survey, we focus on multiview reconstruction from an autonomous driving perspective which mainly concerns the reconstruction of large urban areas, up to whole cities. The goal of urban reconstruction algorithms is to produce fully automatic, high-quality, dense reconstructions of urban areas by addressing inherent challenges such as lighting conditions, occlusions, appearance changes, high-resolution inputs, and large scale outputs. Musialski et al. (2013) provide a survey of urban reconstruction approaches by following an output-based ordering, namely buildings and semantics, facades and images, and finally blocks and cities.
- **Input Data**: Musialski et al. (2013) point out that ground, aerial and satellite imagery, as well as Light Detection and Ranging (LiDAR) scans are the most commonly used sensors for urban reconstruction. Ground-level imagery is the most prevalent one due to easy acquisition, storage and exchange. Aerial and satellite imagery have become more easily available due to the advances of Web-mapping projects. In contrast to aerial or multi-view imagery, satellite imagery provides a worldwide coverage at a high frequency with lower costs, but also with lower resolution. LiDAR delivers semi-dense 3D point-clouds which are fairly precise, both ground-level and aerial. Some approaches also incorporate several of these data types together in order to combine their complementary strengths. To deal with the challenging conditions of outdoor scenes, other methods leverage additional data sources, like Digital Surface Models (DSMs) which capture the Earth’s surface. DSMs are 2:5D representations of an urban scene that provide a height for each point on a regular grid. In the following, we provide recent examples of dierent input modalities.
- 城市重建：在这次调查中，我们从自主驾驶的角度重点关注多视角重建，主要涉及到大城市，直到整个城市的重建。城市重建算法的目标是通过解决诸如照明条件，遮挡，外观变化，高分辨率输入和大规模输出等内在挑战，来实现城市全自动，高质量，密集的重建。 Musialski等（2013年）通过遵循基于输出的排序，即建筑物和语义，外墙和图像，以及最终的街区和城市，提供城市重建方法的调查。
- 输入数据：Musialski et al。 （2013）指出，地面，空中和卫星图像以及光检测和测距（LiDAR）扫描是城市重建中最常用的传感器。地平面图像是最流行的，因为易于采集，存储和交换。由于Web-mapping项目的进步，空中和卫星图像变得更容易获得。与空中或多视点图像相比，卫星图像在高频率下提供了全球覆盖，成本更低，而且分辨率更低。 LiDAR提供半密度3D点云，这些云点相当精确，包括地面和天线。一些方法也将这些数据类型中的几种结合在一起，以便结合它们的互补优势。为了应对户外场景的挑战性条件，其他方法利用了捕获地球表面的数字表面模型（DSM）等附加数据源。帝斯曼是城市场景的2：5D表示，为常规网格上的每个点提供高度。在下文中，我们提供了不同输入模式的最近例子。

- **Stereo Sequences**: Cornelis et al. (2008) point out that the extraction of detailed 3D information from video streams incur high computational cost for reconstruction algorithms. By keeping the necessary level of detail low, they focus on creating compact, memory ecient 3D city models from a stereo pair, at high speed based on simplified geometry assumptions, namely ruled surfaces for facade and road surfaces. Since objects such as cars which are prevalent in urban scenes violate these assumptions, they integrate the detection and localization of cars into the reconstruction. By leveraging efficient stereo matching, Geiger et al. (2011) propose a system to generate accurate 3D reconstructions of static scenes from stereo sequences in realtime. For online reconstruction, they employ two threads: the first thread performs feature matching and ego-motion estimation, while the second thread performs dense stereo matching and 3D reconstruction.
- **Digital Surface Models (DSM)**: Digital Surface Models are either generated from aerial LiDAR point clouds or Multi-View Stereo (MVS) and adapted to geometric descriptions of urban scenes. MVS-based DSMs can be very noisy and therefore Lafarge et al. (2010) propose to generate DSMs from MVS imagery by reconstructing buildings with an assemble of simple urban structures extracted from a library of 3D parametric blocks. In contrast to MVS-based DSMs, laser scans have been also very popular to acquire 3D city models. Lafarge & Mallet (2012) provide a more complete description of urban scenes by simultaneously reconstructing trees and topologically complex ground surfaces in addition to the buildings from point clouds generated by aerial data. They model the original hybrid representation of buildings by combining two dierent types of 3D representations: primitives for regular parts of buildings as in Lafarge et al. (2010) and mesh patches for modeling atypical surfaces such as irregular roofs.
- 立体声序列：Cornelis et al。 （2008）指出，从视频流中提取详细的3D信息会导致重建算法的高计算成本。通过保持必要的细节水平，他们专注于从立体声对，基于简化的几何假设，即立面和路面的规则表面高速创建紧凑，记忆效率的3D城市模型。由于诸如城市场面普遍存在的汽车等物品违反了这些假设，将汽车的检测和本地化整合到重建中。通过利用高效的立体匹配，Geiger等（2011）提出了一种从立体声序列实时生成静态场景的精确3D重建的系统。对于在线重建，它们采用两个线程：第一个线程执行特征匹配和自主运动估计，而第二个线程执行密集的立体匹配和3D重建。
- 数字表面模型（DSM）：数字表面模型可以从空中LiDAR点云或多视点立体声（MVS）生成，并适用于城市场景的几何描述。基于MVS的DSM可能非常嘈杂，因此Lafarge等人（2010）提出从MVS图像生成DSM，通过从3D参数块库提取的简单城市结构的组合重建建筑物。与基于MVS的DSM相反，激光扫描也非常受欢迎，以获得3D城市模型。拉法基和马勒（2012）通过同时重建树木和拓扑复杂的地面以及由航空数据产生的点云的建筑物，提供了更完整的城市场景描述。他们通过组合两种不同类型的3D表示来模拟建筑物的原始混合表示：如Lafarge等人的常规建筑部分的原始图形。 （2010）和用于建模非典型表面（如不规则屋顶）的网格补丁。

- **Air and Street level**: Fruh et al. (2005) register a series of vertical 2D surface scans and camera images to airborne data (DSMs) to generate textured facade meshes of cities. They propose a class of data processing techniques to create visually appealing facade meshes by removing noisy foreground objects and filling holes in the geometry and texture of building facades. B´odis-Szomor´u et al. (2016) point out that airborne and mobile mapping data provide complementary information and need to be exploited together in order to produce complete and detailed large-scale city models. Airborne sensors can acquire roof structures, ground, and vegetation at large scale while on-road mobile mapping by multi-view stereo approaches or LiDAR provide the facade and street-side details. They propose
a solution to fuse a detailed on-road mobile mapping and a coarser but more complete point cloud from airborne acquisition in a joint surface mesh. Their evaluation shows that the quality of the model improves substantially by fusing streetside details into the airborne model.
- **Stereo Satellite**: Duan & Lafarge (2016) propose a method to produce compact 3D city models composed of ground and building objects from stereo pairs of satellite images. They represent the scene using convex polygons and perform joint classification and reconstruction of the semantic class (ground, roof, and facade) and the elevation of each polygon. Although their evaluation shows that the obtained results are not as accurate as LiDAR scans, the proposed method can produce fast, compact, and semantic-aware models robust to low resolution and occlusion problems.
- 空气和街道等级：Fruh et al。 （2005）将一系列垂直二维表面扫描和摄像机图像注册到机载数据（DSM），以生成城市的纹理外观网格。他们提出了一类数据处理技术，通过去除嘈杂的前景物体并填充建筑立面的几何和纹理中的孔，来创建视觉上吸引人的立面网格。 B'odis-Szomor'u等（2016）指出，机载和移动地图数据提供了补充信息，需要一起利用，以便制作完整和详细的大型城市模型。机载传感器可以大规模获得屋顶结构，地面和植被，而通过多视角立体声方式或LiDAR的道路移动地图可提供立面和街道细节。他们提出
一种解决方案，用于融合详细的道路上移动地图以及从联合表面网格中的空中采集获取更粗糙但更完整的点云。他们的评价表明，通过将街头细节融入机载模型，模型的质量大大提高。
- 立体声卫星：Duan＆Lafarge（2016）提出了一种制作紧凑型3D城市模型的方法，由地面和建筑物体组成的立体声对卫星图像。它们使用凸多边形代表场景，并进行语义类（地面，屋顶和立面）的联合分类和重建以及每个多边形的高程。虽然他们的评估表明所获得的结果不如LiDAR扫描的准确性，但是所提出的方法可以产生对低分辨率和闭塞问题坚固的快速，紧凑和语义感知模型。

- 7.3. Reconstruction and Recognition
- In autonomous driving, it is important to understand both the structural and semantic information of the surroundings. Traditionally, image segmentation methods employ priors entirely in the 2D image domain, i.e., spatial smoothness terms, and reconstruction methods usually encourage piecewise smooth surfaces. It has been long argued that semantics and 3D reconstruction carry valuable information to each other. Similarly to stereo, the motivation to incorporate semantics in reconstruction is photo-consistency failing in case of imperfect and ambiguous image information due to specularities, lack of texture, repetitive structures, or strong lighting changes. Semantic labels provide geometric cues about likely surface orientations at a certain location and help resolving inherent ambiguities. 3D reconstruction lifts the reasoning from 2D to 3D and acts as a strong regularizer by enforcing geometric consistency over multiple images for segmentation.
- Planarity and Primitives: Micusik & Kosecka (2009) present a method to overcome these difficulties by exploiting image segmentation cues as well as presence of dominant scene orientations and piecewise planar structures. In particular, they adopt a super-pixel based dense stereo reconstruction method by using the Manhattan world assumption with three orthogonal plane normals in the MRF formulation. Another way of exploiting piecewise planar structures and the shape repetition is to use primitives such as planes, spheres, cylinders, cones and tori (Lafarge et al. (2010); Lafarge & Mallet (2012); Lafarge et al. (2013)). Primitive arrangement-based approaches provide compactness and reduce complexity. However, they remain simplistic representations and fail to model fine details and irregular shapes. Therefore, Lafarge et al. (2013) propose a hybrid approach which is both compact and detailed. Starting from an initial mesh-based reconstruction, they use primitives for regular structures such as columns and walls, while irregular elements are still described by meshes for preserving details.
- 在自主驾驶中，了解周边环境的结构和语义信息很重要。传统上，图像分割方法在2D图像域中完全使用先验，即空间平滑度项，重建方法通常会促使分段平滑表面。长期以来，语义和3D重建相互传递有价值的信息。与立体声类似，在重建中纳入语义的动机是由于镜面反射，缺乏纹理，重复结构或强烈的照明变化而导致的图像信息不完整和模糊的情况下的照片一致性失败。语义标签提供关于某个位置处可能的表面取向的几何线索，并帮助解决固有的模糊性。 3D重建将推理从2D提升到3D，并通过对多个图像执行几何一致性进行分割，并作为强正则化器。
平面性和原始性：Micusik＆Kosecka（2009）提出了一种通过利用图像分割线索以及主要场景取向和分段平面结构的存在来克服这些困难的方法。特别地，它们采用基于超像素的密集立体重建方法，通过在MRF公式中使用具有三个正交平面法线的曼哈顿世界假设。使用分段平面结构和形状重复的另一种方法是使用诸如平面，球体，圆柱体，锥体和托里（Lafarge等人（2010）; Lafarge＆Mallet（2012）; Lafarge等人（2013））的原语。基于原始布置的方法提供了紧凑性并降低了复杂性。然而，它们仍然是简单的表示，并且不能对精细细节和不规则形状进行建模。因此，拉法基等（2013）提出一种既紧凑又详细的混合方法。从初始的基于网格的重建开始，它们使用基本原理用于常规结构，例如列和墙壁，而不规则元素仍由网格描述以保留细节。

- Volumetric: Volumetric scene reconstruction typically segments the volume into occupied and free-space regions. Haene et al. (2013) present the mathematical framework to extend it to a multi-label volumetric segmentation framework which assigns object classes or a free-space label to voxels as shown in Figure 22. They first learn appearance likelihoods and classspecific geometry priors for surface orientations from the training data. Then, these data-driven priors are used to define unary and pairwise potentials in a continuous formulation for volumetric segmentation. Joint reasoning benefits from typical class-specific geometry, such as the normals of the ground plane pointing upwards. In addition, it provides a class-specific smoothness prior in cases of weak cues for the scene geometry. Their evaluation shows the benefit of such a prior over standard smoothness assumptions such as Total Variation.
- Zhou et al. (2015) propose a method for 3D reconstruction of street scenes from a sequence of fisheye cameras by introducing semantic priors. Motivated by recurring objects of similar 3D shapes in outdoor scenes, they first localize buildings and vehicles using 3D object detectors and then jointly reconstruct them while learning a volumetric model of their shape. This allows to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations of the respective category.
- 体积：体积场景重建通常将体积分成占用和自由空间区域。 Haene等人（2013）提出了数学框架，将其扩展到多标签体积分割框架，该框架将对象类或自由空间标签分配给体素，如图22所示。他们首先学习表面取向的外观似然性和类特定几何先验从训练数据。然后，这些数据驱动的先验被用来定义连续公式中的一元和成对的电位，用于体积分割。联合推理受益于典型的类别特定几何，如地平面向上的法线。此外，在场景几何的弱提示的情况下，它提供了一个类别的平滑度。他们的评估显示了这样一个先前过度的标准平滑假设（如总变异）的好处。
- Zhou et al。 （2015）提出了一种通过引入语义先验从一系列鱼眼摄影机的街道场景3D重建的方法。由于户外场景中类似3D形状的反复出现的对象，他们首先使用3D物体检测器对建筑物和车辆进行本地化，然后在学习其形状的体积模型的同时重建它们。这允许在完成缺失表面时减少噪声，因为具有类似形状的对象受益于相应类别的所有观察。

- **Monocular Video**: Failures in multi-view stereo cause problems for approaches like Haene et al. (2013) which require dense depth measurements. Using a monocular image stream as input, Kundu et al. (2014) propose another joint reasoning approach over a sparse point cloud from SfM and dense semantic labeling of the frames. This way, 3D semantic representation is temporally coherent without additional cost. They model the problem with a higher order CRF in 3D which allows realistic scene constraints and priors such as 3D object support. In addition, they explicitly model the free space which provides cues to reduce ambiguities, especially along weakly supported surfaces. Their evaluation on monocular datasets Camvid and Leuven shows improved 3D structure compared to traditional SfM and state-of-the-art multi-view stereo as well as better segmentation quality over video segmentation methods in terms of both per pixel accuracy and temporal consistency.
- 单目视频：多视点立体声故障导致Haene等人等方法出现问题。 （2013年），需要深度测量。 使用单目图像流作为输入，Kundu et al。 （2014）提出了从SfM的稀疏点云和框架的密集语义标注的另一个联合推理方法。 这样，3D语义表示在时间上相干而没有额外的成本。 他们用3D中的高阶CRF对问题进行建模，这样可以实现场景约束和3D对象支持等先修。 此外，他们明确地模拟了提供线索的自由空间，以减少模糊，特别是在弱支撑的表面上。 他们对单目数据集的评估Camvid和Leuven显示出与传统SfM和最先进的多视点立体声相比改进的3D结构以及在每像素精度和时间一致性方面对视频分割方法的更好的分割质量。

- **Volumetric**: Large-scale: Previous works on semantic reconstruction (Haene et al. (2013); Kundu et al. (2014)) are limited to small scenes and low resolution, because of their large memory footprint and computational cost. To scale them up to large scenes, Blaha et al. (2016) point out that high resolution is not required for large regions such as free space, parts under the ground, or inside the building. They propose an extension of Haene et al. (2013) by employing an adaptive octree data structure with coarse-to-fine optimization, in an application to generate 3D city models from terrestrial and aerial images. Starting from a coarse voxel grid, they solve a sequence of problems in which the solution is gradually refined only near the predicted surfaces. The adaptive refinement saves memory and runs much faster while still being as accurate as the fixed voxel discretization at the highest target resolution, both in geometric reconstruction and semantic labeling.
- Besides the spatial extent, the number of different semantic labels is also a problem for scalability due to increasing memory requirements. The complexity is quadratic in the number of labels due to indicator variables for the transitions between the different labels. Cherabier et al. (2016) propose to divide the scene into blocks in which only a set of relevant labels is active, since absence of many semantic classes from a specific block can be determined early on. Accordingly, they can deactivate a label right from the beginning of the optimization which leads to a more efficient processing. The set of active labels in each block is updated during the iterative optimization to recover from wrong initializations. Their evaluation shows that they can increase the number of labels from six to nine with a significant gain in memory compared to Haene et al. (2013).
- 体积：大规模：以前的语义重建（Haene et al。（2013）; Kundu et al。（2014））仅限于小场景和低分辨率，因为它们具有较大的内存占用和计算成本。为了将它们扩大到大型场景，Blaha等（2016年）指出，大型地区，如自由空间，地下部分，建筑物内部都不需要高分辨率。他们提出延长Haene等（2013）通过在从地面和航空图像生成3D城市模型的应用中采用具有粗略到优化的自适应八叉树数据结构。从粗体素网格开始，它们解决了一系列问题，其中解决方案仅在预测表面附近逐渐完善。自适应细化可以节省内存并且运行得更快，同时在几何重建和语义标注中仍然与最高目标分辨率下的固定体素离散度一样准确。
- 除空间范围外，由于内存需求的增加，不同语义标签的数量也是可扩展性的问题。由于不同标签之间的转换的指示符变量，标签数量的复杂度是二次方。 Cherabier等人（2016）提出将场景划分为只有一组相关标签是活动的块，因为可以在早期确定不存在来自特定块的许多语义类别。因此，它们可以从优化开始就停用标签，这导致更有效的处理。每个块中的活动标签集在迭代优化期间被更新，以从错误的初始化中恢复。他们的评估表明，与Haene等人相比，他们可以将标签的数量从6个增加到9个，记忆显着增加。 （2013年）。

- **Shape Priors**: Advances in sensors to acquire 3D shapes and the performance of object detection algorithms have encouraged the use of 3D shape priors in 3D reconstruction. Dimensionality reduction is an effective and popular way of representing shape knowledge. Early approaches use linear dimensionality reduction such as PCA to capture the shape variance in low dimensional latent shape spaces. More recent approaches use nonlinear dimensionality reduction such as Gaussian Process Latent Variable Models (GP-LVM) (Dame et al. (2013)). Dame et al. (2013) investigate the importance of shape priors in a monocular SLAM approach. In parallel with depth estimation, they refine an object’s pose, shape and scale to match an initial segmentation and depth cues. It is finally fused into the volumetric representation. Their experiments show improvement in transparent and specular surfaces, and even in unobserved parts of the scene. In addition to mean shape, Bao et al. (2013) propose to learn a set of anchor points as representative of object shape across several instances. They first perform an initial alignment using 2D object detectors. Next, they align the point cloud from SfM with the mean shape by matching anchor points, and then warp and refine it to approach the actual shape. Their evaluation demonstrates that the model is general enough to learn semantic priors for different object categories such as car, fruit, and keyboard by handling large shape variations across instances.
- While previous approaches (Dame et al. (2013); Bao et al. (2013)) try to fit a parametric shape model to input data, Haene et al. (2014) model the local distribution of normals for an object. They propose an object class specific shape prior in the form of spatially varying anisotropic smoothness terms. Similar to multi-label segmentation approach of Haene et al. (2013), they divide the reconstruction into object region and the supporting ground and apply the shape prior only on object to guide the optimization to the right shape.
- 先前的形状：获取3D形状的传感器的进步和对象检测算法的性能已经鼓励在3D重建中使用3D形状先验。尺寸减少是表示形状知识的有效和受欢迎的方式。早期方法使用线性维数降低（如PCA）来捕获低维隐形形状空间中的形状方差。更近期的方法使用非线性维数降低，如高斯过程潜在变量模型（GP-LVM）（Dame等（2013））。 Dame等人（2013）研究形状先验在单眼SLAM方法中的重要性。与深度估计并行，它们优化对象的姿态，形状和尺度以匹配初始分割和深度线索。它最终融合到体积表示中。他们的实验显示透明和镜面表面的改进，甚至在场景的未观察的部分。除了平均形状外，Bao et al。 （2013）提出在几个实例中学习一组锚点作为对象形状的代表。他们首先使用2D物体检测器进行初始对准。接下来，他们通过匹配锚点将SfM的点云与平均形状对齐，然后扭曲和细化以接近实际形状。他们的评估表明，该模型足以通过处理实例之间的大的形状变化来学习不同对象类别（如汽车，水果和键盘）的语义预测。
- 尽管以前的方法（Dame等人（2013）; Bao等（2013））试图将参数化形状模型拟合为输入数据，Haene等人（2014）模拟了一个对象的法线的局部分布。他们以空间变化各向异性平滑度项的形式提出了对象类特定形状。类似于Haene等人的多标签分割方法（2013），他们将重建划分为物体区域和支撑地面，然后仅在物体上应用形状，将优化引导到正确的形状。

- **Data-Driven**: Instead of modeling a semantic prior for each object explicitly, Wei et al. (2014) propose a data-driven regularization to transfer the shape information of the disparity or flow from semantically matched patches in the training database using the SIFT flow algorithm. They represent the shape information as the relative relationship of scene properties instead of absolute values. It is mainly for reusability of scene properties, such as modeling disparity of car independent of its position. They compare their data-driven prior against popular smoothness terms on Sintel and show improved performance while being comparable to state-of-the-art on KITTI.
- 数据驱动：而不是明确地为每个对象建模一个语义先验，Wei et al。 （2014）提出了一种数据驱动的正则化方法，使用SIFT流程算法从训练数据库中的语义匹配补丁中传输视差或流量的形状信息。 它们将形状信息表示为场景属性而不是绝对值的相对关系。 它主要用于场景属性的可重用性，如独立于其位置的汽车模型差异。 他们比较了他们的数据驱动先前与Sintel上的流行平滑度条款，并显示出改进的性能，同时与KITTI上的最先进技术相媲美。





  8. 运动与姿势估测
  8.1 2D 运动估测-光学流
  8.2 3D 运动估测-场景流
  8.3. Ego-Motion 估计
  8.4. 同步定位与构图 (SLAM)
  8.5. 定位
  9. 追踪
  追踪的目标是给定传感器测量数据的情况下实时评估一个或多个目标的状态。典型来说，目标的状态由它在一定时间的位置、速度和加速度来表达。追踪其他车辆对自动驾驶任务而言非常重要。举个例子，汽车刹车距离随速度变化会有次方级的变化。为了防止相撞，系统需要足够提前做出反应。其他车辆的轨迹足以预测停车的位置和可能相撞的情况。
  在自行车和行人的案例中，比较难以预测未来的行为，因为他们可能会突然改变方向。然而，结合其他车辆的分类进行追踪，能够调整汽车在这种情况下的速度。此外，追踪其他汽车可被用来进行自动距离控制，提前预估其他车辆可能做的变动。
  9.1 立体追踪
  9.2 行人追踪
  9.3 顶级成果
  9.4 讨论
  10. 场景理解
  自动驾驶的基本需求之一是充分理解其周遭环境，比如复杂的交通场景。户外场景理解的复杂任务包括若干个子任务，比如深度估计、场景分类、目标探测与追踪、事件分类以及更多，其中每一个子任务描述场景的一个特定方面。联合建模这些特定方面以利用场景不同元素之间的关系并获得一个整体理解，这样做是有益的。大多数场景理解模型的目标是获得一个丰富但紧凑的场景表征，这个场景包含所有的元素，比如布局元素、交通参与者以及彼此之间的关系。相比于 2D 图像域中的推理，3D 推理在解决几何场景理解的问题上起着重要作用，并以 3D 目标模型、布局元素、闭塞关系等形式促使场景产生了更多的信息表征。场景理解的一个特殊挑战是城市市区与郊区交通情景的阐释。相较于高速公路和农村公路，市区场景包含了很多独立移动的交通参与者，道路与十字路口几何布局中的更多变化性，以及由于模糊的视觉特征和光照变化所带来的难度升级。
  从单一图像到视频
  结合目标探测与跟踪
  其他表征
  11. 传感器运动控制的端到端学习
  当前最先进的自动驾驶方法包含大量的模型，例如（交通信号、灯、汽车、行人的）探测、（车道、门面的）分割、运动估计、交通参与者的跟踪，重建。然后，这些组件的结果按照控制系统的规则组合起来。但是，为了解决操控汽车方向和速度的问题，这需要稳健地解决场景理解中的诸多开放性难题。最近的文献提出了作为替代性方案的若干个端到端自动驾驶方法。端到端驾驶使用的是从一个感觉输入（比如，正面摄像头图像）直接映射到驾驶操作（比如，转向角）的独立系统。
  结论
    本文中，我们就自动驾驶计算机视觉的难题、数据集和方法提供了一个综合性调查。为了完成这一目标，我们的调查同时涵盖了最为相关的历史资料，以及识别、重建、运动估测、追踪、场景理解、端到端学习等当前最先进的专门主题。通过使用 KITTI 基准的全新深入质量分析并考虑其他数据集，我们还讨论了开放问题和当前这些主题下的研究挑战。我们的交互式在线工具平台运用图形可视化了分类方法，从而可使你轻松浏览被调查的文献。将来，我们计划在这一交互式平台上不断更新相关文献，为这一领域提供一个实时的概观。我们希望该项调查和该工具平台可进一步激发新研究，并且通过这一详尽的概述，使得初学者更容易进入该领域
