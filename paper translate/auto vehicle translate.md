## Computer Vision for Autonomous Vehicles:Problems, Datasets and State-of-the-Art
自动驾驶技术的计算机视觉：问题，数据和前沿技术

###Abstract 摘要
- Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. |As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. |While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists.|This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. |Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. |Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.
Keywords: Computer Vision, Autonomous Vehicles, Autonomous Vision
- 概述：人类见证了最近几年AI相关领域的惊人进步，如计算机视觉，机器学习和自动驾驶。<p>
然而任何一个快速发展的领域，保持领先或刚开始进入这些领域变得越来越难(业内人员难以跟上行业节奏或者业外人员难入行)。<p>
尽管已经(有人)发表了一些这方面的专题研究文章，但在自动驾驶技术中，计算机视觉的问题、数据和方法至今没有普遍的研究。<p>
对于这个话题，这篇论文试图通过提供对前沿技术的研究来减少这种缺口。我们的研究包括最相关的历史资料和当前最前沿的技术，包括识别、重建运动估测、追踪场景理解和端到端学习等。<p>
为完成这个目标，我们首先通过分类学对每一个方法进行分类，然后在一些具有挑战性的基础数据集上，如KITTI、ISPRS、MOT和Cityscapes上分析每一个方法在前沿技术上的表现<p>
此外，我们还讨论了一些开放问题和当前研究的挑战，为了轻松访问和适应缺失的参考，我们将提供一个具有主题和方法的驾驶交互平台，并提供额外信息和每篇论文的项目链接<p>
关键词：计算机视觉，自动驾驶，自主视觉

### previous 前言
- Since the first successful demonstrations in the 1980s (Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988)), great progress has been made in the field of autonomous vehicles. |Despite these advances, however, it is safe to believe that fully autonomous navigation in arbitrarily complex environments is still decades away. |The reason for this is two-fold: First, autonomous systems which operate in complex dynamic environments require artificial intelligence which generalizes to unpredictable situations and reasons in a timely manner. |Second, informed decisions require accurate perception, yet most of the existing computer vision systems produce errors at a rate which is not acceptable for autonomous navigation.
- 从20世纪80年代首次成功展示以来(Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988))(Dick-manns & Mysliwetz (1992); Dickmanns & Graefe (1988); Thorpeet al. (1988))，自动驾驶技术领域已经取得了很大进展<p>
尽管有了这些进展，但在任意复杂环境中，实现完全自动驾驶仍然被认为需要几十年<p>
原因有两点：第一，在复杂的、动态的环境中运行的自动驾驶系统需要人工智能来归纳不可预测的情形和原因，给出及时的方法<p>
第二，信息的决策需要准确的感知，目前大多数已有的计算机视觉系统有一定的错误率，这是自动驾驶技术无法接受的

- In this paper, we focus on the second aspect which we call autonomous vision and investigate the performance of current perception systems for autonomous vehicles. |Towards this goal, we first provide a taxonomy of problems and classify existing datasets and techniques using this taxonomy, describing the pros and cons of each method. Second, we analyze the current state-of-the-art performance on several popular publicly available benchmarking datasets. |In particular, we provide a novel in-depth qualitative analysis of the KITTI benchmark which shows the easiest and most difficult examples based on the methods submitted to the evaluation server. |Based on this analysis, we discuss open research problems and challenges. To ease navigation, we also provide an interactive online tool which visualizes our taxonomy using a graph and provides additional information and links to project pages in an easily accessible manner. |We hope that our survey will become a useful tool for researchers in the field of autonomous vision and lowers the entry barrier for beginners by providing an exhaustive overview over the field.
- 在这篇论文中，我们关注第二个方面的问题，也就是自动驾驶视觉，同时调查最近的自动驾驶视觉中感知系统的表现<p>
为完成这个目标，我们首先给出了问题的分类，归类了已有的数据和可使用的技术，描述每种方法的优缺点。第二，我们在几个流行的公开数据集上分析了最近前沿成果的表现<p>
特别是我们给出一种KITTI基准的新的深入定性分析，这些分析展示了提交给评价服务器的方法中最简单和最困难的例子<p>
基于这些分析，我们讨论了开放的研究问题和挑战，为了简化学习，我们也给出一个在线交互式工具，用图像可视化了分类，并提供额外信息和一个简单可行的方法与项目页链接<p>
我们希望我们的研究能够成为自动驾驶领域研究人员的一个有用的工具，并通过透彻的概述，降低新人进入该领域的门槛

- There exist several other related surveys. Winner et al. (2015) explains in detail systems for active safety and driver assistance, considering both their structure and their function. |Their focus is to cover all aspects of driver assistance systems and the chapter about machine vision covers only the most basic concepts of the autonomous vision problem. |Klette (2015) provide an overview over vision-based driver assistance systems. They describe most aspects of the perception problem at a high level, but do not provide an in-depth review of the state-of-the-art in each task as we pursue in this paper. |Complementary to our survey, Zhu et al. (2017) provide an overview of environment perception for intelligent vehicles, focusing on lane detection, traffic sign/light recognition as well as vehicle tracking. |In contrast, our goal is to bridge the gap between the robotics, intelligent vehicles, photogrammetry and computer vision communities by providing an extensive overview and comparison which includes works from all fields.
- 目前也有一些其它相关的研究，Winner et al. (2015)详细地解释了主动安全性和驾驶辅助系统，同时考虑了它们的结构和功能<p>
这些研究注重覆盖辅助驾驶系统的所有方面，但关于机器视觉的章节只覆盖到了自动驾驶技术中最基础的概念。<p>
Klette (2015)提供了一个基于视觉的辅助驾驶系统的概述，他们描述了高层次感知问题的大部分方面，但并没有像我们在论文中追求的一样，给出在各种前沿任务中比较深入的评测<p>
Zhu et al. (2017)提出了智能汽车环境感知的概述，聚焦在车道检测，交通信号灯识别和机车追踪问题，这与我们的研究相互补充。<p>
相比较下，我们的目标是通过提供广泛的概述和比较，包括在这个领域所有的成果，在机器人、智能汽车、摄影测绘和计算机视觉之间建立起一座桥梁

### 1.History of Autonomous Driving 自动驾驶技术历史
- ####1.1. Autonomous Driving Projects 自动驾驶项目
- Many governmental institutions worldwide started various projects to explore intelligent transportation systems (ITS). The PROMETHEUS project started 1986 in Europe and involved more than 13 vehicle manufacturers, several research units from governments and universities of 19 European countries. |One of the first projects in the United States was Navlab Thorpe et al. (1988) by the Carnegie Mellon University which achieved a major milestone in 1995, by completing the first autonomous drive from Pittsburgh, PA and Sand Diego, CA. |After many initiatives were launched by universities, research centers and automobile companies, the U.S. government established the National Automated Highway System Consortium (NAHSC) in 1995. |Similar to the U.S., Japan established the Advanced Cruise-Assist Highway System Research Association in 1996 among many automobile industries and research centers to foster research on automatic vehicle guidance. |Bertozzi et al. (2000) survey many approaches to the challenging task of autonomous road following developed during these projects. They concluded that suffcient computing power is become increasingly available, but diffculties like reflections, wet road, direct sunshine, tunnels and shadows still make data interpretation challenging. |Thus, they suggested the enhancement of sensor capabilities. They also pointed out that the legal aspects related to the responsibility and impact of automatic driving on human passengers need to be considered carefully. |In summary, the automation will likely be restricted to special infrastructures and will be extended gradually.
- 世界各地的许多政府机构启动各式各样的项目来开发智能交通系统（ITS）。PROMETHEUS这个项目1986年在欧洲启动，包括超过13个交通工具生产商，当中的许多研究成员来自19个欧洲国家的政府和高校。<p>
美国的其中一个项目就是由卡耐基梅隆大学的Navlab Thorpe等人(1988)创建的。这个项目完成了第一次从Pittsburgh，PA,Sand Diego和CA的自动驾驶，在1995年是一个重要的里程碑。<p>
在许多大学，研究中心和自动驾驶公司的倡议下，美国政府在1995年成立了自动化公路系统联盟（NAHSC）。<p>
和美国一样，日本于1996年成立了高级巡航公路系统研究协会(Advanced Cruise-Assist Highway System Research Association)，包括各大自动驾驶公司和研究中心，来促进自动驾驶导航的研究。<p>
Bertozzi等人（2000）调查了许多具有挑战性的任务(通过这些项目发展的自动道路跟随),给出解决方法。他们得出结论，计算能力逐渐得到满足，但像反射，湿面潮湿，阳光直射，隧道和阴影这样的困难仍然使数据解释具有挑战性。<p>
因此，他们建议提高传感器性能，同时也指出，关系到自动驾驶对行人法律方面的责任和影响，应该认真的考虑<p>  
总之，自动化技术(发展)可能会受限于特殊的基础设施，然后再慢慢的普及开来。

- Motivated by the success of the PROMETHEUS projects to drive autonomously on highways, Franke et al. (1998) describe a real-time vision system for autonomous driving in complex urban traffic situations. |While highway scenarios have been studied intensively, urban scenes have not been addressed before. Their system included depth-based obstacle detection and tracking from stereo as well as a framework for monocular detection and recognition of relevant objects such as traffic signs.
- The fusion of several perception systems developed by Vis-Lab have led to several proto-type vehicles including ARGO Broggi et al. (1999), TerraMax Braid et al. (2006), and BRAiVE Grisleri & Fedriga (2010). |BRAiVE is the latest vehicle proto-type which is now integrating all systems that VisLab has developed so far. Bertozzi et al. (2011) demonstrated the robustness of their system at the VisLab Intercontinental Autonomous Challenge, a semi-autonomous drive from Italy to China. |The onboard system allows to detect obstacles, lane marking, ditches,berms and identify the presence and position of a preceding vehicle. The information produced by the sensing suite is used to perform different tasks such as leader-following and stop & go.
- PROMETHEUS项目可以实现在高速公路上自动驾驶，在这个成功的案例推动下，Franke等人描述了在复杂的城市交通场景下的自动驾驶的实时视觉系统。<p>
虽然在此之前公路场景情况已经有很多深入的研究，但城市场景却从未得到解决。他们的系统包括基于深度的障碍检测和立体追踪，以及针对相关物体（比如：交通信号）的单目检测和识别框架。<p>
- Vis-Lab发展的多种传感系统的融合促成了几款原型车包括ARGO Broggi（1999），TerraMax Braid（2006）和BRAiVE Grisleri & Fedriga（2010）的出现<p>
BRAiVE是目前VisLab开发的整合所有系统的最新车型。 Bertozzi等人（2011）在VisLab洲际自治挑战赛（VisLab Intercontinental Autonomous Challenge，意大利到中国的半自主驾驶）展示了其系统的稳健性（鲁棒性）。<p>
车载系统允许检测障碍物，标记车道、沟渠、护堤，并识别前方是否存在车辆和车辆位置。感应套件提供的信息被用于执行不同的任务，如(leader-following)和前进/停止。<p>?

- The PROUD project Broggi et al. (2015) slightly modified the BRAiVE prototype Grisleri & Fedriga (2010) to drive in urban roads and freeways open to regular traffic in Parma. |Towards this goal they enrich an openly licensed map with information about the maneuver to be managed (e.g. pedestrian crossing, traffic light, . . . ). |The vehicle was able to handle complex situations such as roundabouts, intersections, priority roads, stops, tunnels, crosswalks, traffic lights, highways, and urban roads without any human intervention.
- The V-Charge project Furgale et al. (2013) presents an electric automated car outfitted with close-to-market sensors. A fully operational system is proposed including vision-only localization, mapping, navigation and control. |The project supported many works on different problems such as calibration Heng et al. (2013, 2015), stereo H¨ane et al. (2014), reconstruction Haene et al. (2012, 2013, 2014), SLAM Grimmett et al.(2015) and free space detection H¨ane et al. (2015). In addition to these research objectives, the project keeps a strong focus on deploying and evaluating the system in realistic environments.
- PROUD的项目Broggi（2015）略微修改了BRAiVE原型Grisleri & Fedriga（2010）使得汽车可以在parma城市道路和高速公路的常规交通情况下开车。<p>
为了实现这一目标，他们丰富了一份公开授权的地图，其中包含有待完成的机动信息（比如行人过路，交通信号灯等）。<p>
该车辆能够在没有人为干涉的情况下处理复杂的场景，例如回旋处，交叉口，优先道路，站点，隧道，人行横道，交通信号灯，高速公路和城市道路。<p>
- V-Charge项目Furgale等人 （2013年）提供配备了近距离市场（close-to-market）传感器的电动自动车。提出了一个全面可使用的系统，包括视觉定位，映射，导航和控制。<p>
该项目解决了诸多困难比如，Heng et al. (2013, 2015)的校准问题, H¨ane(2014)的立体问题,Haene(2012, 2013, 2014)的重建问题, Grimmett(2015)的SLAM问题和 H¨ane(2015)的空白区域检测的问题。除了这些研究目标，该项目还非常重视在现实环境中部署和系统评估。

- Google started their self-driving car project in 2009 and completed over 1,498,000 miles autonomously until March 2016 in Mountain View, CA, Austin, TX and Kirkland, WA. |Different sensors (i.a. cameras, radars, LiDAR, wheel encoder, GPS) allow to detect pedestrians, cyclists, vehicles, road work and more in all directions. |According to their accident reports, Google’s self-driving cars were involved only in 14 collisions while 13 times were caused by others. In 2016, the project was split off to Waymo, an independent self-driving technology company.
- Tesla Autopilot is an advanced driver assistant system developed by Tesla which was first rolled out(推出) in 2015 with version of their software. The automation level of the system allows full automation but requires the full attention of the driver to take control if necessary. |From October 2016, all vehicles produced by Tesla were equipped with eight cameras, twelve ultrasonic sensors and a forward-facing radar to enable full self-driving capability.
- Google于2009年开始了自驾车项目，直到2016年3月完成了超过1,498,000英里的驾驶距离，在美国加利福尼亚州奥斯汀市的Mountain View，WA和柯克兰。<p>
不同的传感器（例如摄像机，雷达，LiDAR，车轮编码器，GPS）可以全方位的检测行人，骑自行车的人，车辆，道路工作等等。<p>
据他们的事故报道，Google的自动驾驶车只涉及14次碰撞，13次是由别人造成的。 在2016年，这个项目分引入到了一家独立的自动驾驶技术公司Waymo。<p>
- Tesla Autopilot是由特斯拉开发的高级驾驶员辅助系统，该系统于2015年第一次推出其视觉软件。系统的自动化级别允许完全的自动化，但是仍然需要 要求驾驶员集中注意来控制。<p>
从2016年10月起，特斯拉生产的所有车辆配备了8台摄像机，12台超声波传感器和一个前置雷达，以实现全自动驾驶能力。<p>

- **Long Distance Test Demonstrations**: In 1995 the team within the PROMETHEUS project Dickmanns et al. (1990); Franke et al. (1994); Dickmanns et al. (1994) performed the first autonomous long-distance drive from Munich, Germany, to Odense, Denmark, at velocities up to 175 km/h with about 95% autonomous driving. |Similarly, in the U.S. Pomerleau & Jochem (1996) drove from Washington DC to San Diego in the ’No hands across America’ tour with 98% automated steering yet manual longitudinal control.
- In 2014, Ziegler et al. (2014) demonstrated a 103 km ride from Mannheim to Pforzheim Germany, known as Bertha Benz memorial route, in nearly fully autonomous manner. |They present an autonomous vehicle equipped with close-to-production sensor hardware. Object detection and free-space analysis is performed with radar and stereo vision. Monocular vision is used for traffic light detection and object classification. |Two complementary vision algorithms, point feature based and lane marking based, allow precise localization relative to manually annotated digital road maps. They concluded that even thought the drive was successfully completed the overall behavior is far inferior to the performance level of an attentive human driver.
- 长距离测试演示：1995年，PROMETHEUS项目里Dickmanns（1990）、Franke（1994）、Dickmanns（1994年）的团队演示了从德国慕尼黑（Munich）到丹麦欧登塞（Odense）进行的第一次自动长途驾驶，速度达175公里/小时，其中约95％为自主驾驶。<p>
同样，在美国Pomerleau和Jochem（1996）在‘No hands across from America ???’中从华盛顿特区开往圣地亚哥，整个行程中有98％的自动驾驶和偶尔的手动纵向控制。<p>
- 2014年，Zieglar（2014）以近乎完全自动的方式，展示了从曼海姆（Mannheim）到德国普福尔茨海姆（Pforzheim Germany）的103km的骑行，也就是众人所熟知的Bertha Benz纪念路线。<p>
他们展示了一种装配有接近生产(close-to-production)的传感器硬件的自动驾驶车辆。由雷达radar和立体视觉来进行物体检测和空白区域分析。单目视觉用来检测交通信号灯和目标分类。<p>
两种互补的算法，基于点特征和基于场景标记，允许相对于手动注释的数字路线图进行精确定位。他们得出结论，甚至认为自动驾驶虽然成功完成了，但是整体行为远远达不到细心的驾驶司机的水平。

- Recently, Bojarski et al. (2016) drove autonomously 98% of the time from Holmdel to Atlantic Highlands in Monmouth County NJ as well as 10 miles on the Garden State Parkway without intervention. |Towards this goal, a convolutional neural network which predicts vehicle control directly from images is used in the NVIDIA DRIVETM PX self-driving car. The system is discussed in greater detail in Section 11.
- While all aforementioned performed impressively, the general assumption of precisely annotated road maps as well as prerecorded maps for localization demonstrates that autonomous systems are still far from human capabilities. |Most importantly, robust perception from visual information but also general artificial intelligence are required to reach human level reliability and react safely even in complex innercity situations.
- 最近，Bojarski（2016）从霍尔姆德尔（Holmdel）到新泽西州蒙茅斯县（Monmouth）的大西洋高原，以及在花园州立大道没有任何干扰的自动行驶了10英里，其中98%是在自动驾驶。<p>
为了实现这一目标，在NVIDIA DRIVETM PX自动驾驶车中使用了一种从图像直接预测车辆控制的卷积神经网络。该系统在第11节中有更详细的讨论。<p>
- 虽然所有上述表现令人印象深刻，但精确注释路线图的一般假设，以及用于定位的预先载入的地图证实了自主性系统仍然差强人意。<p>
最重要的是，这不仅需要视觉信息的强大的感知，也需要一般的人工智能达到和人一样的可靠性，并且在复杂的城市情况下也能安全地做出反应。<p>

- 1.2. Autonomous Driving Competitions 自动驾驶竞赛
- The European Land Robot Trial (ELROB) is a demonstration and competition of unmanned systems in realistic scenarios and terrains, focusing mainly on military aspects such as reconnaissance and surveillance, autonomous navigation and convoy transport. In contrast to autonomous driving challenges, ELROB scenarios typically include navigation in rough terrain.
- The first autonomous driving competition focusing on road scenes (though primarily dirt roads) has been initiated by the American Defense Advanced Research Projects Agency (DARPA) in 2004. The DARPA Grand Challenge 2004 offered a prize money of 1 million for the team first finishing a 150 mile route which crossed the border from California to Nevada. |However, none of the robot vehicles completed the route. One year later, in 2005, DARPA announced a second edition of its challenge with 5 vehicles successfully completing the route (Buehler et al.(2007)). The third competition of the DARPA Grand Challenge, known as the Urban Challenge (Buehler et al. (2009)), took place on November 3, 2007 at the site of the George Air Force Base in California. The challenge involved a 96 km urban area course where traffic regulations had to be obeyed while negotiating with other vehicles and merging into traffic.
- The Grand Cooperative Driving Challenge (GCDC, see also Geiger et al. (2012a)), a competition focusing on autonomous cooperative driving behavior was held in Helmond, Netherlands in 2011 for the first time and in 2016 for a second edition. During the competition, teams had to negotiate convoys, join convoys and lead convoys. The winner was selected based on a system that assigned points to randomly mixed teams.
- European Land Robot Trial （ELROB）是现实场景和地形中无人系统的示范与竞赛，主要集中在军事方面，如侦察监视，自动导航和车队运输。与自主驾驶挑战相反，ELROB场景通常包括崎岖地形的导航。<p>
- 2004年，美国国防高级研究计划署（DARPA）发起了第一个专注于道路场景（主要是泥土路）的自动驾驶比赛。挑战赛提供了100万美元的奖金给首先完成从加利福尼亚州内华达州过境的150英里的路线。<p>
然而，机器人车辆都没有完成路线。 一年后，DARPA公布了第二版的挑战，5辆车顺利完成了路线（Buehler（2007））。DARPA大挑战赛的第三场比赛，被称为城市挑战赛（Buehler（2009）），于2007年11月3日在乔治航空加利福尼亚州的基地。<p>
这个挑战涉及到一个96公里的城市地区航线，在这段路程中车辆在对其他车辆进行判断并汇合车流时，必须遵守交通法规。
- 专注于自动合作驾驶行为的大型合作驾驶挑战（GCDC，Geiger et al（2012a））在荷兰赫尔蒙德（Helmond）举行,2011年首次，2016年第二次。在比赛中，团队需要判断，加入和引导车队。获胜者是基于给随机混合团队分配点数的系统选出来的。<p>

###2.datasets & Benchmarks 数据集和基准
- Datasets have played a key role in the progress of many research fields by providing problem specific examples with ground truth. They allow quantitative evaluation of approaches providing key insights about their capacities and limitations. |In particular, several of these datasets Geiger et al. (2012b); Scharstein & Szeliski (2002); Baker et al. (2011); Everingham et al. (2010); Cordts et al. (2016) also provide online evaluation servers which allow for a fair comparison on held-out test sets and provide researchers in the field an up-to-date overview over the state-of-the-art. |This way, current progress and remaining challenges can be easily identified by the research community. In the context of autonomous vehicles, the KITTI dataset Geiger et al. (2012b) and the Cityscapes dataset Cordts et al.(2016) have introduced challenging benchmarks for reconstruction, motion estimation and recognition tasks, and contributed to closing the gap between laboratory settings and challenging real-world situations. |Only a few years ago, datasets with a few hundred annotated examples were considered sufficient for many problems. The introduction of datasets with many hundred to thousands of labeled examples, however, has led to spectacular breakthroughs in many computer vision disciplines by training high-capacity deep models in a supervised fashion. |However, collecting a large amount of annotated data is not an easy endeavor, in particular for tasks such as optical flow or semantic segmentation. This initiated a collective effort to produce that kind of data in several areas by searching for ways to automate the process as much as possible such as through semi-supervised learning or synthesization.
- 数据集在许多研究领域进展方面发挥了关键作用，提供了真实的(ground truth)问题特例。它们允许通过提供有关其能力与局限的核心信息，数据集还可以对方法进行量化评估。<p>
- 特别地，这些数据集中的几个比如Geiger（2012b）;Scharstein＆Szeliski（2002）; Baker（2011）;Everinghamet al（2010）; Cordts（2016）也提供在线评估服务器允许在延期测试（held-out）中进行公平的比较，而且为该领域的研究人员提供更新的目前最好的算法。<p>
- 这种方式可以让研究团队很容易地确定目前的进展和剩下的挑战。在自主车辆的环境中，KITTI数据集Geiger（2012b）和Cityscapes数据集Cordts （2016）为重建、运动估计和识别任务引入了挑战性的基准，因此缩小了实验室设置与挑战现实世界的情况之间的差距。<p>
- 几年前，有数百个注释例子的数据集对于解决很多问题是足够的。然而，有数百到数千个有标签的例子的数据集的引入，通过以监督的方式训练大容量深度模型，已经使得许多计算机视觉学科的重大突破。<p>
- 然而，收集大量的注释数据不是一个容易的事情，特别是对于诸如光流或者语义分割的任务。这使得集体努力通过搜索尽可能多的方式来自动化过程，例如通过半监督学习或合成，从而在多个领域产生了这种数据。<p>

- 2.1. Real-World Datasets 真实数据集
- While several algorithmic aspects can be inspected using synthetic data, real-world datasets are necessary to guarantee performance of algorithms in real situations. For example, algorithms employed in practice need to handle complex objects and environments while facing challenging environmental conditions such as direct lighting, reflections from specular surfaces, fog or rain. The acquisition of ground truth is often labor intensive because very often this kind of information cannot be directly obtained with a sensor but requires tedious manual annotation. |For example, (Scharstein & Szeliski (2002),Baker et al. (2011)) acquire dense pixel-level annotations in a controlled lab environment whereas Geiger et al. (2012b); Kondermann et al. (2016) provide sparse pixel-level annotations of real street scenes using a LiDAR laser scanner.
- Recently, crowdsourcing with Amazon’s Mechanical Turk9 have become very popular to create annotations for large scale datasets, e.g., Deng et al. (2009); Lin et al. (2014); Leal-Taix´e et al. (2015); Milan et al. (2016). However, the annotation quality obtained via Mechanical Turk is often not sufficient to be considered as reference and significant efforts in post-processing and cleaning-up the obtained labels is typically required. |In the following, we will first discuss the most popular computer vision datasets and benchmarks addressing tasks relevant to autonomous vision. Thereafter, we will focus on datasets particularly dedicated to autonomous vehicle applications.
- 虽然可以使用合成数据检查几个算法方面，但实际数据集对于确保算法在实际情况下的性能是必要的。例如，在实践中使用的算法需要处理复杂的对象和环境，同时面对挑战性的环境条件，例如直接照明，镜面反射，雾或雨。获取ground truth通常是劳动密集型的，因为这种信息通常不能用传感器直接获得，而是需要繁琐的手动注释。<p>
- 例如，（Scharstein＆Szeliski（2002），Baker（2011））在受控实验室环境中获得了密集的像素级注释，而Geiger等人（2012B）; Kondermann等人（2016）使用LiDAR激光扫描仪提供实际街景场景的稀疏像素级注解。<p>
- 最近，亚马逊的Mechanical Turk的众包已经变得非常受欢迎，为大型数据集创建注释，例如Deng（2009）;Lin（2014）; Leal-Taix'e（2015）; Milan（2016）。然而，通过Mechanical Turk获得的注释质量通常不太合适被认为是参考，并且通常需要在后处理中最初的重大努力和清理所获得的标签中也是非常需要的。<p>
- 在下文中，我们将首先讨论最流行的计算机视觉数据集和基准，以解决与自主视觉相关的任务。此后，我们将专注于数据集，尤其致力于自动驾驶车辆的应用。

- **Stereo and 3D Reconstruction**: The Middlebury stereo benchmark introduced by Scharstein & Szeliski (2002) provides several multi-frame stereo data sets for comparing the performance of stereo matching algorithms. |Pixel-level ground truth is obtained by hand labeling and reconstructing planar components in piecewise planar scenes. Scharstein & Szeliski (2002) further provide a taxonomy of stereo algorithms that allows the comparison of design decisions and a test bed for quantitative evaluation. |Approaches submitted to their benchmark website are evaluated using the root mean squared error and the percentage of bad pixels between the estimated and ground truth disparity maps.
- Scharstein & Szeliski (2003) and Scharstein et al. (2014) introduced novel datasets to the Middlebury benchmark comprising more complex scenes and including ordinary objects like chairs, tables and plants. In both works a structured lighting system was used to create ground truth. |For the latest version Middlebury v3, Scharstein et al. (2014) generate highly accurate ground truth for high-resolution stereo images with a novel technique for 2D subpixel correspondence search and self-calibration of cameras as well as projectors. This new version achieves significantly higher disparity and rectification accuracy than those of existing datasets and allows a more precise evaluation. An example depth map from the dataset is illustrated in Figure 1.
- The Middlebury multi-view stereo (MVS) benchmark11 by Seitz et al. (2006) is a calibrated multi-view image dataset with registered ground truth 3D models for the comparison of MVS approaches. The benchmark played a key role in the advances of MVS approaches but is relatively small in size with only two scenes. |In contrast, the TUD MVS dataset12 by Jensen et al. (2014) provides 124 different scenes that were also recorded in controlled laboratory environment. Reference data is obtained by combining structured light scans from each camera position and the resulting scans are very dense, each containing 13.4 million points on average. For 44 scenes the full 360 degree model was obtained by rotation and scanning four times with 90 degree intervals. In contrast to the datasets so far, Sch¨ops et al. (2017) provide scenes that are not carefully staged in a controlled laboratory environment and thus represent real world challenges. Sch¨ops et al. (2017) recorded high-resolution DSLR imagery as well as synchronized low-resolution stereo videos in a variety of indoor and outdoor scenes. A high-precision laser scanner allows to register all images with a robust method. The high-resolution images enable the evaluation of detailed 3D reconstruction while the low-resolution stereo images are provided to compare approaches for mobile devices.
- 立体与 3D 重建类数据集：由Scharstein＆Szeliski（2002）引入的Middlebury立体声基准测试仪提供了多个立体声数据集，用于比较立体匹配算法的性能。<p>
- 通过在分段平面场景中手工标记和重建平面构成获得像素级地面真值。Scharstein和Szeliski（2002）进一步提供立体声算法的分类法，允许通过比较设计决策和测试台来进行定量评估。<p>
- 使用均方误差以及估计值和地面真实视差图之间坏像素的百分比来评估提交给其基准网站的方法。<p>
- Scharstein & Szeliski (2003) 和 Scharstein et al. (2014)为Middlebury基准引入了一种新颖的数据集，这个数据及包含更多复杂的场景和普通的物体，比如椅子、桌子、植物等对象。在这两个工作中，均使用一个结构化的照明系统来创造地面实况。<p>
- 对于最新版本的Middlebury v3，Scharstein（2014）采用新颖的2D子像素对应搜索和相机自动校准技术以及投影机为高分辨率立体图像生成高精度的地面实况。与现有数据集相比，该新版本的差异和整改精度明显提高，可以进行更精确的评估。 Figure 1是来自数据集的示例深度图：
Seitz等人的Middlebury多视点立体声（MVS）基准测试（2006）是注册地面真相3D模型用于比较MVS方法一种校准的多视图图像数据集。基准测试在MVS方法的进步中发挥了关键作用，但只有两个场景，尺寸相对较小。相比之下，Jensen等人的TUD MVS数据集（2014年）提供了124个不同的场景，这些场景也被记录在受控实验室环境中。 参考数据通过组合来自每个摄像机位置的结构光扫描获得，并且所得到的扫描非常密集，平均每个包含13.4million个点。对于44个场景，通过以90度的间隔旋转和扫描四次获得完整的360度模型。 与迄今为止的数据集相比，Sch¨ops等人（2017年）提供了在受控实验室环境中未仔细分级的场景，从而代表了现实世界的挑战。Sch¨ops et al. (2017) 录制了高分辨率DSLR单反相机图像以及各种室内和室外场景中同步的低分辨率立体视频。 高精度激光扫描仪允许以强大的方法注册所有图像。高分辨率图像可以评估详细的3D重建，同时提供低分辨率立体图像来比较移动设备的方法。

- **Optical Flow**: The Middlebury flow benchmark13 by Baker et al. (2011) provides sequences with non-rigid motion, synthetic sequences and a subset of the Middlebury stereo benchmark sequences (static scenes) for the evaluation of optical flow methods. For all non-rigid sequences, ground truth flow is obtained by tracking hidden fluorescent textures sprayed onto the objects using a toothbrush. The dataset comprises eight different sequences with eight frames each. Ground truth is provided for one pair of frames per sequence.
- Besides the limited size, real world challenges like complex structures, lighting variation and shadows are missing as the dataset necessitates laboratory conditions which allow for manipulating the light source between individual captures. In addition, it only comprises very small motions of up to twelve pixels which do not admit the investigation of challenges provided by fast motions. Compared to other datasets, however, the Middlebury dataset allows to evaluate sub-pixel precision since it provides very accurate and dense ground truth. Performance is measured using the angular error (AEE) and the absolute end point error (EPE) between the estimated flow and the ground truth.
- Janai et al. (2017) present a novel optical flow dataset comprising of complex real world scenes in contrast to the laboratory setting in Middlebury. High-speed video cameras are used to create accurate reference data by tracking pixel through densely sampled space-time volumes. This method allows to acquire optical flow ground truth in challenging everyday scenes in an automatic fashion and to augment realistic effects such as motion blur to compare methods in varying conditions. Janai et al. (2017) provide 160 diverse real-world sequences of dynamic scenes with a significantly larger resolution (1280X1024 Pixels) than previous optical datasets and compare several state of-the-art optical techniques on this data.
光流类数据集：Baker等人的“Middlebury流量标准” （2011）提供了具有非刚性运动序列，合成序列和Middlebury立体声基准序列（静态场景）的子集的序列，用于评估光流方法。 对于所有非刚性序列，通过使用toothbrush牙刷追踪在物体上喷洒的隐藏的荧光纹理来获得地面真实流。 数据集包含八个不同的序列，每个序列具有八个帧。 每个序列提供一对帧的地面实况。
除了有限的大小之外，由于数据集需要实验室条件，允许在各个捕获之间操纵光源，所以缺少像复杂结构，照明变化和阴影这样的真实世界挑战。 此外，它只包含最多十二个像素的非常小的运动，不承认对快速运动提供的挑战的调查。 然而，与其他数据集相比，Middlebury数据集可以评估子像素精度，因为它提供了非常精确和密集的地面实例。 使用角度误差（AEE）和估计流量与地面实数之间的绝对终点误差（EPE）来测量性能。
Janai等人 （2017）提出了一个新颖的光流数据集，其中包括复杂的现实世界场景，与Middlebury的实验室设置相反。 高速视频摄像机用于通过密集采样的时空容量跟踪像素来创建精确的参考数据。 该方法允许以自动方式在挑战性的日常场景中获取光流场地真相，并且增加诸如运动模糊的现实效果以在不同条件下比较方法。  Janai等人 （2017年）提供了160个不同的现实世界动态场景序列，具有比以前的光学数据集显着更大的分辨率（1280x1024像素），并比较了这些数据的几种最先进的光学技术。

- **Object Recognition and Segmentation**: The availability of large-scale, publicly available datasets such as ImageNet (Denget al. (2009)), PASCAL VOC (Everingham et al. (2010)), Microsoft COCO (Lin et al.(2014)), Cityscapes (Cordts et al.(2016)) and TorontoCity (Wang et al. (2016)) have had a major impact on the success of deep learning in object classification, detection, and semantic segmentation tasks.
- The PASCAL Visual Object Classes (VOC) challenge14 by Everingham et al. (2010) is a benchmark for object classification, object detection, object segmentation and action recognition. It consists of challenging consumer photographs collected from Flickr with high quality annotations and contains large variability in pose, illumination and occlusion. Since its introduction, the VOC challenge has been very popular and was yearly updated and adapted to the needs of the community until the end of the program in 2012. Whereas the first challenge in 2005 had only 4 dierent classes, 20 dierent object classes
were introduced in 2007. Over the years, the benchmark grew in size reaching a total of 11,530 images with 27,450 ROI annotated objects in 2012.
- In 2014, Lin et al. (2014) introduced the Microsoft COCO dataset15 for the object detection, instance segmentation and contextual reasoning. They provide images of complex everyday scenes containing common objects in their natural context. The dataset comprises 91 object classes, 2.5 million annotated instances and 328k images in total. Microsoft COCO is significantly larger in the number of instances per class than the PASCAL VOC object segmentation benchmark. All objects are annotated with per-instance segmentations in an extensive crowd worker eort. Similar to PASCAL VOC, the intersection-overunion metric is used for evaluation.
对象识别与分割类数据集
大量的公开数据集，如ImageNet（Deng等人（2009）），PASCAL VOC（Everingham等（2010）），Microsoft COCO（Lin等人（2014）），Cityscapes（Cordts （2016））和TorontoCity（Wang等人（2016年））对物体分类，目标检测和语义分割任务中深入学习的成功产生了重大影响。
由Everingham等人（2010）提供的PASCAL视觉对象类（VOC）挑战是对象分类，物体检测，物体分割和动作识别的基准。它由具有高质量标注的Flickr收集的有挑战性的消费者照片组成，并且包含姿势，照明和遮挡的大变化。 自从介绍以来，VOC的挑战一直很受欢并且逐年更新并适应社区的需求直到2012年计划结束。而2005年的第一个挑战只有4个不同的类，2007年引入了20个不同的对象类。多年来，基准规模在2012年达到总共11,530张图像当中共有27,450张ROI注释物体。
2014年，Lin等 （2014）介绍了Microsoft COCO数据集，用于物体检测，实例分割和上下文推理。 它们在自然环境中提供包含常见对象的复杂日常场景的图像。 数据集总共包括91个对象类，250万个注释实例和328k个图像。 Microsoft COCO在PASCAL VOC对象分割基准测试中每个类的实例数显著增加。 所有物体都在广泛的人群工作人员的努力下对每个实例进行标注。 与PASCAL VOC类似，IOU度量用于评估。

- **Tracking**: Leal-Taix´e et al. (2015); Milan et al. (2016) present the MOTChallenge16 which addresses the lack of a centralized benchmark for multi object tracking. The benchmark contains 14 challenging video sequences in unconstrained environments filmed with static and moving cameras and subsumes many existing multi-object tracking benchmarks such as PETS (Ferryman & Shahrokni (2009)) and KITTI (Geiger et al.(2012b)). The annotations for three object classes are provided: moving or standing pedestrians, people that are not in an upright position and others. They use the two popular tracking measures, Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) introduced by Stiefelhagen et al. (2007) for the evaluation of the approaches. Detection ground truth provided by the authors allows to analyze the performance of tracking systems independent of a detection system. Methods using a detector and methods using the detection ground truth can be compared separately on their website. - Aerial Image Datasets: The ISPRS benchmark17 (Rottensteiner et al. (2013, 2014)) provides data acquired by airborne sensors for urban object detection and 3D building reconstruction and segmentation. It consists of two datasets: Vaihingen and Downtown Toronto. The object classes considered in the object detection task are building, road, tree, ground, and car. The Vaihingen dataset provides three areas with various object classes and a large test site for road detection algorithms. The Downtown Toronto dataset covers an area of about 1.45 km2 in the central area of Toronto, Canada. Similarly to Vaihingen, there are two smaller areas for object extraction and building reconstruction, as well as one large area for road detection. For each test area, aerial images with orientation parameters, digital surface model (DSM), orthophoto mosaic and airborne laser scans are provided. The quality of the approaches is assessed using several metrics for detection and reconstruction. In both cases completeness, correctness and quality is assessed on a per-area level and a per-object level.
追踪：Leal-Taix'e（2015），Milan（2016）提出了MOTChallenge16，解决了多对象跟踪缺乏集中的基准。该基准测试包含14个具有静态和移动摄像机拍摄的无约束环境的挑战性视频序列，并包含许多现有的多对象跟踪基准，如PETS（Ferryman＆Shahrokni（2009））和KITTI（Geiger等（2012b））。提供三个对象类的注释：移动或站立的行人，不在直立位置的人等。他们使用Stiefelhagen等人介绍的两个流行的跟踪措施，多目标跟踪精度（MOTA）和多对象跟踪精度（MOTP）。 （2007）评估方法。作者提供的检测基准真实性可以分析独立于检测系统的跟踪系统的性能。使用检测器的方法和使用检测基准的方法可以在其网站上单独进行比较。 - 空中图像数据集：ISPRS benchmark17（Rottensteiner等（2013，2014））提供了用于城市物体检测和3D建筑重建和分割的机载传感器获取的数据。它包括两个数据集：Vaihingen和多伦多市区。对象检测任务中考虑的对象类是建筑，道路，树木，地面和汽车。 Vaihingen数据集提供了三个不同对象类别的区域和一个用于道路检测算法的大型测试站点。多伦多市中心数据集在加拿大多伦多的中部地区面积约1.45平方公里。与Vaihingen类似，有两个较小的对象提取和建筑重建区域，以及一个大面积的道路检测。对于每个测试区域，提供具有取向参数，数字表面模型（DSM），正射影像马赛克和机载激光扫描的航空图像。使用检测和重建的几个度量来评估方法的质量。在这两种情况下，完整性，正确性和质量都在每个面积水平和每个物体水平上进行评估。

- **Autonomous Driving**: In 2012, Geiger et al. (2012b, 2013) have introduced the KITTI Vision Benchmark18 for stereo, optical flow, visual odometry/SLAM and 3D object detection (Figure). The dataset has been captured from an autonomous driving platform and comprises six hours of recordings using high-resolution color and grayscale stereo cameras, a Velodyne 3D laser scanner and high-precision GPS/IMU inertial navigation system. The stereo and optical flow benchmarks derived from this dataset comprise 194 training and 195 test image pairs at a resolution of 1280  376 pixels and sparse ground truth obtained by projecting accumulated 3D laser point clouds onto the image. Due to the limitations of the rotating laser scanner used as reference sensor, the stereo and optical flow benchmark is restricted to static scenes with camera motion.
- To provide ground truth motion fields for dynamic scenes, Menze & Geiger (2015) have annotated 400 dynamic scenes, fitting accurate 3D CAD models to all vehicles in motion in to order to obtain flow and stereo ground truth for these objects. The KITTI flow and stereo benchmarks use the percentage of erroneous (bad) pixels to assess the performance of the submitted methods. Additionally, Menze & Geiger (2015) combined the stereo and flow ground truth to form a novel 3D scene flow benchmark. For evaluating scene flow, they combine classical stereo and optical flow measures.
- The visual odometry / SLAM challenge consists of 22 stereo sequences, with a total length of 39.2 km. The ground truth pose is obtained using GPS/IMU localization unit which was fed with RTK correction signals. The translational and rotational error averaged over a particular trajectory length is considered for evaluation.
- For the KITTI object detection challenge, a special 3D labeling tool has been developed to annotate all 3D objects with 3D bounding boxes for 7481 training and 7518 test images. The benchmark for the object detection task was separated into a vehicle, pedestrian and cyclist detection tasks, allowing to focus the analysis on the most important problems in the context of autonomous vehicles. Following PASCAL VOC Everingham et al. (2010), the intersection-over-union (IOU) metric is used for evaluation. For an additional evaluation, this metric has been extended to capture both 2D detection and 3D orientation estimation performance. A true 3D evaluation is planned to be released shortly.
- The KITTI benchmark was extended by Fritsch et al. (2013) to the task of road/lane detection. In total, 600 diverse training and test images have been selected for manual annotation of road and lane areas. Mattyus et al. (2016) used aerial images to enhance the KITTI dataset with fine grained segmentation categories such as parking spots and sidewalk as well as the number and location of road lanes. The KITTI dataset has established itself as one of the standard benchmarks in all of the aforementioned tasks, in particular in the context of autonomous driving applications.
- 2012年，Geiger等（2012b，2013）推出了用于立体声，光流，视觉测距/ SLAM和3D物体检测的KITTI Vision Benchmark18（图）。数据集已从自主驾驶平台捕获，包括使用高分辨率彩色和灰度立体相机的六小时录音，Velodyne 3D激光扫描仪和高精度GPS / IMU惯性导航系统。从该数据集派生的立体声和光流基准测试包括194次训练和195个测试图像对，分辨率为1280？通过将累积的3D激光点云投影到图像上获得的376个像素和稀疏的地面真实。由于用作参考传感器的旋转激光扫描仪的局限性，立体声和光学流量基准仅限于具有摄像机运动的静态场景。
- 为了为动态场景提供地面真相运动场，Menze＆Geiger（2015）已​​经注明了400个动态场景，将精确的3D CAD模型适用于所有运动的车辆，以获得这些物体的流动和立体声地面实况。 KITTI流量和立体声基准使用错误（不良）像素的百分比来评估提交的方法的性能。此外，Menze＆Geiger（2015）结合了立体声和流动地面的真相，形成了一种新颖的3D场景流动基准。为了评估场景流，它们结合了古典立体声和光学流量测量。
- 视觉测距/ SLAM挑战包括22个立体声序列，总长39.2公里。使用馈送有RTK校正信号的GPS / IMU定位单元获得地面真实姿势。考虑在特定轨迹长度上平均的平移和旋转误差进行评估。
- 对于KITTI对象检测挑战，已经开发了一种特殊的3D标签工具，用于通过3D边界框注释所有3D对象，用于7481个训练和7518个测试图像。物体检测任务的基准被分为车辆，行人和骑车人员检测任务，允许将分析集中在自主车辆的上下文中最重要的问题。按照PASCAL VOC Everingham等（2010），交叉联合（IOU）度量用于评估。为了进一步评估，该指标已扩展到捕获2D检测和3D定向估计性能。计划即将发布真正的3D评估。
- 由Fritsch等人扩展了KITTI基准。 （2013年）到道路/车道检测任务。总共选择了600多种不同的训练和测试图像，用于手动注释道路和车道区域。 Mattyus等人（2016）使用航空图像来增强KITTI数据集，并提供诸如停车位和人行道之类的细粒度细分类别，以及道路的数​​量和位置。 KITTI数据集已经成为所有上述任务的标准基准之一，特别是在自主驾驶应用的上下文中。

- Complementary to other datasets, the HCI benchmark19 proposed in Kondermann et al. (2016) specifically includes realistic, systematically varied radiometric and geometric challenges. Overall, a total of 28,504 stereo pairs with stereo and flow ground truth is provided. In contrast to previous datasets, ground truth uncertainties have been estimated for all static regions. The uncertainty estimate is derived from pixel-wise error distributions for each frame which are computed based on Monte Carlo sampling. Dynamic regions are manually masked out and annotated with approximate ground truth for 3,500 image pairs.
- The major limitation of this dataset is that all sequences were recorded in a single street section, thus lacking diversity. On the other hand, this enabled better control over the content and environmental conditions. In contrast to the mobile laser scanning solution of KITTI, the static scene is scanned only once using a high-precision laser scanner in order to obtain a dense and highly accurate ground truth of all static parts. Besides the metrics used in KITTI and Middlebury, they use semantically meaningful performance metrics such as edge fattening and surface smoothness for evaluation Honauer et al. (2015). The HCI benchmark is rather new and not established yet but the controlled environment allows to simulate rarely occurring events such as accidents which are of great interest in the evaluation of autonomous driving systems.
- The Caltech Pedestrian Detection Benchmark20 proposed by Dollar et al. (2009) provides 250,000 frames of sequences recorded by a vehicle while driving through regular traffic in an urban environment. 350,000 bounding boxes and 2,300 unique pedestrians were annotated including temporal correspondence between bounding boxes and detailed occlusion labels. Methods are evaluated by plotting the miss rate against false positives and varying the threshold on detection confidence.
- The Cityscapes Dataset21 by Cordts et al. (2016) provides a benchmark and large-scale dataset for pixel-level and instancelevel semantic labeling that captures the complexity of realworld urban scenes. It consists of a large, diverse set of stereo video sequences recorded in streets of different cities. High quality pixel-level annotations are provided for 5,000 images while 20,000 additional images have been annotated with coarse labels obtained using a novel crowd sourcing platform. For two semantic granularities, i.e., classes and categories, they report mean performance scores and evaluate the intersection-overunion metric at instance-level to assess how well individual instances are represented in the labeling.
- The TorontoCity benchmark presented byWang et al. (2016) covers the greater Toronto area with 712 km2 of land, 8,439 km of road and around 400,000 buildings. The benchmark covers a large variety of tasks including building height estimation (reconstruction), road centerline and curb extraction, building instance segmentation, building contour extraction, semantic labeling and scene type classification. The dataset was captured from airplanes, drones, and cars driving around the city to provide different perspectives.
- 与其他数据集的补充，在Kondermann等人提出的HCI基准19。 （2016）具体包括现实的，有系统地变化的辐射和几何挑战。总的来说，共提供了28,504立体声和流动地面真相的立体声对。与以前的数据集相比，所有静态区域的地面真实不确定度已被估计。不确定性估计是根据基于蒙特卡洛取样计算的每个帧的像素误差分布得出的。手动屏蔽动态区域并用3,500个图像对的近似地面实例进行注释。
- 这个数据集的主要限制是所有序列记录在单个街区，因此缺乏多样性。另一方面，这能够更好地控制内容和环境条件。与KITTI的移动激光扫描解决方案相比，静态场景仅使用高精度激光扫描仪扫描一次，以获得所有静态部件的致密和高精度的地面实况。除了KITTI和Middlebury使用的指标之外，他们使用语义有意义的性能指标，如边缘育肥和表面平滑度评估Honauer等。 （2015年）。 HCI基准相当新，尚未建立，但受控环境允许模拟很少发生的事件，例如对自主驾驶系统的评估感兴趣的事故。
- 美国加州大学提出的Caltech行人检测基准20 （2009）提供了车辆记录的25万帧序列，同时在城市环境中经常进行交通。包括350,000个边界框和2,300个独特的行人，包括边界框和详细遮挡标签之间的时间对应关系。通过绘制误差率与误报率并在检测置信度上改变阈值来评估方法。
- 由Cordts等人的Cityscapes Dataset21 （2016）为像素级和实例级语义标注提供了基准和大型数据集，捕捉到现实城市场景的复杂性。它由不同城市的街道上记录的大型，多样化的立体视频序列组成。为5,000张图像提供了高质量的像素级注释，而使用新颖的人群采购平台获得的粗略标签已经注明了20,000张附加图像。对于两个语义粒度，即类别和类别，他们报告平均绩效评分，并评估实例级别的交叉点平均度量，以评估在标签中表示个体实例的程度。
- Wang等人提出的多伦多城市基准（2016年）覆盖多伦多地区，712平方公里的土地，8,439公里的道路和大约40万个建筑物。该基准涵盖了建筑高度估计（重建），道路中心线和路缘提取，建筑物实例分割，建筑轮廓提取，语义标注和场景类型分类等各种任务。数据集被从飞机，无人驾驶飞机和汽车驾驶在城市周围捕获，以提供不同的观点。

- **Long-Term Autonomy**: Several datasets such as KITTI or Cityscapes focus on the development of algorithmic competences for autonomous driving but do not address challenges of long-term autonomy, as for examples environmental changes over time. To address this problem, a novel dataset for autonomous driving has been presented by Maddern et al. (2016). They collected images, LiDAR and GPS data while traversing 1,000 km in central Oxford in the UK during one year. This allowed them to capture large variations in scene appearance due to illumination, weather and seasonal changes, dynamic objects, and constructions. Such long-term datasets allow for in-depth investigation of problems that detain the realization of autonomous vehicles such as localization in dierent times of the year.
长期自动：几个数据集，如KITTI或Cityscapes，着重于开发自主驾驶的算法能力，但不能解决长期自主的挑战，例如随着时间的推移环境变化。 为了解决这个问题，Maddern等人提出了一个用于自主驾驶的新型数据集。（2016）。 他们在一年内在英国牛津中心穿过1000公里的地方收集图像，LiDAR和GPS数据。 这允许他们捕获由于照明，天气和季节变化，动态对象和结构而导致的场景外观的大变化。 这些长期数据集允许深入调查在一年中的不同时期扣留自主车辆的实现问题，例如本地化。


- 2.2. Synthetic Data
- The generation of ground truth for real examples is very labor intensive and often not even possible at large scale when pixel-level annotations are required. On the other hand, pixel-level ground truth for large-scale synthetic datasets can be easily acquired. However, the creation of realistic virtual world is time-consuming. The popularity of movies and video games have led to an industry creating very realistic 3D content which nourishes the hope to replace real data completely using synthetic datasets. Consequently, several synthetic datasets have been proposed, recently, but it remains an open question whether the realism and variety attained is sufficient to replace real world datasets. Besides, creating realistic virtual content is a time consuming and expensive process itself and the trade-off between real and synthetic (or augmented) data is not clear yet.
为真实的例子生成地面真相是非常劳动密集型的，并且在需要像素级注释时通常甚至不可能大规模地实现。 另一方面，可以轻松获取大规模合成数据集的像素级地面实况。 然而，创造现实的虚拟世界是耗时的。 电影和视频游戏的普及导致了行业创造了非常逼真的3D内容，这些内容丰富了使用合成数据集完全替代实际数据的希望。 因此，最近已经提出了几个合成数据集，但是现实主义和品种是否足以替代现实世界数据集仍然是一个悬而未决的问题。 此外，创建逼真的虚拟内容是一个耗时且昂贵的过程本身，真实和合成（或增强）数据之间的权衡尚不清楚。

- **MPI Sintel**: The MPI Sintel Flow benchmark22 presented by Butler et al. (2012) takes advantage of the open source movie Sintel, a short animated film, to render scenes of varying complexity with optical flow ground truth. In total, Sintel comprises 1,628 frames. Different datasets obtained using different passes of the rendering pipeline vary in complexity shown in Figure 3. The albedo pass has roughly piecewise constant colors without illumination effects while the clean pass introduces illumination of various kinds. The final pass adds atmospheric effects, blur, color correction and vignetting. In addition to the average endpoint error, the benchmark website provides different rankings of the methods based on speed, occlusion boundaries, and disocclusions.
- **Flying Chairs and Flying Things**: The limited size of optical flow datasets hampered the training of deep high-capacity models. To train a convolutional neural network, Dosovitskiy et al.(2015) thus introduced a simple synthetic 2D dataset of flying chairs rendered on top of random background images from Flickr. As the limited realism and size of this dataset proved insufficient to learn highly accurate models, Mayer et al. (2016) presented another large-scale dataset consisting of three synthetic stereo video datasets: FlyingThings3D, Monkaa, Driving. FlyingThings3D provides everyday 3D objects flying along randomized 3D trajectories in a randomly created scene. Inspired by the KITTI dataset a driving dataset has been created which uses car models from the same pool as FlyingThings3D and additionally highly detailed tree and building models from 3D Warehouse. Monkaa is an animated short movie similar to Sintel used in the MPI Sintel benchmark.
- **Game Engines**: Unfortunately, data from animated movies is very limited since the content is hard to change and such movies are rarely open source. In contrast, game engines allow for creating an infinite amount of data. One way to create virtual worlds using a game engine is presented by Gaidon et al. (2016) which introduces the Virtual KITTI dataset23. They present an efficient real-to-virtual world cloning method to create realistic proxy worlds. A cloned virtual world allows to vary conditions such as weather or illumination and to use different camera settings. This way, the proxy world can be used for virtual data augmentation to train deep networks. Virtual KITTI contains 35 photo-realistic synthetic videos with a total of 17,000 high resolution frames. They provide ground truth for object detection, tracking, scene and instance segmentation, depth and optical flow.
MPI Sintel ：由Butler等人提出的MPI Sintel Flow benchmark22 （2012）利用开源电影Sintel（短片动画），以光流地面的真相呈现不同复杂度的场景。总共有Sintel包括1,628帧。使用不同渲染流程获得的不同数据集的复杂度如图3所示。反照率传递具有大致分段恒定颜色，无照明效果，而清洁通道则引入各种照明。最后的通行证增加了大气效果，模糊，颜色校正和渐晕。除了平均终点误差之外，基准网站还提供了基于速度，遮挡边界和不相关的方法的不同排名。
- 飞行椅和飞行事物：光流数据集的数量有限，妨碍了深层大容量模型的训练。为了训练卷积神经网络，Dosovitskiy等人（2015）引入了一个简单的合成2D数据集，它们呈现在Flickr的随机背景图像之上。由于该数据集的有限现实性和大小证明不足以学习高精度模型，Mayer等（2016）提出了另外一个由三个合成立体视频数据集组成的大型数据集：FlyingThings3D，Monkaa，Driving。 FlyingThings3D在随机创建的场景中提供随机3D轨迹飞行的每天3D对象。受KITTI数据集的启发，已经创建了一个驱动数据集，它使用与FlyingThings3D相同的池中的汽车模型，以及来自3D Warehouse的另外高度详细的树和建筑模型。 Monkaa是一个类似于Sintel的动画短片，用于MPI Sintel基准测试。
- 游戏引擎：不幸的是，动画电影的数据非常有限，因为内容很难改变，这样的电影很少是开源的。相比之下，游戏引擎允许创建无限量的数据。 Gaidon等人提出了使用游戏引擎创建虚拟世界的一种方式。 （2016）介绍了虚拟KITTI数据集23。他们提出了一种高效的实时虚拟世界克隆方法来创建现实的代理世界。克隆的虚拟世界允许改变诸如天气或照明的条件，并使用不同的相机设置。这样，代理世界可以用于虚拟数据扩充来训练深层网络。虚拟KITTI包含35张照片合成视频，总共17,000个高分辨率帧。它们为物体检测，跟踪，场景和实例分割，深度和光流提供了基础。

- In concurrent work, Ros et al. (2016) created SYNTHIA24, a synthetic collection of Imagery and Annotations of urban scenarios for semantic segmentation. They rendered a virtual city with the Unity Engine. The dataset consists of 13,400 randomly taken virtual images from the city and four video sequences with 200,000 frames in total. Pixel-level semantic annotations are provided for 13 classes.
- Richter et al. (2016) have extracted pixel-accurate semantic label maps for images from the commercial video game Grand Theft Auto V. Towards this goal, they developed a wrapper which operates between the game and the graphics hardware to obtain pixel-accurate object signatures across time and instances. The wrapper allows them to produce dense semantic annotations for 25 thousand images synthesized by the photorealistic open-world computer game with minimal human supervision. However, for legal reasons, the extracted 3D geometry can not be made publicly available. Similarly, Qiu & Yuille (2016) provide an open-source tool to create virtual worlds by accessing and modifying the internal data structure of Unreal Engine 4. They show how virtual worlds can be used to test deep learning algorithms by linking them with the deep learning framework Caffe Jia et al. (2014).
- 在并行工作中，Ros et al。 （2016）创建了SYNTHIA24，一种用于语义分割的城市场景图像和注释的综合集合。他们用Unity Engine渲染了一个虚拟的城市。该数据集由13,400个随机抽取的城市虚拟图像和四个视频序列组成，共20万帧。为13个类提供像素级语义注释。
- Richter et al。 （2016）已经为商业视频游戏“侠盗猎车手”V提取了图像的像素精确语义标签贴图。为了实现这一目标，他们开发了一种在游戏和图形硬件之间运行的包装器，以便跨越时间获得像素精确的对象签名，实例。包装器允许他们通过最小的人力监督来生成由真实感的开放世界电脑游戏合成的2.5万张图像的密集语义注释。然而，出于法律原因，提取的3D几何不能公开获得。同样，Qiu&Yuille（2016）通过访问和修改虚幻引擎4的内部数据结构，提供了一个开源工具来创建虚拟世界。他们展示了虚拟世界如何通过将深度学习算法与深层次学习框架Caffe Jia（2014）。

### 3. Cameras Models & Calibration 摄像头模块&校准
- 3.1. Calibration 校准
- Multiple sensors including odometry, range sensors, and different types of cameras such as perspective and fish-eye are widely used in automotive context. Calibration is the problem of estimating intrinsic and extrinsic parameters of these sensors to relate 2D image points to 3D world points and represent sensed information in a common coordinate system in case of multiple sensors. Fiducial markings on checkerboard patterns are the standard tool for calibration. Almost all systems use them either for initialization or for joint optimization to improve the intrinsics. Reprojection error which is the pixel distance between a projected point and a measured one, is used as a way of measuring accuracy quantitatively. Accuracy of calibration is a key issue in driver assistance applications requiring 3D reasoning, and consequently in the safety of autonomous vehicles. Besides accuracy, other desired qualities in a calibration system are speed, robustness to varying imaging conditions, full automation, minimum restrictions in terms of assumptions such as overlapping field of view or information required such as an initial guess of the parameters.
- Modern systems are equipped with multiple sensors for different purposes. Geiger et al. (2012c) use a setup involving two cameras and a single range sensor such as Kinect or Velodyne laser scanner. They present two algorithms for camera-tocamera and camera-to-range calibration using a single image per sensor. They assume a common field of view for the sensors which is particularly useful for applications such as generating stereo or scene flow ground truth. Heng et al. (2013) and Heng et al. (2015) tackle the automatic intrinsic and extrinsic calibration of a multi-camera rig system with four fish-eye cameras and odometry without assuming overlapping fields of view. Heng et al. (2015) propose an improved version of Heng et al. (2013). While Geiger et al. (2012c) require fiducial markings to re-calibrate the system before every run, they remove the requirement to modify infrastructure by using a map and natural features instead. They first build a map of the calibration area and then perform calibration by using this map and image-based geo-localization. In contrast to SLAM-based selfcalibration methods, image-based localization removes the burden of exhaustive feature matching between dierent cameras and bundle adjustment.
多个传感器，包括测距仪，量程传感器以及不同类型的摄像机，如透视和鱼眼，广泛应用于汽车领域。校准是估计这些传感器的内在和外在参数以将2D图像点与3D世界点相关并且在多个传感器的情况下在公共坐标系中表示感测信息的问题。棋盘图案上的基准标记是校准的标准工具。几乎所有系统都使用它们进行初始化或联合优化来改进内在函数。作为投影点和测量点之间的像素距离的重新投影误差被用作定量测量精度的一种方式。校准的准确性是驾驶员辅助应用中的关键问题，需要3D推理，因此在自主车辆的安全性方面。除了准确性之外，校准系统中的其他期望的质量是对变化的成像条件的速度，鲁棒性，全自动化，在诸如重叠视野或所需信息（诸如参数的初始猜测）的假设方面的最小限制。
- 现代系统配备多个传感器用于不同的目的。盖革等人（2012c）使用一个包含两个摄像头和单范围传感器（如Kinect或Velodyne激光扫描仪）的设置。它们使用每个传感器单个图像呈现两种相机摄像机和摄像机到距离校准的算法。它们假设传感器的共同视野，这对于诸如产生立体声或场景流场实况的应用特别有用。 Heng等（2013）和Heng等（2015）利用四个鱼眼摄像机和距离测量法来处理多摄像机钻机系统的自动内在和外在校准，而不考虑重叠的视野。 Heng等（2015）提出了Heng等人的改进版。 （2013年）。而Geiger等人（2012c）要求在每次运行之前重新校准系统的基准标记，它们通过使用地图和自然特征来消除修改基础设施的要求。他们首先构建校准区域的图，然后使用该地图和基于图像的地理定位进行校准。与基于SLAM的自动校准方法相比，基于图像的定位消除了不同相机与捆绑调整之间的穷举特征匹配的负担。

- 3.2. Omnidirectional Cameras
- A panoramic field of view is desirable in autonomous driving to gain maximum information about the surrounding area for safe navigation. An omnidirectional camera with a 360-degree field of view provides enhanced coverage by eliminating the need for more cameras or mechanically turnable cameras. There are dierent types of omnidirectional cameras with a visual field that covers a hemisphere or even approximately the entire sphere. Catadioptric cameras combine a standard camera with a shaped mirror, such as a parabolic, hyperbolic, or elliptical mirror while dioptric cameras use purely dioptric fisheye
lenses. Polydioptric cameras use multiple cameras with overlapping field of view to provide a full spherical field of view.
- One classification often used in the literature for omnidirectional cameras is based on the projection center: central and noncentral. In central cameras, the optical rays to the viewed objects intersect in a single point in 3D which is known as the single effective viewpoint property. This property allows the generation of geometrically correct perspective images from the images captured by omnidirectional cameras and consequently, application of epipolar geometry which holds for any central camera. Central catadioptric cameras are built by choosing the mirror shape and the distance between the camera and the mirror.
在自主驾驶中需要全景视野以获得关于周围区域的最大信息以用于安全导航。具有360度视野的全向摄像机可以通过消除对更多摄像机或机械可转换摄像机的需求而提供更高的覆盖范围。有不同类型的全方位摄像机，其视野覆盖半球或甚至大致整个球体。反折射相机将标准相机与成型镜相结合，例如抛物线，双曲线或椭圆镜，而折光相机使用纯粹的折射鱼眼
镜头。多折照相机使用多个具有重叠视野的相机，以提供完整的球面视野。
- 文献中常用于全向摄像机的一个分类是基于投影中心：中央和非中央。在中央相机中，到所观看的物体的光线在3D中的单个点相交，这被称为单个有效视点属性。该属性允许从由全向照相机拍摄的图像产生几何正确的透视图像，并因此产生适用于任何中央相机的对极几何形状。通过选择镜面形状和相机与镜子之间的距离来构建中央反折射相机。

- In contrast to pinhole cameras, calibration of omnidirectional cameras cannot be modeled by a linear projection due to very high distortion. The model should take into account the reflection of the mirror in the case of a catadioptric camera or the refraction caused by the lens in the case of a fisheye camera. Geyer & Daniilidis (2000) provide a unifying theory for all central catadioptric systems which is known as unified projection model in the literature and widely used by different calibration toolboxes (Mei & Rives (2007); Heng et al. (2013, 2015)). They prove that every projection, both standard perspective and
catadioptric using a hyperbolic, parabolic, or elliptical mirror, can be modeled with projective mappings from the sphere to a plane where projection center is on a sphere diameter and the plane perpendicular to it. Scaramuzza & Martinelli (2006) propose modeling the imaging function by a Taylor series expansion whose degree and the coecients are the parameters to be estimated. Polynomials of order three or four are able to model accurately all catadioptric cameras and many types of fisheye cameras. Mei & Rives (2007) improve upon unified projection model of Geyer & Daniilidis (2000) to account for real-world errors by modeling distortions with well identified parameters.
- As desirable as it is, the single viewpoint property is often violated in practice due to varifocal lenses and difficulty of precise alignment. However, non-central models as the alternative are computationally demanding, hence not suitable for real-time applications. Sch¨onbein et al. (2014) extend a noncentral approach in order to accurately obtain the viewing ray orientations, and then propose a fast central approximation with a mapping to match the obtained orientations. This kind of approach, tested on hypercatadioptric cameras, achieves a reprojection error lower than the central models (Geyer & Daniilidis(2000); Scaramuzza & Martinelli (2006); Mei & Rives (2007)) and comparable to non-central models while being much faster.
- 与针孔相机不同，由于非常高的失真，全向摄像机的校准不能由线性投影建模。在反折射相机的情况下，该模型应考虑到镜子的反射，或者在鱼眼相机的情况下应考虑镜头引起的折射。 Geyer＆Daniilidis（2000）为所有中心反射折射系统提供了统一的理论，在文献中被称为统一投影模型，并被不同的校准工具箱广泛使用（Mei＆Rives（2007）; Heng等（2013，2015）） 。他们证明了每一个投影，既有标准的视角又有
使用双曲线，抛物面或椭圆镜的反射折射可以用从球体到投影中心在球体直径和垂直于其的平面的平面进行投影映射来建模。 Scaramuzza＆Martinelli（2006）提出了通过泰勒级数展开来建模成像函数，其程度和系数是要估计的参数。三阶或四阶的多项式能够准确地模拟所有反射折射相机和许多类型的鱼眼相机。 Mei＆Rives（2007）改进了Geyer＆Daniilidis（2000）的统一投影模型，通过建立具有良好识别参数的失真模型来解释现实世界的错误。
- 根据需要，由于变焦透镜和精确对准的困难，实际上常常违反单一视点特性。然而，非中心模型作为替代方案在计算上要求苛刻，因此不适合实时应用。 Sch¨onbein等（2014）扩展了非中心的方法，以便准确地获得观察射线取向，然后提出一个具有映射的快速中心近似以匹配获得的方向。这种在过度反射摄影机上进行测试的方法实现了比中央模型（Geyer＆Daniilidis（2000）; Scaramuzza＆Martinelli（2006）; Mei＆Rives（2007））更低的重现误差，并且与非中心模型相当，快多了。

- **Applications**: Omnidirectional cameras are more and more used in autonomous driving. For feature based applications such as navigation, motion estimation and mapping, large field of view enables extraction and matching of interesting points from all around the car. For instance, omnidirectional feature matches improve the rotation estimate significantly when doing visual odometry or simultaneous localization and mapping(SLAM). Scaramuzza&Siegwart (2008) estimate the ego-motion of the vehicle relative to the road from a single, central omnidirectional camera by using a homography based tracker for the ground plane and an appearance-based tracker for the rotation of the vehicle. 3D perception also benefits from the unified view offered by omnidirectional sensors, despite the limited effective resolution which leads to noisy reconstructions. Laserbased solutions as an alternative provide only sparse point clouds without color, are extremely expensive and suffer from rolling shutter effects. Sch¨onbein & Geiger (2014) propose a method for 3D reconstruction through joint optimization of disparity estimates from two temporally and two spatially adjacent omnidirectional view in a unified omnidirectional space by using plane-based priors. H¨ane et al. (2014) extend the planesweeping stereo matching for fisheye cameras by incorporating unified projection model for fisheye cameras directly into the plane-sweeping stereo matching algorithm. This kind of approach allows producing dense depth maps directly from fisheye images in real time using GPUs and opens the way for dense 3D reconstruction with large a field of view in real time.
- 应用：全自动摄像机越来越多地用于自主驾驶。对于基于功能的应用程序，如导航，运动估计和映射，大视野可以提取和匹配来自汽车周围的有趣点。例如，全方位特征匹配在进行视觉测距或同时定位和映射（SLAM）时会显着改善旋转估计。 Scaramuzza＆Siegwart（2008）通过使用用于地平面的基于单应性的跟踪器和用于车辆旋转的基于外观的跟踪器来估计来自单个中央全向照相机的车辆相对于道路的自主运动。尽管有限分辨率导致噪声重建，3D感知也受益于全向传感器提供的统一视图。作为替代的基于激光的解决方案仅提供没有颜色的稀疏云，是非常昂贵的并且受到滚动快门效应的影响。 Sch¨onbein＆Geiger（2014）提出了一种通过使用基于平面的先验在统一的全向空间中从两个时间和两个空间相邻的全向视差联合优化差异估计的三维重建方法。 H¨ane等（2014年）通过将鱼眼摄影机的统一投影模型直接纳入平扫立体匹配算法，扩展了鱼眼摄像机的扫描立体匹配。这种方法允许使用GPU实时直接从鱼眼图像生成密集的深度图，并且以实时的大视野打开密集3D重建的方式。

- 3.3. Event Cameras 活动相机
- Contrary to conventional frame-based imagers at constant frame rates, event-based sensors have very recently been introduced. They produce a stream of asynchronous events at microsecond resolution in case of a brightness change surpassing a pre-defined threshold (Dynamic Vision Sensor) as shown in Figure 4. An event contains the location, sign, and precise timestamp of the change. This kind of data is sparse in nature, thus reducing redundancy in transmission and processing. Another advantage is high temporal resolution, allowing the design of highly reactive systems. These properties, namely low latency and low bandwidth requirement make event-based sensors interesting for autonomous driving. However, standard computer-vision algorithms cannot be applied directly to the output of event-based vision sensors which is fundamentally different from intensity images. Events occur at high frequency and each event doesn’t carry enough information by itself. A straightforward solution is to generate intensity images by accumulating
events over a fixed time interval, but this kind of event to-frame conversion introduces some latency and obstructs the efficiency which comes with the high temporal resolution.
- Instead, algorithms should ideally exploit the high rate at which events are generated. Consequently several methods have recently been introduced which exploit the high temporal resolution and the asynchronous nature of the sensor for different problems in autonomous vision. The design goal of such algorithms is that each incoming event can asynchronously change the estimated state, thus respecting the event-based nature of the sensor and allowing for perception and state estimation in highly dynamic scenarios. For trajectory estimation, Mueggler et al. (2015b) propose a continuous temporal model as a natural representation of the pose trajectory described by a smooth parametric model. Rebecq et al. (2016) propose an event-based 3D reconstruction algorithm to produce a parallel tracking and mapping pipeline that runs in real-time on the CPU. Eventbased SLAM does not suffer from motion blur due to high speed motions and very high dynamic range scenes which can be challenging for standard camera approaches.
- Lifetime Estimation: In addition to enabling novel solutions for existing problems where low latency and high frame rates are required, event-based sensors also give rise to new problems. One such problem is lifetime estimation of events by modeling the set of active events. An event is considered active as long as the brightness gradient causing the event is visible by the pixel. Explicit modeling of active events can be used to generate sharp gradient images at any point in time, or for clustering of events in tracking of multiple objects. For this task, Mueggler et al. (2015a) propose using event-based optical flow with optional regularization, independent of a temporal window.
- 与传统的基于帧的成像器在恒定帧速率相反，基于事件的传感器最近已经被引入。在亮度变化超过预定阈值（动态视觉传感器）的情况下，它们以微秒分辨率产生异步事件流，如图4所示。事件包含更改的位置，符号和精确时间戳。这种数据本质上是稀疏的，从而减少传输和处理的冗余。另一个优点是高时间分辨率，允许设计高反应性系统。这些属性，即低延迟和低带宽需求使得基于事件的传感器对于自主驾驶是有意义的。然而，标准计算机视觉算法不能直接应用于与强度图像基本不同的基于事件的视觉传感器的输出。事件发生在高频率，每个事件本身不携带足够的信息。一个简单的解决方案是通过积累产生强度图像
事件在固定的时间间隔内，但这种事件帧间转换引入了一些延迟并阻碍了高时间分辨率带来的效率。
相反，算法应该理想地利用生成事件的高速率。因此，最近已经引入了几种方法，其利用自主视觉中的不同问题的传感器的高时间分辨率和异步性质。这种算法的设计目标是每个传入事件可以异步地改变估计状态，从而满足传感器的基于事件的性质，并允许在高度动态的情况下的感知和状态估计。对于轨迹估计，Mueggler et al。 （2015b）提出了一种连续时间模型作为由平滑参数模型描述的姿势轨迹的自然表示。 Rebecq等（2016）提出了一种基于事件的3D重建算法，以产生在CPU上实时运行的并行跟踪和映射管道。基于事件的SLAM由于高速运动和非常高的动态范围场景而不受运动模糊的影响，这对于标准相机方法来说可能是具有挑战性的。
- 寿命估算：除了为需要低延迟和高帧速率的现有问题提供新颖的解决方案外，基于事件的传感器也会引起新的问题。一个这样的问题是通过对一组活动事件建模来对事件进行寿命估计。只要导致事件的亮度梯度被像素看到，事件被认为是活动的。活动事件的显式建模可用于在任何时间点生成锐利梯度图像，或用于在跟踪多个对象时对事件进行聚类。对于这个任务，Mueggler等人（2015a）提出了使用基于事件的光流与可选正则化，独立于时间窗口。

### 4. Representations 表现
- A wide variety of representations at different levels of granularity is used in the computer vision literature. Variables or parameters can be associated directly with 2D pixels in an image or describe high-level primitives in 3D space. In pixel-based representation each pixel is a separate entity, for example a random variable in a graphical model. Pixels are amongst the most fine-grained representations, but are harder to relate to physical properties of our 3D world. Furthermore, pixel-based representations increase complexity of inference algorithms due to the large number of variables in high resolution images. As a consequence, many approaches model only local interactions between pixels which do not capture the structure of our world sufficiently well to overcome all ambiguities in the ill-posed inverse
problems computer vision is trying to solve.
- 在计算机视觉文献中使用了不同粒度级别的各种表示。 变量或参数可以直接与图像中的2D像素相关联，或者描述3D空间中的高级原语。 在基于像素的表示中，每个像素是一个单独的实体，例如图形模型中的随机变量。 像素是最细粒度的表示，但更难与3D世界的物理属性相关。 此外，由于高分辨率图像中的大量变量，基于像素的表示增加了推理算法的复杂性。 因此，许多方法仅模拟像素之间的局部相互作用，这些相互作用不能很好地捕获我们世界的结构，以克服不正确的逆向中的所有模棱两可
计算机视觉问题正在试图解决。

- **Superpixels**: Consequently, compact representations based on grouping of pixels, i.e. superpixels, have gained popularity. Superpixel-based representations are obtained by a segmentation of the image into atomic regions which are ideally similar in color and texture, and respect image boundaries (Ren & Malik (2003); Achanta et al. (2012); Li & Chen (2015)). The implicit assumption each superpixel-based method makes is that certain properties of interest remain constant within a superpixel, e.g., the semantic class label or the slant of a surface. However, boundary adherence with respect to these properties is easily violated, especially for cluttered images when relying on standard segmentation algorithms which leverage color or intensity cues.
- If available, depth information can be leveraged as valuable feature for accurate superpixel extraction (Badino et al.(2009); Yamaguchi et al. (2014)). Superpixels are used as building blocks for various tasks such as stereo and flow estimation (Yamaguchi et al. (2012, 2013, 2014); G¨uney & Geiger (2015); Bai et al. (2016)), scene flow (Menze & Geiger (2015); Menze et al. (2015b); Lv et al. (2016)), semantic segmentation (Xiao & Quan (2009); Wegner et al. (2013)), scene understanding (Ess et al. (2009b); Liu et al. (2014)) and 3D reconstruction (Sch¨onbein et al. (2014)). In cases that include geometric reasoning
such as stereo estimation, superpixels often represent 3D planar segments. When the goal is to represent real-world scenes with independent object motion as in scene flow or optical flow, superpixels can be generalized to rigidly moving segments (Vogel et al. (2015); Menze & Geiger (2015)), or semantic segments (Bai et al. (2016); Sevilla-Lara et al. (2016)).
- 超像素：因此，基于像素分组（即，超像素）的紧凑表示已经受欢迎。基于超像素的表示是通过将图像分割成在颜色和纹理理想上相似的原子区域和尊重图像边界来获得的（Ren＆Malik（2003）; Achanta等人（2012）; Li＆Chen（2015） ）。每个基于超像素的方法所产生的隐含假设是在某个超像素（例如，语义类标签或表面的倾斜）中保持一定的感兴趣的特性。然而，相对于这些属性的边界附着容易受到侵害，特别是对依赖于利用颜色或强度提示的标准分割算法的混乱图像。
- 如果可用，深度信息可以用作准确的超像素提取的有价值的特征（Badino等（2009）; Yamaguchi等（2014））。超像素被用作立体声和流量估计等各种任务的构建块（Yamaguchi et al。（2012，2013，2014）;G¨uney＆Geiger（2015）; Bai et al。（2016）），场景流（Menze ＆Geiger（2015）; Menze等（2015b）; Lv et al。（2016）），语义分割（Xiao＆Quan（2009）; Wegner等（2013）），场景理解（Ess et al。 2009b）; Liu et al。（2014））和3D重建（Sch¨onbein等（2014））。在包括几何推理的情况下
例如立体声估计，超像素通常表示3D平面片段。当目标是在场景流或光流中表现具有独立物体运动的现实世界场景时，可以将超像素推广到刚性移动段（Vogel等人（2015）; Menze＆Geiger（2015））或语义段（Bai et al。（2016）; Sevilla-Lara et al。（2016））。

- **Stixels**: Stixels are presented as a medium level representation of 3D traffic scenes with the goal to bridge the gap between pixels and objects (Badino et al. (2009)). The so-called “Stixel World” representation originates from the observation that free space in front of the vehicle is mostly limited by vertical surfaces. Stixels are represented by a set of rectangular sticks standing vertically on the ground to approximate these surfaces. Assuming a constant width, each stixel is defined by its 3D position relative to the camera and its height. The main goal is to gain eciency through a compact, complete, stable, and robust representation. In addition, Stixel representations provide an encoding of the free space and the obstacles in the scene.
- Using depth maps from SGM Hirschm¨uller (2008) as input, Badino et al. (2009) use dynamic programming based on occupancy grids to compute free space (determining the Stixels’ lower positions) and foreground/background segmentation on the disparity map (to compute the Stixels’ height). Pfeiffer & Franke (2011) extend Badino et al. (2009) to a unified probabilistic scheme. They lift the constraint of Stixels to touch the ground and allow multiple stixels along an image column. This way objects can be located at multiple depths in a single image column (Figure 5).
- Stixels：Stixels被呈现为3D流量场景的中等级别表示，目的是弥补像素和对象之间的差距（Badino等人（2009））。所谓的“Stixel World”表现起源于观察车前面的自由空间主要受垂直面的限制。柱状物由垂直放置在地面上的一组矩形棒表示以近似这些表面。假设一个恒定的宽度，每个Stixel是通过它相对于相机的3D位置及其高度来定义的。主要目标是通过紧凑，完整，稳定和强大的表现来提高效率。此外，Stixel表示提供了现场的自由空间和障碍物的编码。
- 使用SGMHirschm¨uller（2008）的深度图作为输入，Badino et al。 （2009）使用基于占用网格的动态规划来计算自由空间（确定Stixels的较低位置）和视差图上的前景/背景分割（以计算Stixels的高度）。 Pfeiffer＆Franke（2011）扩展了Badino等人（2009）统一概率方案。它们提升Stixels的约束以触摸地面，并允许沿着图像列的多个Stixels。这样，对象可以位于单个图像列中的多个深度（图5）。

- Pfeiffer & Franke (2010) extend the Stixel world representation to dynamic scenes by tracking stixels using a 6D Kalman filter framework and optical flow as input. Erbs et al. (2012, 2013) propose a CRF framework for segmenting a traffic scene based on the Dynamic StixelWorld representation. G¨unyel et al. (2012) show that motion estimation for stixels can be reduced to a 1D problem and can be solved eciently via 2D dynamic programming by avoiding costly dense optical flow computation.
- Levi et al. (2015) propose to use a CNN called StixelNet for learning to extract the foot point of each Stixel from the image. Cordts et al. (2014) propose to incorporate top-down objectlevel cues into bottom-up Stixel representation in a probabilistic approach. In order to achieve that, they leverage probability images derived from the output of three different object detectors, namely pedestrian, vehicle, and guard rail. Schneider et al.(2016) propose a semantic Stixel representation to jointly infer semantic and geometric layout of the scene from a dense disparity map and a pixel-level semantic scene labeling.
- Pfeiffer＆Franke（2010）通过使用6D卡尔曼滤波器框架和光流作为输入，通过跟踪Stixels将Stixel世界表示扩展到动态场景。 Erbs等人（2012年，2013年）提出了一个基于Dynamic StixelWorld表示来分割交通场景的CRF框架。 G¨unel等人（2012）显示，Stixels的运动估计可以减少到1D问题，可以通过2D动态编程有效地解决，避免了昂贵的密集光流计算。
- Levi et al。 （2015）建议使用名为StixelNet的CNN学习从图像中提取每个Stixel的脚点。 Cordts et al。 （2014）提出将自顶向下的物体等级线索纳入自下而上的Stixel表示法中，以概率方法。为了实现这一点，它们利用从三个不同物体检测器（即行人，车辆和护栏）的输出得到的概率图像。 Schneider等人（2016）提出了一种语义Stixel表示，从密集视差图和像素级语义场景标记共同推断场景的语义和几何布局。

- **3D Primitives**: The use of 3D geometric primitives is very common in 3D reconstruction, particularly when reconstructing urban areas. Atomic regions which are geometrically meaningful allow the shape of urban objects to be better preserved. In addition, simplified geometric assumptions can provide significant speedups as well as a compact model. In Cornelis et al.(2008), 3D city models are composed of ruled surfaces for both the facades and the roads. Duan & Lafarge (2016) use polygons with elevation estimate for 3D city modeling from pairs of satellite images. de Oliveira et al. (2016) update a list of large-scale polygons over time for an incremental scene representation from 3D range measurements. Lafarge et al. (2010) use a library of 3D blocks for reconstructing buildings with different roof forms. Lafarge & Mallet (2012); Lafarge et al. (2013) use 3D-primitives such as planes, cylinders, spheres or cones for describing regular structures of the scene. Dub´e et al. (2016) segments point clouds into distinct elements for a loop-closure detection algorithm based on the matching of 3D segments.
- 3D原语：在3D重建中使用3D几何图元非常常见，特别是在重建城市地区时。几何有意义的原子区域可以更好地保存城市物体的形状。此外，简化的几何假设可以提供显着的加速以及紧凑的模型。在Cornelis等人（2008）中，3D城市模型由外墙和道路的统治曲面组成。 Duan＆Lafarge（2016）使用多边形，对卫星图像进行三维城市建模的高程估计。 de Oliveira et al。 （2016）根据3D范围测量更新一个随时间推移的大尺寸多边形的增量场景表示。拉法基等人（2010）使用3D块库来重建具有不同屋顶形式的建筑物。拉法基与马勒（2012）;拉法基等人（2013）使用3D原始图像，例如平面，圆柱体，球体或锥体来描述场景的常规结构。 Dub'e等人（2016）将云划分成基于3D段匹配的闭环检测算法的不同元素。

###5. Object Detection 目标检测
- Reliable detection of objects is a crucial requirement to realize autonomous driving. As the car is sharing the road with many traffic participants, particularly in urban areas, the awareness of other traffic participants or obstacles is necessary to avoid accidents that might be life threatening. The detection in urban areas is hard because of the wide variety of object appearances and occlusions caused by other objects or the object of interest itself. In addition, the resemblance of objects to each other or to the background and physical effects like cast shadows or reflections can make the distinction difficult.
对物体的可靠检测是实现自主驾驶的关键要求。 由于汽车与许多交通参与者，特别是在城市交通拥挤的道路上，其他交通参与者或障碍物的意识是必要的，以避免可能危及生命的事故。 由于其他物体或感兴趣的物体引起的物体外观和遮挡物的种类繁多，所以在城市地区的检测是困难的。 另外，对象之间的相似性或背景和物理效果（如投射阴影或反射）的区别可能使得区分变得困难。

- **Sensors**: The object detection task can be addressed with a variety of different of sensors. Cameras are the cheapest and most commonly used type of sensors for the detection of objects. The visible spectrum (VS) is typically used for daytime detections whereas the infrared spectrum can be used for nighttime detection. Thermal infrared (TIR) cameras capture relative temperature which allows to distinguish warm objects like pedestrians from cold objects like vegetation or the road. Active sensors, that emit signals and observe their reflection, like laser scanners can provide range information which is helpful for detecting an object and localizing it in 3D. Depending on the weather conditions or material properties it can be problematic to rely on a single type of sensor alone. VS cameras and laser scanners are affected by reflective or transparent surfaces while hot objects (like engines) or warm temperatures can influence TIR cameras. The combination of information from different sensors via sensor fusion (Enzweiler & Gavrila (2011); Chen et al. (2016b); Gonz´alez et al. (2016)) allows for the robust integration of this complementary information.
- 传感器：物体检测任务可以用各种不同的传感器来寻址。相机是用于检测物体的最便宜和最常用的传感器类型。可见光谱（VS）通常用于日间检测，而红外光谱可用于夜间检测。热红外（TIR）摄像机捕获相对温度，允许区分温暖的物体，如行人与植物或道路等寒冷物体。发射信号并观察其反射的主动传感器，如激光扫描仪，可提供范围信息，有助于检测物体并将其定位在3D中。根据天气条件或材料特性，依靠单一类型的传感器可能是有问题的。 VS相机和激光扫描仪受到反光或透明表面的影响，而热物体（如发动机）或温暖的温度可影响TIR相机。通过传感器融合的不同传感器信息的组合（Enzweiler＆Gavrila（2011）; Chen等（2016b）; Gonz'alez等（2016））允许这种补充信息的稳健整合。

- **Standard Pipeline**: A traditional detection pipeline consists of the following steps: preprocessing, region of interest extraction (ROI), object classification and verification/refinement. In the preprocessing step tasks such as exposure and gain adjustment, as well as camera calibration and image rectification are usually performed. Some approaches leverage temporal information with a joint detection and tracking system. We give a detailed overview of the tracking problem in Section 9.
- Regions of interest can be extracted using a sliding window approach which shifts a detector over the image at different scales. As exhaustive search is very expensive, several heuristics have been proposed for reducing the search space. Typically, the number of evaluations is reduced by assuming a certain ratio, size and position of candidate bounding boxes. Apart from that, image features, stereo or optical flow can be leveraged for focusing the search on the relevant regions. Broggi et al. (2000) filter pedestrian candidates using morphological characteristics (size, ratio and shape) and vertical symmetry of human shape. In addition, they exploit the distance information obtained from stereo vision in the ROI extraction and refinement steps of the algorithm. Selective Search (Uijlings et al.(2013)) is an alternative approach to generate regions of interest. They exploit segmentation for efficiently extracting approximate locations instead of performing an exhaustive search over the full image domain.
- In their survey on pedestrian detection systems from monocular images, Dollar et al. (2011) present an extensive evaluation focusing on the evaluation of sliding window approaches. They claim that these approaches are most promising for low to medium resolution detection but found that detection with low-resolution inputs and occlusions are still problematic for the considered approaches.
- 标准管道：传统的检测流程包括以下步骤：预处理，感兴趣区域提取（ROI），对象分类和验证/细化。在预处理步骤中，通常执行诸如曝光和增益调整之类的任务，以及相机校准和图像校正。一些方法利用联合检测和跟踪系统来利用时间信息。我们详细介绍第9节中的跟踪问题。
- 可以使用将检测器移动到不同尺度的图像上的滑动窗口方法来提取感兴趣区域。由于穷举搜索非常昂贵，因此已经提出了几种启发式方法来减少搜索空间。通常，通过假设候选边界框的一定比例，大小和位置来减少评估的数量。除此之外，可以利用图像特征，立体声或光流来将搜索集中在相关区域。 Broggi等人（2000）过滤行人候选人使用形态特征（尺寸，比例和形状）和人体形状的垂直对称性。此外，它们利用在算法的ROI提取和细化步骤中从立体视觉获得的距离信息。选择性搜索（Uijlings等人（2013））是产生感兴趣区域的替代方法。它们利用分割来有效地提取近似位置，而不是在完整图像域上进行详尽的搜索。
- 在他们对单眼图像行人检测系统的调查中，Dollar et al。 （2011）提出了广泛的评估，侧重于滑动窗口方法的评估。他们声称这些方法对于中低分辨率检测是最有希望的，但发现用低分辨率输入和遮挡的检测对于所考虑的方法仍然是有问题的。

- **Classification**: The classification of all candidates in an image using the sliding window approach can become quite costly due to the vast amount of image regions which need to be classified. Therefore, a fast decision is necessary which quickly discards candidates in the background region of the image. Viola et al. (2005) combine simple and efficient classifiers, learned using AdaBoost, in a cascade which allows to quickly discard false candidates while spending more time on promising regions. With the work of Dalal & Triggs (2005), linear Support Vector Machines (SVMs), that maximizes the margin of all samples from a linear decision boundary, in combination with Histogram of Orientation (HOG) features have become popular tools for classification. However, all previous methods rely on hand-crafted features that are difficult to design. With the renaissance of deep learning, convolutional neural networks have automated this task while significantly boosting performance. For example, Sermanet et al. (2013) introduced CNNs to the pedestrian detection problem using unsupervised convolutional sparse auto-encoders to pre-train features and end-to-end supervised training to train the classifier while fine-tuning the features. Today, all state of the art detection approaches are learned in an end-to-end fashion from large datasets as we will discuss in Section 5.1.
- 分类：使用滑动窗口方法对图像中所有候选人的分类可能会变得相当昂贵，因为需要分类的大量图像区域。因此，需要快速地决定是否在图像的背景区域中放弃候选。 Viola等人（2005）结合了简单而有效的分类器，在AdaBoost中学习使用了一个级联，可以在有希望的地区花费更多的时间来快速丢弃错误的候选人。通过Dalal＆Triggs（2005）的工作，线性支持向量机（SVM）将线性决策边界中的所有样本的边缘与定向（HOG）特征的组合结合起来，已经成为流行的分类工具。然而，以前的所有方法都依赖于难以设计的手工制作功能。随着深度学习的复兴，卷积神经网络已经自动化了这项任务，同时显着提升了性能。例如，Sermanet等人（2013）引入了CNN对行人检测问题，采用无监督卷积稀疏自动编码器预先训练特征和端对端监控训练，对微分器进行微调。今天，所有最先进的检测方法都是以大型数据集的端到端方式学习，我们将在5.1节中讨论。

- **Part-based Approaches**: Learning the appearance of articulated objects is difficult because all possible articulations need to be considered. The idea of part-based approaches is to split the complex appearance of non-rigidly moving objects like humans into simple parts and to represent any articulation using these parts. This provides greater flexibility and reduces the number of training examples required for learning the object appearance. The Deformable Part Model (DPM) by Felzenszwalb et al. (2008) attempts to break down the complex appearance of objects into easier parts for training SVMs with latent structure variables which represent the model configuration and need to be inferred at training time. They use a coarse global template covering the entire object and higher resolution part templates to model the appearance of each part as illustrated in Figure 6. All templates are represented using HOG features. In addition, they generalize SVMs to handle latent variables such as the part position location.
- An alternative to this representation is the Implicit Shape Model proposed by Leibe et al. (2008a) which learns a highly flexible representation of object shape. They extract local features around interest points and perform clustering to build up a codebook of local appearances that are characteristic for the particular object class under consideration. Based on this codebook, they learn where on the object the codebook entries may occur.
- While the part-based models presented so far have been very successful, they can not represent contextual information which is necessary for occlusion reasoning. Usually, a separate context model is learned to handle occlusions, see Hoiem et al. (2008); Tu & Bai (2010); Desai et al. (2011); Yang et al. (2012). And-Or models embed a grammar to represent large structural and appearance variations in a reconfigurable hierarchy. Wu et al. (2016a) propose to learn an And-Or model which takes into account structural and appearance variations at multi-car, single-car and part-levels jointly to represent both context and occlusions.
基于部分的方法：学习表达对象的外观是困难的，因为需要考虑所有可能的表述。基于部分的方法的想法是将非刚性移动物体（如人类）的复杂外观分解成简单的部分，并使用这些部分来表示任何发音。这提供了更大的灵活性，并减少了学习对象外观所需的训练示例的数量。 Felzenszwalb等人的变形部分模型（DPM） （2008）尝试将对象的复杂外观分解为更容易的部分，用于训练具有潜在结构变量的SVM，这些变量表示模型配置，需要在训练时推断出。它们使用覆盖整个对象的粗糙全局模板和更高分辨率部件模板来对每个部件的外观进行建模，如图6所示。所有模板都使用HOG功能表示。此外，它们推广SVM来处理潜在变量，如零件位置位置。
- 这种表示的替代方法是Leibe等人提出的隐式形状模型。 （2008a），学习对象形状的高度灵活的表示。他们提取利益点附近的局部特征，并执行聚类，以建立对于正在考虑的特定对象类特征的局部外观的码本。基于这本代码本，他们将学习在物体上可能发生的代码簿条目。
- 尽管目前提出的基于零件的模型已经非常成功，但它们不能表示阻塞推理所必需的上下文信息。通常，学习单独的上下文模型来处理闭塞，参见Hoiem等人（2008）;涂和（2010）; Desai et al。 （2011）;杨等（2012年）。和 - 或者模型嵌入语法来表示可重构层次结构中的大型结构和外观变化。吴等（2016a）建议学习一个兼并多车型，单车和部分级别的结构和外观变化的And-Or模型，共同表示上下文和闭塞。

- 5.1. 2D Object Detection 二维目标侦测
- KITTI Geiger et al. (2012b) is among the most popular benchmarks for object detection systems in the autonomous car context. A similar popularity for the pedestrian detection task has the Caltech-USA dataset (Doll´ar et al. (2012)). In this work we would like to focus our attention on the KITTI benchmark since it allows us to compare object and pedestrian detection systems on the same data. We refer the interested reader to the survey papers (Benenson et al. (2014); Zhang et al. (2016b)) for an in-depth comparison of pedestrian detection systems on Caltech-USA. In Table 1 we show the state-of-the-art on the KITTI benchmark for object, pedestrian and cyclist detection from images. Note that for all result tables in this paper, we list only public methods which have a paper associated with them as the details for the anonymous entries cannot be discussed yet. The performance is assessed for three level of difficulties using PASCAL VOC intersection-over-union (IOU) (Everingham et al. (2010)). Easy examples have a minimum bounding box height of 40 px and are fully visible, whereas moderate examples have a minimum height of 25 px including partial occlusion and hard examples have the same minimum height but includes the maximum occlusion level. In Table 2 the estimation of the object’s orientation is evaluated using the average orientation similarity (AOS) proposed in Geiger et al. (2012b).
KITTI Geiger等人（2012b）是自主汽车背景下对象检测系统最受欢迎的基准之一。行人检测任务的类似知名度有Caltech-USA数据集（Doll'ar et al。（2012））。在这项工作中，我们希望将注意力集中在KITTI基准上，因为它允许我们在同一数据上比较物体和行人检测系统。我们将有兴趣的读者参考调查文件（Benenson等（2014）; Zhang等（2016b）），对Caltech-USA的行人检测系统进行了深入的比较。在表1中，我们展示了从图像中的对象，行人和骑自行车者检测的KITTI基准测试的最新技术。请注意，对于本文中的所有结果表，我们仅列出与它们相关联的公共方法，因为匿名条目的详细信息不能讨论。使用PASCAL VOC交叉联盟（IOU）（Everingham等人（2010））评估了表现的三个难度。简单的例子具有40像素的最小边界框高度，并且是完全可见的，而适度示例具有25像素的最小高度，包括部分遮挡，硬实例具有相同的最小高度，但包括最大遮挡水平。在表2中，使用Geiger等人提出的平均取向相似度（AOS）来评估对象的取向的估计。 （2012B）。

- Convolutional Neural Networks allowed a significant improvement in the performance of object detectors. In the beginning, CNNs were integrated in sliding-window approaches (Sermanet et al. (2013)). However, the precise localization of objects is challenging because of the large receptive fields and strides. Girshick et al. (2014), on the other hand, propose RCNNs to solve the CNN localization problem with a “recognition using regions” paradigm. They generate many region proposals using selective search (Uijlings et al. (2013)), extract a fixed-length feature vector for each proposal using a CNN and classify each region with a linear SVM. Region-based CNNs are computationally expensive but several improvements have been proposed to reduce the computational burden (He et al.(2014); Girshick (2015)). He et al. (2014) use spatial pyramid pooling which allows to compute a convolutional feature map for the entire image with only one run of the CNN in contrast to R-CNN that needs to be applied on many image regions. Girshick (2015) further improve with a single-stage training algorithm that jointly learns to classify object proposals and refine their spatial locations. Even though these region-based networks have proven to be very successful on the PASCAL VOC benchmark, they could not achieve similar performance on KITTI. The main reason for this is that the KITTI dataset contains objects at many different scales and small objects
which are often heavily occluded or truncated. These objects are hard to detect using the region-based networks. Therefore, several methods for obtaining better object proposals have been proposed (Ren et al. (2015); Chen et al. (2016b,a); Yang et al. (2016); Cai et al. (2016)).
- 卷积神经网络可以显着改善物体探测器的性能。一开始，CNN被纳入滑窗方法（Sermanet等（2013））。然而，物体的精确定位是具有挑战性的，因为大的接收场和步幅。 Girshick等人（2014），另一方面，提出RCNNs以“使用区域认知”模式解决CNN本地化问题。他们使用选择性搜索生成许多区域提案（Uijlings等人（2013）），使用CNN提取每个提案的固定长度特征向量，并使用线性SVM对每个区域进行分类。基于区域的CNN在计算上是昂贵的，但是已经提出了几种改进来减少计算负担（He等人（2014）; Girshick（2015））。他等（2014）使用空间金字塔池，其允许计算整个图像的卷积特征图，只有CNN的一次运行与需要应用于许多图像区域的R-CNN相反。 Girshick（2015）通过单阶段训练算法进一步改进，共同学习对对象建议进行分类并优化其空间位置。尽管这些基于区域的网络已被证明在PASCAL VOC基准上非常成功，但是它们在KITTI上无法达到类似的表现。其主要原因是KITTI数据集包含许多不同尺度和小对象的对象
它们经常被遮蔽或截断。这些对象很难使用基于区域的网络进行检测。因此，提出了几种获得更好的对象提案的方法（Ren et al。（2015）; Chen et al。（2016b，a）; Yang et al。（2016）; Cai et al。（2016））。

- Ren et al. (2015) have introduced Region Proposal Networks (RPN) in which the region proposal network shares full-image convolutional features with the detection network and thus doesn’t increase computational costs. RPNs are trained end-to-end to generate high quality region proposals which are classified using the Fast R-CNN detector (Girshick (2015)). Chen et al. (2015c) use 3D information estimated from a stereo camera pair to extract better bounding box proposals. They place 3D candidate boxes on the ground plane and score them using 3D point cloud features. Finally, a CNN exploiting contextual information and using a multi-task loss jointly regresses the object’s coordinates and orientation. Inspired by this approach, Chen et al. (2016a) learn to generate class-specific 3D object proposals for monocular images, exploiting contextual models as well as semantics. They generate proposals by exhaustively placing 3D bounding boxes on the ground plane and scoring them with a standard CNN pipeline (Chen et al. (2015c)). Both methods Chen et al. (2015c) and Chen et al. (2016a) achieve comparable results to the best performing method in all detection task while outperforming all other methods on easy examples of KITTI car (Table 1a). In addition, they are among the best performing methods for the orientation estimation (Table 2).
- ren（2015）引入了区域提案网络（RPN），区域提案网络与检测网络共享全图像卷积特征，从而不增加计算成本。 RPN端对端进行培训，以生成使用Fast R-CNN检测器分类的高质量区域提案（Girshick（2015））。陈等（2015c）使用从立体相机对估计的3D信息来提取更好的边框提案。他们将3D候选箱放置在地面上，并使用3D点云特征对其进行评分。最后，利用上下文信息和使用多任务丢失的CNN联合回归对象的坐标和方向。灵感来自于这种方法，Chen et al。 （2016a）学习为单眼图像生成类特定的3D对象提案，利用上下文模型以及语义。他们通过在地平面上彻底放置3D边界框并用标准的CNN流水线（Chen et al。（2015c））得出建议。 Chen等人（2015c）和陈等人（2016a）在所有检测任务中实现与最佳性能方法相当的结果，同时在KITTI车的简单示例上表现超过所有其他方法（表1a）。此外，它们是方位估计中表现最好的方法之一（表2）。

- An alternative approach is presented by Yang et al. (2016). In case of small objects a strong activation of convolutional neurons is more likely to occur in earlier layers. Therefore, Yang et al. (2016) use scale-dependent pooling which allows to represent a candidate bounding box using the convolutional features from the corresponding scale. In addition, they propose layer-wise cascaded rejection classifiers, treating convolutional features in early layers as weak classifiers, to efficiently eliminate negative object proposals. The proposed scale-dependent pooling approach is one of the best-performing methods in all tasks ( Table 1 ).
- Cai et al. (2016) propose a multi-scale CNN consisting of a proposal sub-network and a detection sub-network. The proposal network, illustrated in Figure 7, performs detection at multiple output layers and these complementary scale-specific detectors are combined to produce a strong multi-scale object detector. Their multi-scale CNN outperforms all other methods on KITTI pedestrian and cyclist (Tables 1b,1c) while ranking second on KITTI car (Table 1a). Xiang et al. (2016) propose a region proposal network that uses subcategory information obtained from 3DVP (Xiang et al. (2015b)), to guide the proposal generating process, and a detection network for joint detection and subcategory classification. Object subcategories are defined for objects with similar properties or attributes such as appearance, pose or shape. The subcategory information allows them to outperform all other methods for the detection task on KITTI cars (Table 1a) and to achieve the best performance in the orientation estimation (Table 2).
- Yang等人提出了一种替代方法（2016）。在小物体的情况下，卷积神经元的强烈活化更可能发生在较早的层中。因此，Yang等（2016）使用规模依赖池，其允许使用来自相应量表的卷积特征来表示候选边界框。此外，他们提出层次级联拒绝分类器，处理早期层次中的卷积特征作为弱分类器，以有效消除负面对象提案。所提出的规模依赖性池化方法是所有任务中表现最好的方法之一（表1）。
- cai（2016）提出了由提案子网和检测子网组成的多尺度CNN。如图7所示的提议网络在多个输出层执行检测，并且将这些互补的比例特异性检测器组合以产生强大的多尺度对象检测器。他们的多尺度CNN优于KITTI行人和骑自行车的其他方法（表1b，1c），同时在KITTI车上排名第二（表1a）。 Xiang et al。 （2016）提出了一个使用从3DVP（Xiang et al。（2015b）获得的子类别信息）的区域提案网络，指导建议生成过程和联合检测和子类别分类的检测网络。对象子类别定义为具有类似属性或属性（如外观，姿态或形状）的对象。子类别信息使他们能够胜过KITTI车辆检测任务的所有其他方法（表1a），并在方位估计中达到最佳性能（表2）。

- 5.2. 3D Object Detection from 2D Images 用二维图像完成检测
- Geometric 3D representations of object classes can recover far more details than just 2D or 3D bounding boxes, however most of today’s object detectors are focused on robust 2D matching. Zia et al. (2013) exploit the fact that high-quality 3D CAD models are available for many important classes. From these models, they obtain coarse 3D wireframe models using principal components analysis and train detectors for the vertices of the wireframe. At test time, they generate evidence for vertices by densely applying the detectors. Zia et al. (2015) extend this work by directly using detailed 3D CAD models in their formulation, combining them with explicit representations of likely occlusion patterns. Further, a ground plane is jointly estimated to stabilize the pose estimation process. This extension outperforms the pseudo-3D model of Zia et al. (2013) and shows the benefits of reasoning in true metric 3D space.
- While these 3D representations provide more faithful descriptions of objects they can not yet compete with state-of-theart detectors using 2D bounding boxes. To overcome this problem, Pepik et al. (2015) propose a 3D extension of the powerful deformable part model (Felzenszwalb et al. (2008)), which combines the 3D geometric representation with robust matching to real-world images. They further add 3D CAD information of the object class of interest as geometry cue to enrich the appearance model.
- 对象类的几何3D表示可以恢复比2D或3D边界框更多的细节，然而，今天的大多数对象检测器都集中在鲁棒的2D匹配上。 Zia等人（2013）利用高质量3D CAD模型可用于许多重要课程。从这些模型中，他们使用主成分分析和线框检测器来获取粗略的3D线框模型。在测试时间，它们通过密集地应用检测器产生顶点的证据。 Zia等人（2015）通过在制定中直接使用详细的3D CAD模型来扩展这项工作，将它们与可能的闭塞模式的显式表示相结合。此外，共同估计地平面以稳定姿态估计过程。该扩展优于Zia等人的伪3D模型。 （2013），并显示了在真正的度量3D空间推理的好处。
- 虽然这些3D表示提供了对物体的更忠实的描述，但是它们还不能与使用2D边界框的状态检测器竞争。为了克服这个问题，Pepik（2015）提出了强大的可变形部分模型的3D扩展（Felzenszwalb等人（2008）），其将3D几何表示与实际图像的鲁棒匹配相结合。他们进一步添加感兴趣的对象类的3D CAD信息作为几何提示，以丰富外观模型。

- 5.3. 3D Object Detection from 3D Point Clouds 用三维点群做三维检测
- The KITTI dataset Geiger et al. (2012b) provides synchronized camera and LiDAR frames and allows the comparison of image-based and LiDAR-based approaches on the same data. In contrast to cameras, LiDAR laser range sensors directly provide accurate 3D information which simplifies the extraction of object candidates and can be helpful for the classification task as it provides 3D shape information. However, 3D data from laser scanners is typically sparse and its spatial resolution is limited. Therefore, the state-of-the-art relying only on laser range data can not reach the performance of camera-based detection systems, yet. In Table 3 we show the LiDAR-based state-of-the-art on the KITTI benchmark for object, pedestrian and cyclist detection. The performance is assessed similar to the image-based approaches using the PASCAL intersection over- union by projecting the 3D bounding boxes into the image plane.
- Wang & Posner (2015) propose an efficient scheme to apply the common 2D sliding window detection approach to 3D data. More specifically, they exploit the sparse nature of the problem using a voting scheme to search all possible object locations and orientations. Li et al. (2016b) improve upon these results by exploiting a fully convolutional neural network for detecting vehicles from range data. They represent the data in a 2D point map, and predict an objectness confidence and a bounding box simultaneously using a single 2D CNN. The encoding used to represent the data allows them to predict the full 3D bounding box of the vehicles. Engelcke et al. (2016) leverage a featurecentric voting scheme to implement a novel convolutional layer which exploits the sparsity of the point cloud. Additionally, they propose to use the L1 penalty for regularization.
- Relying on laser range data alone makes the detection task challenging due to the limited density of the laser scans. Thus, existing LiDAR-based approaches perform weaker compared to their image-based counterparts on the KITTI datasets. Chen et al. (2016c) combine LiDAR laser range data with RGB images for object detection. In their approach, the sparse point cloud is encoded using a compact multi-view representation and a proposal generation network utilizes the bird’s eye view representation of the point cloud to generate 3D candidates. Finally, they combine region-wise features from multiple views with a deep fusion scheme as illustration in Figure 8. This approach outperforms the other LiDAR-based approaches by a significant margin and achieves state-of-the-art performance in the KITTI car benchmarks (Tables 1a,3a).
- KITTI数据集Geiger等（2012b）提供了同步的摄像机和LiDAR帧，并允许在相同的数据上比较基于图像和基于LiDAR的方法。与摄像机相反，LiDAR激光测距传感器直接提供精确的3D信息，简化了对象候选的提取，可以为分类任务提供3D形状信息。然而，来自激光扫描仪的3D数据通常很稀疏，其空间分辨率有限。因此，依靠激光测距数据的最先进技术还不能达到基于摄像机的检测系统的性能。在表3中，我们展示了基于LiDAR的最先进的KITTI基准测试对象，行人和骑车人检测。通过将3D边界框投影到图像平面中，性能被评估与使用PASCAL交叉联合的基于图像的方法相似。
- Wang＆Posner（2015）提出了一种将常用2D滑动窗口检测方法应用于3D数据的有效方案。更具体地说，它们使用投票方案来利用问题的稀疏性来搜索所有可能的对象位置和方向。 Li et al。 （2016b）通过利用完全卷积神经网络来检测来自范围数据的车辆来改善这些结果。它们表示2D点图中的数据，并使用单个2D CNN同时预测物体信心和边界框。用于表示数据的编码允许他们预测车辆的完整3D边界框。 Engelcke等人（2016）利用特征中心投票方案来实现利用点云稀疏性的新颖卷积层。此外，他们建议将L1惩罚用于正规化。
- 依靠激光测距数据，由于激光扫描的密度有限，使得检测任务具有挑战性。因此，与基于KITTI数据集的基于图像的对手相比，现有的基于LiDAR的方法表现较弱。chen（2016c）将LiDAR激光测距数据与用于物体检测的RGB图像相结合。在他们的方法中，使用紧凑的多视图表示对稀疏点云进行编码，并且提案生成网络利用点云的鸟瞰图表示来生成3D候选。最后，他们将来自多个视图的区域特征与深度融合方案结合在一起，如图8所示。该方法在显着优势下胜过其他基于LiDAR的方法，并在KITTI汽车基准测试中达到最先进的性能（表1a，3a）。

- 5.4. Person Detection 人物检测
- While so far we have discussed general object detection algorithms, we now focus on specific approaches to person or pedestrian detection which are of high relevance to any autonomous system interacting with a real environment. As human behavior is less predictable than the behavior of a car, reliable person detection is necessary to drive safely in the proximity of pedestrians. The detection of people is particularly difficult because of the large variety of appearances due to different clothing and articulated poses. Furthermore, the articulation and interaction of pedestrians can strongly affect the appearance of pedestrians in case of partial occlusion.
- **Pedestrian Protection Systems**: This problem has been deeply investigated for advanced driver assistance systems to increase road safety. Pedestrian protection systems (PPS) detect the presence of stationary and moving people around a moving vehicle in order to warn the driver against dangerous situations. Even though missed detections of a PPS can still be handled by the driver, the pedestrian detection of an autonomous car needs to be flawless. The pedestrian detection system needs to be robust against all weather conditions and efficient for real-time detection. Geronimo et al. (2010) survey pedestrian detection for Advanced Driver Assistance Systems.
- **Surveys**: Enzweiler&Gavrila (2009) give a very broad overview of different architectures for monocular pedestrian detection. They make the observation that the HOG/SVM combination as proposed by Dalal & Triggs (2005) works well at higher resolutions with higher processing time whereas AdaBoost cascade approaches are superior at lower resolutions, achieving near real-time performance. In their survey, Benenson et al. (2014) found no clear evidence that a certain type of classifier (e.g., SVM or decision forests) is better suited than others. In particular, Wojek & Schiele (2008b) show that AdaBoost and linear SVM perform roughly the same if enough features are given. Moreover, Benenson et al. (2014) observe that part based models like (Felzenszwalb et al. (2008)) improve results only slightly compared to the much simpler approach of Dalal & Triggs (2005). They conclude that the number and diversity of features is clearly an important factor for the performance of classifiers since the classification problem becomes easier with higher dimensional representations. Consequently, today all state-of-the-art pedestrian detection systems use convolutional neural networks and learn feature representations in an end-to-end fashion (Cai et al. (2016); Xiang et al. (2016); Zhu et al. (2016); Yang et al. (2016); Chen et al. (2015c); Ren et al. (2015)).
- 虽然到目前为止，我们已经讨论了一般对象检测算法，但是我们现在关注与人或行人检测的具体方法，这些方法与任何与真实环境相互作用的自治系统都具有高度的相关性。由于人的行为比汽车的行为不太可预测，所以需要可靠的人员检测来安全行驶在行人附近。人的检测是特别困难的，因为由于不同的衣服和关节姿势，出现了各种各样的外观。此外，在部分闭塞的情况下，行人的关节和相互作用可能会严重影响行人的出现。
- 行人保护系统：这个问题已经深入调查了先进的驾驶员辅助系统，以增加道路安全。行人保护系统（PPS）检测移动车辆周围的固定和移动人员的存在，以警告驾驶员处于危险状况。虽然驾驶员仍然可以处理PPS的错误检测，但自行车的行人检测需要完美无缺。行人检测系统需要对所有天气状况都很强大，对于实时检测是有效的。 Geronimo等人（2010年）高级驾驶员辅助系统行人检测调查。
- 调查：Enzweiler＆Gavrila（2009）给出了单眼行人检测的不同架构的非常广泛的概述。他们提出，Dalal＆Triggs（2005）提出的HOG / SVM组合在更高的分辨率下处理时间更长，而AdaBoost级联方法在较低分辨率下更好，实现近乎实时的性能。在他们的调查中，Benenson等（2014）发现没有明确的证据表明某种类型的分类器（例如SVM或决策树）比其他分类器更适合。特别地，Wojek＆Schiele（2008b）表明，如果给出足够的特征，AdaBoost和线性SVM执行大致相同。此外，Benenson等（2014）观察到，像Felzenszwalb等人（2008）这样的部分模型比Dalal＆Triggs（2005）更简单的方法略有提高。他们得出结论，特征的数量和多样性显然是分类器性能的重要因素，因为分类问题变得更容易，具有更高的维度表示。因此，今天所有最先进的行人检测系统都使用卷积神经网络，并以端到端的方式学习特征表征（Cai et al。（2016）; Xiang et al。（2016）; Zhu et al （2016）; Yang et al。（2016）; Chen et al。（2015c）; Ren et al。（2015））。

- **Temporal Cues**: Similarly, Shashua et al. (2004) point out the importance of good features for the person detection task. They noted that the integration of additional cues measured over time (dynamic gait, motion parallax) and situation specific features (such as leg positions at certain poses) are key for reliable detection. Wojek et al. (2009) notice that most pedestrian detection systems rely only on a single image as input and do not exploit the available temporal information of objects in video sequences. They show significant improvement in detection performance by incorporating motion cues and combining different complementary feature types.
- **Scarcity of Target Class**: The enlargement of training data allows to train sophisticated models for the detection problem. However, the generation of examples belonging to the target class is usually time consuming because of manual labeling while many negative examples can be easily obtained. Enzweiler & Gavrila (2008) address the bottleneck caused by the scarcity of samples of the target class. They create synthesized virtual samples with a learned generative model to enhance a discriminative model. The generative model captures prior knowledge about the pedestrian class and allows significant improvement in the classification performance.
- **Real-time Pedestrian Detection**: In case of a potential collision with pedestrians a fast detection allows early intervention of the autonomous system. Benenson et al. (2012) provide fast and high quality pedestrian detections based on better handling of scales and exploiting depth extracted from stereo. Instead of resizing the images, they scale HOG features similar to Viola & Jones (2004). The Stixel World representation (Badino et al. (2009)) provides depth information which allows to significantly reduce the search space and detect pedestrians at 80 Hz in a parallel framework.
- 时间线：同样，Shashua等（2004）指出了人员检测任务的良好功能的重要性。他们指出，随时间推移的额外提示的集成（动态步态，运动视差）和情况特定特征（例如某些姿势下的腿部位置）是可靠检测的关键。 Wojek（2009）指出，大多数行人检测系统仅依赖于单个图像作为输入，并且不会利用视频序列中对象的可用时间信息。它们通过结合运动线索和组合不同的互补特征类型，显着提高了检测性能。
- 目标类的稀缺：扩大训练数据可以训练出复杂的模型用于检测问题。然而，属于目标类的示例的生成通常是耗时的，因为手工标记，而许多负面示例可以容易地获得。 Enzweiler＆Gavrila（2008）解决了目标课程样本稀缺所造成的瓶颈。他们使用学习的生成模型创建合成的虚拟样本，以增强歧视性模型。生成模型捕获有关行人阶级的先前知识，并且可以显着提高分类性能。
- 实时行人检测：如果与行人有潜在的碰撞，快速检测可以让自主系统的早期干预。 Benenson（2012）基于更好地处理尺度和利用从立体声提取的深度提供快速和高质量的行人检测。他们不是调整图像大小，而是缩放与“Viola＆Jones”相似的HOG功能（2004）。 Stixel World表示（Badino等人（2009））提供了深度信息，允许在并行框架中显着减少搜索空间并检测80Hz的行人。

- 5.5. Human Pose Estimation 人物姿态估计
- The pose and gaze of a person provides important information to the autonomous vehicle about the behavior and intention of the person. However, the pose estimation problem is challenging since the pose space is very large and typically people can only be observed on low resolutions, because of their size and distance to the vehicle. Several approaches have been proposed to jointly estimate the pose and body parts of a person. Traditionally, a two-staged approach was used by first detecting body parts and then estimating the pose as in (Pishchulin et al.(2012); Gkioxari et al. (2014); Sun & Savarese (2011)). This is problematic in cases when people are in proximity of each other because body-parts can be wrongly assigned to different instances.
- Pishchulin et al. (2016) present DeepCut, a model which jointly estimates the poses of all people in an image. The formulation is based on partitioning and labeling a set of bodypart hypotheses obtained from a CNN-based part detector. The model jointly infers the number of people, their poses, spatial proximity and part level occlusions. Bogo et al. (2016) use DeepCut to estimate the 3D pose and 3D shape of a human body from a single unconstrained image. SMPL, a 3D body shape model proposed by Loper et al. (2015), is fit to predictions of the 2D body joint locations from DeepCut. SMPL captures correlations in human shape across the population which allows to robustly fit human poses even in the presence of weak observations.
- 一个人的姿势和目光向自治车辆提供关于该人的行为和意图的重要信息。然而，姿势估计问题是具有挑战性的，因为姿态空间非常大，并且通常人们只能以低分辨率观察，因为它们的大小和与车辆的距离。已经提出了几种方法来共同估计一个人的姿势和身体部位。传统上，通过首先检测身体部位然后估计姿势（Pishchulin等人（2012）; Gkioxari等人（2014）; Sun＆Savarese（2011））采用了两阶段方法。在人们彼此接近的情况下，这是有问题的，因为身体部位可能被错误地分配给不同的实例。
- Pishchulin等（2016）目前DeepCut是一个共同估计图像中所有人的姿势的模型。该公式基于从基于CNN的部件检测器获得的一组bodypart假设的划分和标记。该模型共同推断了人数，姿势，空间接近度和部分等级闭塞。 Bogo等人（2016）使用DeepCut从单个无约束图像估计人体的3D姿态和3D形状。 SMPL，由Loper等人提出的3D体形模型（2015）适用于DeepCut的2D身体关节位置的预测。 SMPL捕获人口形态的相关性，即使在存在弱观察的情况下，也可以强制适应人类的姿势。

- 5.6. Discussion 讨论
- Object detection works already quite well in case of high resolution with little occlusions. For the easy and moderate cases of the car detection task (Table 1a) many methods show impressive performance. The pedestrian and cyclist detection task (Tables 1b,1c) is more challenging and thus weaker overall performance can be observed. One reason for this is the limited number of training examples and the possibility of confusing cyclists and pedestrians which differ only via their context and semantics. Remaining major problems across tasks are detection of small objects and highly occluded objects. In the leaderboards this manifests in a significant drop in performance when comparing easy, moderate and hard examples. Qualitatively, this can be observed in Figures 9, 10,11 where we show typical estimation errors of the best performing methods on the KITTI dataset. A major source of errors are crowds of pedestrians, groups of cyclists and lines of cars that cause many occlusions and lead to missing detections for all methods. Furthermore, a large amount of distant objects needs to be detected in some cases which is still a challenging task for modern methods since the amount of information provided by these objects is very low.
- 在高分辨率的情况下，对象检测工作已经很好了，几乎没有遮挡。对于容易和适度的汽车检测任务（表1a），许多方法表现出令人印象深刻的性能。行人和骑车人员检测任务（表1b，1c）更具挑战性，因此可以观察到整体性能较差。这样做的一个原因是训练样本数量有限，以及混淆骑自行车者和行人的可能性，只有通过上下文和语义不同。任务中仍然存在的主要问题是检测小物体和高遮蔽物体。在排行榜中，当比较简单，中等和难度的例子时，这表现出显着的性能下降。定性地，这可以在图9,10,11中进行观察，其中我们显示KITTI数据集上最佳性能方法的典型估计误差。错误的主要原因是行人群，骑自行车的人群和汽车线，导致许多障碍，导致所有方法的检测失踪。此外，在某些情况下需要检测大量的遥远物体，这对现代方法来说仍然是一项具有挑战性的任务，因为这些物体所提供的信息量非常低。


### 6. Semantic Segmentation
- Semantic segmentation, is a fundamental topic in computer vision. The goal of semantic segmentation is to assign each pixel in the image a label from a predefined set of categories. The task is illustrated in Figure 12 with all pixel of a certain category colorized in as specific color in a scene of the Cityscapes dataset by Cordts et al. (2016) recorded in Zurich. Segmentation of images into semantic regions usually found in street scenes, such as cars, pedestrians, or road affords a comprehensive understanding of the surrounding which is essential to autonomous navigation. Challenges of semantic segmentation arise from the complexity of the scene and the size of the label space.
- 语义分割是计算机视觉中的一个基本课题。 语义分割的目标是为图像中的每个像素分配来自预定义类别集合的标签。 该任务在图12中示出，Cordts等人在Cityscapes数据集的场景中以特定颜色的所有像素着色。 （2016年）记录在苏黎世。 将图像分割成通常在街道场景中发现的语义区域，例如汽车，行人或道路，可以全面了解对自主导航至关重要的周边环境。 语义分割的挑战源于场景的复杂性和标签空间的大小。

- **Formulation**: Traditionally, the semantic segmentation problem was posed as maximum a posteriori (MAP) inference in a conditional random field (CRF), defined over pixels or super pixels (He et al. (2004, 2006)). However, these early formulations were not efficient and could only handle only datasets of limited size and a small number of classes. Furthermore, only very simple features such as color, edge and texture information have been exploited. Shotton et al. (2009) observed that more powerful features can significantly boost performance and proposed an approach based on a novel type of features called texture-layout filter that exploits the textural appearance of objects, its layout and textural context. They combine texturelayout filters with lower-level image features in a CRF to obtain pixel-level segmentations. Randomized boosting and piecewise training techniques are exploited to efficiently train the model.
- Hierarchical and long-range connectivity, as well as higherorder potentials defined on image regions were considered to tackle the limited ability of CRFs to model long-range interactions within the image. However, methods based on image regions (He et al. (2004); Kumar & Hebert (2005); He et al. (2006); Kohli et al. (2009); Ladicky et al. (2009, 2014)) are restricted by the accuracy of the image segmentations used as input. In contrast, Krahenbuhl & Koltun (2011) propose a highly efficient inference algorithm for fully connected CRF models which models pairwise potentials between all pairs of pixels in the image.
- 制定：传统上，语义分割问题被构成为在像素或超像素上定义的条件随机场（CRF）中的最大后验（MAP）推理（He et al。（2004，2006））。 然而，这些早期方法并不有效，只能处理有限大小和少数类的数据集。 此外，只有非常简单的特征，如颜色，边缘和纹理信息已被利用。 Shotton等人 （2009）观察到，更强大的功能可以显着提高性能，并提出了一种基于一种新颖的功能类型的方法，称为纹理布局过滤器，利用了对象的纹理外观，其布局和纹理上下文。 它们将纹理布局过滤器与CRF中的较低级别图像特征相结合，以获得像素级分割。 利用随机提升和分段训练技术有效地训练模型。
- 被认为分层和远距离连接以及图像区域定义的更高阶电位可以解决CRF在图像中建立长距离相互作用的有限能力。 然而，基于图像区域的方法（He et al。（2004）; Kumar＆Hebert（2005）; He et al。（2006）; Kohli et al。（2009）; Ladicky et al。（2009，2014）） 受用作输入的图像分割的准确性的限制。 相比之下，Krahenbuhl＆Koltun（2011）为完全连接的CRF模型提出了一种高效的推理算法，它们在图像中的所有像素对之间建模成对的电位。

- The methods so far consider each object class independently while the co-occurrence of object classes can be an important clue for semantic segmentation, for example cars are more likely to occur in a street scene than in an office. Consequently, Ladicky et al. (2010) propose to incorporate object class cooccurrence as global potentials in a CRF. They show how these potentials can be efficiently optimized using a graph cut algorithm and demonstrate improvements over simpler pairwise models.
- The success of deep convolutional neural networks for image classification and object detection has sparked interest in leveraging their power for solving the pixel-wise semantic segmentation task. The fully convolutional neural network (Long et al. (2015)) is one of the earliest works which applies CNNs to the image segmentation problem. However, while modern convolutional neural networks for image classification combine multi-scale contextual information by consecutive pooling and subsampling layers that lower the resolution, semantic segmentation requires multi-scale contextual reasoning together with full-resolution dense prediction. In the following we will review recent approaches which address this problem.
- We focus the comparison of different semantic segmentation approaches on the Cityscapes dataset by Cordts et al. (2016) described in Section 2 because of the autonomous driving context. Table 4a shows the leaderboard of Cityscapes for the pixel-level semantic labeling task. The intersection-over union metric is provided for two semantic granularities, i.e., classes and categories, and additionally the instance-weighted IoU is reported for both granularities to penalize methods ignoring small instances.
- 迄今为止，这些方法独立地考虑每个对象类，而对象类的共现可能是语义分割的重要线索，例如，汽车在街景场景中比在办公室中更可能发生。 因此，Ladicky et al。 （2010）提出将对象类并发作为全球潜力纳入CRF。 他们展示了如何使用图形切割算法有效地优化这些潜力，并通过简单的成对模型来证明改进。
- 用于图像分类和物体检测的深卷积神经网络的成功引发了兴趣，利用他们的力量来解决像素方面的语义分割任务。 完全卷积神经网络（Long et al。（2015））是将CNN应用于图像分割问题的最早的作品之一。 然而，虽然用于图像分类的现代卷积神经网络通过连续的合并和降低分辨率的子采样层组合多尺度上下文信息，语义分割需要多尺度上下文推理以及全分辨率密集预测。 在下文中，我们将回顾最近解决这个问题的方法。
- Cordts等人对Cityscapes数据集的不同语义分割方法进行了比较。 （2016）由于自主驾驶环境而在第2节中描述。 表4a显示了Cityscapes针对像素级语义标注任务的排行榜。 提供了交叉联合度量用于两个语义粒度，即类和类别，另外为两个粒度报告了实例加权的IoU，以惩罚忽略小实例的方法。

- **Structured CNNs**: Recently, several methods have been proposed to tackle the opposing needs of multi-scale inference and full-resolution prediction output. Dilated convolutions have been proposed (Chen et al. (2015b); Yu & Koltun (2016)) to enlarge the receptive field of neural networks without loss of resolution. Their operation corresponds to regular convolution with dilated filters which allows for efficient multi-scale reasoning while limiting the increase in the number of model parameters.
- In the SegNet model, Badrinarayanan et al. (2015) have replaced the traditional decoder in a deep architecture with a network which consists of a hierarchy of decoders one corresponding to each encoder. Each decoder maps a low resolution feature map of an encoder (max-pooling layer) to a higher resolution feature map. In particular, the decoder in their model takes advantage of the pooling indices computed in the max-pooling step of the corresponding encoder to implement the upsampling process. This eliminates the need to learn the upsampling and thus results in a smaller number of parameters. Furthermore, sharper segmentation boundaries have been demonstrated using this approach.
- 结构化CNN：最近，已经提出了几种方法来解决多尺度推理和全分辨率预测输出的相反需求。 已经提出了扩张卷积（Chen et al。（2015b）; Yu＆Koltun（2016））来扩大神经网络的接受场而不损失分辨率。 它们的操作对应于具有扩张过滤器的常规卷积，其允许有效的多尺度推理，同时限制模型参数的数量的增加。
- 在SegNet模型中，Badrinarayanan等 （2015）已经用深层架构代替了传统的解码器，网络由一个对应于每个编码器的解码器层次组成。 每个解码器将编码器的低分辨率特征图（最大池）映射到更高分辨率的特征图。 特别地，其模型中的解码器利用在相应编码器的最大汇集步骤中计算的汇集指数来实现上采样过程。 这消除了学习上采样的需要，从而导致较少数量的参数。 此外，已经使用这种方法证明了更清晰的分割边界。

- While activation maps at lower-levels of the CNN hierarchy lack object category specificity, they do contain higher spatial resolution information. Ghiasi & Fowlkes (2016) leverage this assumption and propose to construct a Laplacian pyramid based on a fully convolutional network. Aggregating information at multiple scales allows them to successively refine the boundary reconstructed from lower-resolution layers. They achieve this by using skip connections from higher resolution feature maps and multiplicative confidence gating, penalizing noisy high-resolution outputs in regions where the low-resolution predictions have high confidence. With this approach Ghiasi&Fowlkes (2016) achieve competitive results on Cityscapes Table 4a.
- One of the best performing methods on Cityscapes was proposed by Zhao et al. (2016) using a pyramid scene parsing network, illustrated in Figure 13, to incorporate global context information into the pixel-level prediction task. Specifically, they apply a pyramid parsing module to the last convolutional layer of a CNN which fuses features of several pyramid scales to combine local and global context information. The resulting representation is fed into a convolution layer to obtain final per-pixel predictions.
- 虽然CNN层次较低层的激活映射缺少对象类别的特异性，但它们确实包含较高的空间分辨率信息。 Ghiasi＆Fowlkes（2016）利用这一假设，并提出构建基于完全卷积网络的拉普拉斯金字塔。 在多个尺度上聚合信息使得它们可以连续细化从较低分辨率层重建的边界。 他们通过使用来自较高分辨率特征图的跳跃连接和乘法置信门控来实现这一点，在低分辨率预测具有高置信度的地区惩罚嘈杂的高分辨率输出。 通过这种方式，Ghiasi＆Fowlkes（2016）在“城市风景”表4a中获得了竞争优势。
- Zhao等人提出了Cityscapes最佳表现方法之一。 （2016），使用如图13所示的金字塔场景解析网络，以将全局上下文信息并入到像素级预测任务中。 具体来说，它们将金字塔解析模块应用于CNN的最后卷积层，该融合层融合了几个金字塔尺度的特征以组合局部和全局上下文信息。 所得到的表示被馈送到卷积层以获得最终的每像素预测。

- Simonyan & Zisserman (2015) and Szegedy et al. (2015) have shown that the depth of a CNN is crucial to represent rich features. However, increasing the depth of a network lead to the saturation and degradation of the accuracy. He et al. (2016) propose deep residual learning framework (ResNet) to address this problem. They let each stacked layer learn a residual mapping instead of the original, unreferenced mapping. This allows them to train deeper networks with improving accuracy while plain networks (simply stacked networks) exhibited higher training errors. Pohlen et al. (2016) present a ResNet-like architecture that provides strong recognition performance while preserving high-resolution information throughout the entire network by combining two different processing streams. One stream passes through a sequence of pooling layers, whereas the other stream processes feature maps at full image resolution. The two processing streams are combined at the full image resolution using residuals. Wu et al. (2016b) have proposed a more efficient ResNet architecture by analyzing the effective depths of residual units. They point out that ResNets behave as linear ensembles of shallow networks. Based on this understanding they design a group of relatively shallow convolutional networks for the task of semantic image segmentation. While Pohlen et al. (2016) achieve competitive results on Cityscapes (Table 4a), Wu et al. (2016b) outperform all others in all measures besides the instance-weighted class-level IoU.
- Simonyan＆Zisserman（2015）和Szegedy等人（2015）表明，CNN的深度对于表现丰富的特征至关重要。然而，增加网络的深度导致饱和度和精度的降低。他等（2016）提出了深层次的残留学习框架（ResNet）来解决这个问题。它们使每个堆叠层学习残差映射，而不是原始的未引用映射。这允许他们在提高准确性的同时训练更深入的网络，而简单的网络（简单的堆叠网络）展示更高的训练误差。 Pohlen等人（2016）提出了一种类似ResNet的架构，通过组合两种不同的处理流，可以在整个网络中保持高分辨率信息，提供强大的识别性能。一个流通过一系列池池，而另一个流以完整图像分辨率处理特征图。两个处理流以全图像分辨率使用残差进行组合。吴等（2016b）通过分析剩余单位的有效深度，提出了一种更有效的ResNet架构。他们指出ResNets表现为浅网络的线性集合。基于这一理解，他们为语义图像分割任务设计了一组相对浅的卷积网络。而Pohlen等人（2016）在“城市景观”中获得了竞争优势（表4a），Wu et al。 （2016b）除了实例加权的类水平IoU之外的所有措施都胜过所有其他。

- **Conditional Random Fields**: A different way to address the needs of multi-scale inference and full resolution prediction is the combination of CNNs with CRF models. Chen et al.(2015b) propose to refine the label map obtained using a convolutional neural network using a fully connected CRF model (Krahenbuhl & Koltun (2011)). The CRF allows to capture fine details based on the raw RGB input which are missing in the CNN output due to the limited spatial accuracy of the CNN model. In similar spirit, Jampani et al. (2016) generalize bilateral filters and unroll the CRF program which allows for end-to-end training of the (generalized) filter parameters from data. This effiectively allows for reasoning over larger spatial regions within one convolutional layer by leveraging input features as a guiding signal.
- Inspired by higher order CRFs for semantic segmentation, Gadde et al. (2016a) propose a new Bilateral Inception module for CNN architectures as an alternative to structured CNNs and CRF techniques. They use the assumption that pixels which are spatially and photometrically similar are more likely to have the same label. This allows them to directly learn long-range interactions, thereby removing the need for post-processing using CRF models. Specifically, the proposed modules propagate edge-aware information between distant pixels based on their spatial and color similarity, incorporating the spatial layout of superpixels. Propagation of information is achieved by applying bilateral filters with Gaussian kernels at various scales.
- 条件随机场：解决多尺度推理和全分辨率预测需求的不同方法是将CNN与CRF模型相结合。 Chen等人（2015b）提出使用完全连接的CRF模型（Krahenbuhl＆Koltun（2011））来改进使用卷积神经网络获得的标签图。 由于CNN模型的空间精度有限，CRF可以根据CNN输出中缺少的原始RGB输入来捕获细节。 类似的精神，Jampani等人 （2016）推广双边筛选器并展开CRF程序，允许对来自数据的（广义）过滤器参数进行端对端培训。 这有效地允许通过利用输入特征作为引导信号来推理在一个卷积层内的较大空间区域。
- 灵感来自语义分割的高阶CRF，Gadde et al。 （2016a）提出了一种用于CNN架构的新的双边入局模块，作为结构化CNN和CRF技术的替代方案。 他们使用这样的假设：空间和光度相似的像素更可能具有相同的标签。 这允许他们直接学习远程交互，从而消除了使用CRF模型进行后处理的需要。 具体地说，所提出的模块基于它们的空间和颜色相似性，在远距离像素之间传播边缘感知信息，并结合超像素的空间布局。 通过在各种尺度上应用具有高斯核的双边滤波器来实现信息的传播。

- **Discussion**: The focus on multi-scale inference of recent methods led to impressive results in pixel-level semantic segmentation on Cityscapes. Today, the top methods in Cityscapes Table 4b reach an impressive IoU of almost 81% over classes and 91% over categories. In contrast, the instance-weighted IoU is always below 58% over classes and 80% over categories. This indicates that semantic segmentation works well with instances covering large image areas but is still problematic with instances covering small regions. Similarly to the detection in low resolutions discussed in Section 5.6, small regions provide only little information to assign the correct label. Furthermore segmenting out small, and possibly occluded objects is a challenging task which might require novel approaches to jointly perform depth estimation and depth-adaptive recognition.
- 讨论：对近期方法的多尺度推论的关注导致了Cityscapes中像素级语义分割的令人印象深刻的结果。 今天，“城市风景”表4b中的顶级方法达到令人印象深刻的几率，达到81％，超过类别91％。 相比之下，实例加权IoU总是低于58％，超过类别80％。 这表明语义分割对于覆盖大图像区域的实例很好，但是对于覆盖小区域的实例仍然是有问题的。 类似于5.6节中讨论的低分辨率检测，小区域只提供很少的信息来分配正确的标签。 此外，分割出小的可能闭塞的物体是一项具有挑战性的任务，可能需要新颖的方法来共同执行深度估计和深度自适应识别。

- 6.1. Semantic Instance Segmentation  语义实例分割
- The goal of semantic instance segmentation is simultaneous detection, segmentation and classification of every individual object in an image. Unlike semantic segmentation, it provides information about the position, semantics, shape and count of individual objects, and therefore has many applications in autonomous driving. For the task of semantic instance segmentation, there exist two major lines of research: Proposal-based and proposal-free instance segmentation.
- In Table 4b we show the leaderboard of semantic instance segmentation methods on the Cityscapes dataset. The performance is assessed with the average precision on the region level averaged across a range of overlap thresholds (AP), for an overlap value of 50 % (AP 50%) and for objects within 100 m and 50 m (AP 100m, AP 50m).
- 语义实例分割的目标是对图像中每个单独对象进行同时检测，分割和分类。 与语义分割不同，它提供了关于单个对象的位置，语义，形状和数量的信息，因此在自主驾驶中具有许多应用。 对于语义实例分割的任务，存在两个主要研究领域：基于提案和无提议的实例分割。
- 在表4b中，我们在Cityscapes数据集上显示语义实例分割方法的排行榜。 对于重叠值为50％（AP 50％）和100米和50米范围内的物体（AP 100米，AP 50米），区域水平平均精度在平均重叠阈值（AP）范围内进行评估）。

- **Proposal-based Instance Segmentation**: Proposal-based instance segmentation methods extract class-agnostic proposals which are classified as an instance of a certain semantic class in order to obtain pixel-level instance masks. Region proposals like Multiscale Combinatorial Grouping (Arbel´aez et al. (2014)) can be directly used as instance segments. Coarser representations such as bounding boxes need further refinement to obtain the instance mask. Unfortunately, proposal-based algorithms are slow at inference time due to the computationally expensive proposal generation step. To avoid this bottleneck, Dai et al. (2016) propose a fully convolutional network with three stages. They extract box proposals, use shared features to refine these to segments, and finally classify them into semantic categories. The causal relations between the outputs of the stages complicate training of the multi-task cascade. However, the authors show how these difficulties can be overcome using a differentiable layer which allows for training the whole model in an end-to-end fashion.
- Proposal-based instance segmentation methods that use proposals in the form of bounding boxes to predict a binary segmentation mask are sensitive to errors in the proposal generation process including wrongly scaled or shifted bounding boxes. To tackle this problem, Hayder et al. (2016) present a new object representation. More specifically, they propose a shape aware object mask network that predicts a binary mask for each bounding box proposal, potentially extending beyond the box itself. They integrate the object mask network into the Multitask Network Cascade framework of Dai et al. (2016) by replacing the original mask prediction stage. The shape aware approach is the second best performing method on Cityscapes(Table 4b).
- 基于提案的实例分段：基于提案的实例分段方法提取类别不可知的提案，被分类为某个语义类的实例，以获得像素级实例掩码。区域建议如多尺度组合分组（Arbel'aez et al。（2014））可直接用作实例。较粗糙的表示，例如边界框需要进一步细化以获得实例掩码。不幸的是，由于计算昂贵的建议生成步骤，基于提议的算法在推理时间较慢。为了避免这个瓶颈，Dai等（2016）提出了一个具有三个阶段的完全卷积网络。他们提取框提案，使用共享功能将这些细分为细分，最后将它们分类为语义类别。这些阶段的产出之间的因果关系使多任务级联的训练复杂化。然而，作者展示了如何使用可微分层来克服这些难题，从而可以以端对端的方式对整个模型进行训练。
- 使用基于边界框形式的提案以预测二进制分割掩码的基于投标的实例分割方法对提案生成过程中的错误（包括错误缩放或移位的边界框）敏感。 为了解决这个问题，Hayder等 （2016）提出了一个新的对象表示。 更具体地说，它们提出了一种形状感知对象掩模网络，其预测每个边界框提案的二进制掩码，潜在地延伸超出框本身。 他们将对象掩码网络集成到Dai等人的多任务网络级联框架中。 （2016）通过替换原始的掩模预测阶段。 形状感知方法是Cityscapes第二好的表现方法（表4b）。

- Proposal-free Instance Segmentation: Recently, a number of alternative methods to proposal-based instance segmentation have been proposed in the literature. These methods jointly infer the segmentation and the semantic category of individual instances by casting instance segmentation directly as a pixel labeling task.
- Zhang et al. (2015, 2016c) train a fully convolutional neural networks (FCN) to directly predict pixel-level instance segmentation while the instance ID encodes a depth ordering. They improve the predictions and enforce consistency with a subsequent Markov Random Field. Uhrig et al. (2016) propose a method based on FCN to jointly predict semantic segmentation as well as depth and an instance-based direction relative to the centroid of each instance. The instance segmentation pipeline is illustrated in Figure 14. However, they require ground-truth depth data for training their model. Kirillov et al. (2016) present a proposal-free method which combines semantic segmentation and object boundary detection via global reasoning in a multi-cut formulation to infer semantic instance segmentation. Bai & Urtasun (2016) combine intuitions from classical watershed transform and deep learning to create an energy map where the basins corresponds to object instances. This allows them to cut at a single energy level to obtain an pixel-level instance segmentation. Kirillov et al. (2016) and Bai & Urtasun (2016) both achieve competitive results on Cityscapes (Table 4b). However, Arnab & Torr (2017) outperform all others by feeding an initial semantic segmentation into an instance subnetwork. Specifically, the initial category-level segmentation is used along cues from the output of an object detector within an end-to-end CRF to predict pixel-level instances.
- 无提议实例分割：最近，文献中提出了一些基于提案的实例分割的替代方法。 这些方法通过直接将实例分割作为像素标注任务来共同推断单个实例的分割和语义类别。
- Zhang et al（2015，2016c）训练完全卷积神经网络（FCN）直接预测像素级实例分割，而实例ID编码深度排序。他们改进了预测，并强制与随后的马尔可夫随机场一致。 Uhrig等人（2016）提出了一种基于FCN的方法来共同预测语义分割以及相对于每个实例的重心的深度和基于实例的方向。实例分割管线如图14所示。但是，它们需要用于训练其模型的地面真相深度数据。基里洛夫等人（2016）提出了一种无提议方法，通过全局推理将语义分割和对象边界检测结合在一个多切分公式中，以推断语义实例分割。 Bai＆Urtasun（2016）结合了经典流域变换和深度学习的直觉，创建了一个能量图，其中盆地对应于对象实例。这允许他们在单个能级切割以获得像素级的实例分割。基里洛夫等人（2016年）和Bai＆Urtasun（2016年）都在Cityscapes上取得了竞争力的结果（表4b）。然而，Arnab＆Torr（2017）通过将初始语义分割提供给实例子网络来胜过所有其他方式。具体来说，初始类别级别的分段被用于端到端CRF内的对象检测器输出的提示，以预测像素级实例。

- **Discussion**: The instance segmentation task is much more difficult than the semantic segmentation task. Each instance need to be carefully annotated separately whereas in semantic segmentation groups of one semantic class can be annotated together when they occur next to each other. In addition, the number of instance varies greatly between different images. In the autonomous driving context often a wide view is present. Therefore, a large number of instances that appear are rather small in the image making them challenging to detect. In contrast to bounding boxes discussed in Section 5.6, the exact shape of each object instance needs to be inferred in this task. For these reasons, the state-of-the-art is still struggling with the Cityscape dataset (Table 4b) reaching an average precision of 20% or less.
- 讨论：实例分割任务比语义分割任务困难得多。 每个实例需要分别仔细注释，而在语义分割中，一个语义类的组可以在彼此相邻发生时一起注释。 另外，实例的数量在不同的图像之间变化很大。 在自主驾驶环境中，通常有广泛的观点。 因此，出现的大量实例在图像中相当小，使得它们具有挑战性。 与第5.6节讨论的边界框相反，每个对象实例的确切形状需要在此任务中推断出来。 由于这些原因，最先进的技术仍然在与Cityscape数据集（表4b）挣扎，达到20％以下的平均精度。...

- 6.2. Label Propagation 标签传播
- Creating large-scale image datasets with highly accurate pixel-level annotations is labor intensive, and thus very expensive to obtain the desired degree of quality. Semi-supervised methods for annotation of video sequences can help to reduce this cost. Compared to annotating individual images, video sequences offer the advantage of temporal consistency between consecutive frames. Label propagation techniques take advantage of this fact by propagating annotations from a small set of annotated keyframes to all unlabeled frames based on color
information and motion estimates.
- 创建具有高精度像素级注解的大规模图像数据集是劳动密集型的，因此获得所需的质量程度非常昂贵。 用于注释视频序列的半监督方法可以帮助降低成本。 与注释单个图像相比，视频序列提供连续帧之间的时间一致性的优点。 标签传播技术通过将注释从一小部分注释关键帧传播到基于颜色的所有未标记的帧来利用这一事实
信息和运动估计。

- Towards this goal, Badrinarayanan et al. (2010) propose a coupled Bayesian network for joint modeling of the image sequence and pixel-wise labels. Specifically, they employ a propagation scheme based on correspondences obtained from image patch based similarities and semantically consistent regions to transfer label information to unlabeled frames between annotated keyframes. Budvytis et al. (2010) extend this approach by proposing a hybrid model of the generative propagation introduced in Badrinarayanan et al. (2010) as well as a discriminative classification stage which tackles occlusions and dis-occlusions, and allows to propagate over larger time frames. To correct erroneous label propagation, Badrinarayanan et al. (2014) propose a superpixel based mixture-of-tree model for temporal correlation. Vijayanarasimhan & Grauman (2012) tackle the
problem of selecting the most promising keyframes for manual labeling such that the expected propagation error is minimized.
- 为实现这一目标，Badrinarayanan等 （2010）提出了一种耦合贝叶斯网络，用于图像序列和像素方向标签的联合建模。 具体地说，它们采用基于从基于图像块的相似性和语义上一致的区域获得的对应的传播方案来将标签信息转移到带标注的关键帧之间的未标记的帧。 Budvytis等人 （2010）通过提出在Badrinarayanan等人引入的生成繁殖的混合模型来扩展这种方法。 （2010）以及解决闭塞和闭塞的歧视性分类阶段，并允许在更大的时间范围内传播。 为了纠正错误的标签传播，Badrinarayanan等 （2014）提出了一种用于时间相关的基于超像素的树混合模型。 Vijayanarasimhan＆Grauman（2012）解决了
选择用于手动标签的最有希望的关键帧的问题，使得期望的传播误差最小化。

- While the aforementioned methods transfer annotations in 2D, Chen et al. (2014); Xie et al. (2016) propose to annotate directly in 3D and then transfer these annotations into the image domain. Given a source of 3D information (e.g., stereo, laser), these approaches are able to produce improved semantic accuracy and time coherent labels while limiting annotation costs. Towards this goal, Chen et al. (2014) use annotations from KITTI (Geiger et al. (2013)) and leverage 3D car CAD models to infer separate figure-ground segmentations for all cars in the image. In contrast, Xie et al. (2016) reason jointly about all objects in the scene and also handle categories for which CAD models or 3D point measurements are unavailable. To this end, they propose a non-local CRF model which reasons jointly about semantic and instance labels of all 3D points and pixels in the image.
- 虽然上述方法在2D中传输注释，但Chen等（2014）; 谢等人 （2016）建议直接在3D中注释，然后将这些注释传输到图像域中。 给定3D信息的来源（例如，立体声，激光），这些方法能够产生改进的语义准确度和时间相干标签，同时限制注释成本。 为了实现这一目标，Chen et al。 （2014）使用KITTI（Geiger等人（2013））的注释，并利用3D汽车CAD模型推测图像中所有汽车的独立图形分割。 相比之下，谢等人 （2016）共同理解场景中的所有对象，并处理CAD模型或3D点测量不可用的类别。 为此，他们提出了一种非本地CRF模型，它们共同对图像中所有3D点和像素的语义和实例标签进行了理解。

- 6.3. Semantic Segmentation with Multiple Frames 多帧语义分割
- Semantic segmentation from movable platforms such as autonomous vehicles has become an active area of research due to the need of autonomous systems for recognizing their surrounding environment. As such systems are typically equipped with video cameras, temporal correlation between adjacent frames can be exploited to improve segmentation accuracy, efficiency and robustness.
- Towards this goal, Floros & Leibe (2012) propose graphical models operating on video sequences in order to enforce temporal consistency between frames. Specifically, they have proposed a CRF where temporal consistency between consecutive video frames is ensured by linking corresponding image pixels to the inferred 3D scene points obtained by Structure from Motion (SfM). Compared to an image-only baseline they achieve an improved segmentation performance and observe a good generalization to varying image conditions.
- 自动车辆等移动平台的语义分割已经成为一个活跃的研究领域，由于需要自主系统来识别周围的环境。 由于这样的系统通常配备有摄像机，因此可以利用相邻帧之间的时间相关性来提高分割精度，效率和鲁棒性。
- 为了实现这一目标，Floros＆Leibe（2012）提出了对视频序列进行操作的图形模型，以便强化帧之间的时间一致性。 具体来说，他们提出了一种CRF，其中通过将对应的图像像素与通过运动结构（SfM）获得的推断的3D场景点相关联来确保连续视频帧之间的时间一致性。 与仅基于图像的基线相比，它们实现了改进的分割性能，并观察到不同图像条件的良好泛化。

- 3D reconstruction works relatively well for static scenes but is still an open problem in dynamic scenes. Feature-sensitive CRF models have been very successful in semantic image segmentation but the considered distance measure does not appropriately model spatiotemporal correspondences. The presence of both scene and camera motion makes temporal association in videos a challenging task. Because of the possibility of significant optical flow due to such motions, Euclidean distance in the space-time volume is not a good surrogate for correspondence. To tackle this problem, Kundu et al. (2016) propose a method for optimizing the feature space of a dense CRF for spatiotemporal regularization. Specifically, the feature space is optimized such that distances between features associated with corresponding points are minimized using correspondences from optical flow. The resulting mapping is exploited by the CRF to achieve long-range regularization over the entire video volume.
- 3D重建对于静态场景效果较好，但在动态场景中仍然是一个开放的问题。 功能敏感的CRF模型在语义图像分割中取得了非常成功，但考虑的距离度量并不能适当地模拟时空对应关系。 场景和相机运动的存在使视频中的时间关联成为一项具有挑战性的任务。 由于这种运动的可能性很大，所以在时空体积中的欧几里德距离不是很好的对应关系。 为了解决这个问题，Kundu et al。 （2016）提出了一种用于优化密集CRF的时空正则化特征空间的方法。 具体地，特征空间被优化，使得使用来自光流的对应来最小化与对应点相关联的特征之间的距离。 所得到的映射由CRF利用，以在整个视频卷上实现远程正则化。

- 6.4. Semantic Segmentation of 3D Data 3D数据的语义分割
- Autonomous systems need to recognize their surroundings to identify and interact with objects of interest. While the problem of semantic object labeling has been studied extensively, most of these algorithms work in the 2D image domain where each pixel in the image is labeled with a semantic category such as car, road or pavement. However, 2D images lack important information such as the 3D shape and scale of objects which are strong cues for object class segmentation and facilitate the detection and separation of individual object instances.
- Sengupta et al. (2012) present an approach to generate a semantic overhead map of an urban scene from street level images. They formulate the problem using two CRFs. The first is used for semantic image segmentation of the street view images treating each image independently. Each street view image is then related by a geometrical function that back projects a region from the image into the overhead map. The outputs of this phase are then aggregated over many images to form the input for a second CRF producing a labeling of the ground plane. However, their method does not go beyond the flat world assumption to deliver dense semantic reconstruction using multiple street view images.
- 自治系统需要识别他们的周围环境，以识别和与感兴趣的对象进行交互。 虽然已经广泛研究了语义对象标注的问题，但是大多数这些算法在2D图像域中工作，其中图像中的每个像素被标记有诸如汽车，道路或路面的语义类别。 然而，2D图像缺少诸如对象类别分割的强线索的对象的3D形状和尺度等重要信息，并且便于各个对象实例的检测和分离。
- Sengupta（2012）提出了一种从街道图像生成城市场景的语义开销图的方法。 他们使用两个CRF制定问题。 第一个用于独立处理每个图像的街景图像的语义图像分割。 然后，每个街景图像通过几何函数相关联，后者将区域从图像投影到架空地图中。 然后将该相的输出聚集在许多图像上以形成用于产生接地平面的标记的第二CRF的输入。 然而，他们的方法不超越平面世界的假设，使用多个街景图像提供密集的语义重建。

- Towards this goal, Sengupta et al. (2013) propose an approach illustrated in Figure 15 where a dense semantic 3D reconstruction is generated using multiple street view images. They use visual odometry for ego-motion estimation according to which depth-maps generated from input stereo image pairs are fused. This allows them to generate a volumetric 3D representation of the scene. In parallel, input images are semantically classified using a CRF model. The results of segmentation are then aggregated across the sequence to generate the final 3D semantic model. However, the object labeling is performed in the image domain and then projected onto the model. As a result, these methods fail to fully exploit all structural constraints present in road scenes.
- Valentin et al. (2013) tackle the problem of semantic scene reconstruction in 3D space by combining both structural and appearance cues. They use input depth estimates to generate a triangulated mesh representation of the scene and apply a cascaded classifier to learn geometric cues from the mesh and appearance cues from images. Subsequently, they solve for the labeling in 3D by defining a CRF over the scene mesh. However, they approach requires inference on the whole mesh an does not allow for incrementally adding information in an online setting as common in the autonomous driving context.
- Hackel et al. (2016) propose a fast semantic segmentation approach for 3D point clouds with strongly varying densities. They construct approximate multi-scale neighborhoods by downsampling the entire point cloud, to generate a multi-scale pyramid with decreasing density, and searching for the nearest neighbors per scale. This scheme allows to extract rich feature representation, that captures the geometry in a point’s local neighborhood such as roughness, surface orientation, height over ground and others, in very little time. A random forest classifier finally predicts the class-conditional probabilities. The proposed method can process point clouds with many million of points in a matter of minutes.
- 为了实现这一目标，Sengupta等 （2013）提出了图15所示的方法，其中使用多个街景图像生成密集的语义3D重建。 根据从输入立体图像对生成的深度图被融合，他们使用视觉测距法进行自我运动估计。 这允许他们生成场景的体积3D表示。 并行地，输入图像使用CRF模型进行语义分类。 然后，分割结果跨序列进行聚合，以生成最终的3D语义模型。 然而，对象标注在图像域中执行，然后投影到模型上。 因此，这些方法无法充分利用道路场景中存在的所有结构性限制。
- 瓦伦丁等人 （2013）通过结合结构和外观线索来解决3D空间中语义场景重构的问题。 他们使用输入深度估计来生成场景的三角网格表示，并应用级联分类器从网格和图像中的外观线索学习几何提示。 随后，他们通过在场景网格上定义CRF来解决3D中的标签问题。 然而，他们的方法需要对整个网格进行推理，不允许在自主驾驶环境中常见的在线设置中逐渐添加信息。
- Hackel等人 （2016）提出了一种具有强烈变化密度的3D点云的快速语义分割方法。 它们通过对整个点云进行下采样来构造近似的多尺度邻域，以生成具有减小密度的多尺度金字塔，并且每个尺度搜索最近的邻居。 该方案允许提取丰富的特征表示，其在非常少的时间内捕获点的本地邻域中的几何，例如粗糙度，表面取向，地面高度等。 随机森林分类器最终预测了类条件概率。 所提出的方法可以在几分钟内处理具有数百万点的点云。

- **Online Methods**: Vineet et al. (2015) propose an end-to-end system which processes data incrementally and performs realtime dense stereo reconstruction and semantic segmentation of outdoor environments. They achieve this using voxel hashing (Nießner et al. (2013)), a hash-table-driven 3D volumetric representation that ignores unoccupied space in the target environment. Furthermore, they employ an online volumetric mean-field inference technique that incrementally refines the voxel labeling. They are able to achieve semantic reconstruction at real-time rates by harnessing the processing power of modern GPUs.
- McCormac et al. (2016) propose a pipeline for dense 3D semantic mapping designed to work online by fusing semantic predictions of a CNN with the geometric information from a SLAM system (ElasticFusion by Whelan et al. (2015)). Specifically, ElasticFusion provides correspondences between 2D frames and a globally consistent map of surfels. Furthermore, they use a Bayesian update scheme which computes the class probabilities for each surfel based on the CNN’s predictions. The advantage of using surfel-based surface representations is their ability to fuse long-range information, for instance after a loop closure has been detected and the poses have been corrected accordingly.
- 在线方法：Vineet et al。 （2015）提出了一种端到端系统，可以逐步处理数据，并进行户外环境的实时密集立体重构和语义分割。 他们使用体素散列（Nießneret al。（2013））实现了这一点，这是一种散列表驱动的3D体积表示，忽略了目标环境中未占用的空间。 此外，他们采用在线体积平均场推理技术，逐步优化体素标签。 通过利用现代GPU的处理能力，他们能够以实时速率实现语义重建。
- McCormac等人 （2016）提出了一种用于密集3D语义映射的流水线，旨在通过将CNN的语义预测与SLAM系统的几何信息（Whelan等人（2015）的ElasticFusion）融合在线工作）。 具体来说，ElasticFusion可以提供2D帧与全局一致的冲浪图之间的对应关系。 此外，他们使用贝叶斯更新方案，其基于CNN的预测计算每个冲浪的类概率。 使用基于浮标的表面表示的优点是它们能够融合长距离信息，例如在检测到闭环并且相应地纠正姿态之后。

- **3D CNN**: While convolutional networks have proven very successful segmenting 2D images semantically, there exists relatively little work on labeling 3D data using convolutional networks. Huang & You (2016) propose a framework for labeling 3D point cloud data using a 3D Convolutional Neural Network (3D-CNN). Specifically, they compute 3D occupancy grids of size 203 centered at a set of randomly generated key points. The occupancy and the labels form the input to a 3D CNN, which is composed of convolutional layers, max-pooling layers, a fully connected layer and a logistic regression layer. Due to the dense voxel representation, 3D CNNs are only able to process voxel grids of very coarse resolution considering the memory limitations of modern GPUs.
- To alleviate this problem, Riegler et al. (2017) propose Oct-Nets, a 3D convolutional network, that allows for training deep architectures at significantly higher resolutions. They build on the observation that 3D data (e.g., point clouds, meshes) is often sparse in nature. The proposed OctNet exploits this sparsity property by hierarchically partitioning the 3D space into a set of octrees and applying pooling in a data-adaptive fashion. This leads to a reduction in computational and memory requirements as the convolutional network operations are defined on the structure of these trees and thus can dynamically allocate resources depending on the structure of the input.
- 3D CNN：虽然卷积网络已经证明非常成功地在语义上分割2D图像，但是使用卷积网络标记3D数据的工作相对较少。 Huang＆You（2016）提出了一个使用3D卷积神经网络（3D-CNN）标记3D点云数据的框架。 具体地说，它们以一组随机产生的关键点为中心计算大小为203的3D占用网格。 占有率和标签形成3D CNN的输入，该三维CNN由卷积层，最大池层，完全连接层和逻辑回归层组成。 考虑到现代GPU的内存限制，由于密集的体素表示，3D CNN只能处理非常粗略的分辨率的体素网格。
- 为了减轻这个问题，Riegler等 （2017）提出了一个3D卷积网络Oct-Nets，它允许以更高的分辨率训练深层架构。 它们建立在3D数据（例如，点云，网格）本质上经常稀疏的观察的基础上。 提出的OctNet通过将3D空间分层划分成一组八叉树并以数据自适应方式应用池来利用此稀疏属性。 这导致计算和存储器需求的减少，因为卷积网络操作在这些树的结构上被定义，并且因此可以根据输入的结构而动态地分配资源。

- 6.5. Semantic Segmentation of Street Side Views 街景视图的语义分割
- One important application of semantic segmentation for autonomous vehicles is to segment street-side images (i.e., building facades) into its components (wall, door, window, vegetation, balcony, store, mailbox etc.). Such semantic segmentations are useful for accurate 3D reconstruction, memory-efficient 3D mapping, robust localization as well as path planning.
- Xiao & Quan (2009) propose a multi-view semantic segmentation framework for images captured by a camera mounted on a car driving along the street. Specifically, they define a pairwise MRF across superpixels in multiple views, where the unary terms are based on 2D and 3D features. Furthermore, they minimize color differences for spatial smoothness and use dense correspondences to enforce smoothness across different views. Existing approaches for multi-view semantic segmentation typically require labeling all pixels in all images used for the 3D model which, depending on the semantic segmentation algorithm, can be prohibitively slow. To increase efficiency, Riemenschneider et al. (2014) exploit the inherent redundancy in the labeling of all overlapping images used for the 3D model. They propose an approach that exploits the geometry of a 3D mesh model obtained from multi-view reconstruction to predict the best view for each face of the mesh before performing the actual semantic image labeling. This allows them to accelerate the pipeline by two orders of magnitude.
- 用于自主车辆的语义分割的一个重要应用是将街道侧图像（即，建筑物立面）分割成其部件（墙壁，门，窗，植被，阳台，商店，邮箱等）。 这样的语义分割对于精确的3D重建，高效的内存记录，三维映射，鲁棒的本地化以及路径规划都是有用的。
- Xiao＆Quan（2009）提出了一种用于安装在沿着街道行驶的汽车上拍摄的照相机拍摄的图像的多视图语义分割框架。具体来说，它们在多个视图中定义跨越多个像素的成对MRF，其中一元项基于2D和3D特征。此外，它们使颜色差异最小化，使空间平滑度和密集的对应关系在不同视图之间实现平滑度。用于多视图语义分割的现有方法通常需要标注用于3D模型的所有图像中的所有像素，其取决于语义分割算法可能会非常缓慢。为了提高效率，Riemenschneider等（2014）利用3D模型中所有重叠图像的标签固有冗余。他们提出了一种方法，利用从多视图重建获得的3D网格模型的几何，以便在执行实际的语义图像标记之前预测网格的每个面的最佳视图。这允许他们加速管道两个数量级。

- Gadde et al. (2016b) describe a system for segmentation of 2D images and 3D point clouds of building facades that is fast at inference time and is easily adaptable to new datasets. In contrast to existing methods which exploit the structure of facade images by imposing strong priors, they implement a sequence of boosted decision tree classifiers, that are stacked using auto-context features and learn all correlations from data.
- Xiao et al. (2009) propose another method to generate streetside 3D photo-realistic models from images captured at ground level. In particular, they segment each image into semantically meaningful areas, such as building, sky, ground, vegetation or car. Then, they partition buildings into independent blocks and employ a regularization term by exploiting architectural priors in the orthographic view for inference. This allows them to cope with noisy and missing reconstructed 3D data and produces visually compelling results.
- Gadde等 （2016b）描述了一种在推理时间快的建筑立面的2D图像和3D点云的分割系统，并且易于适应新的数据集。 与通过强加优先权利用立面图像结构的现有方法相比，它们实现了一系列增强的决策树分类器，它们使用自动上下文特征进行堆叠，并从数据中学习所有相关性。
- Xiao等 （2009）提出了另一种方法，从地面捕获的图像生成街头3D逼真模型。 特别是，它们将每个图像分割成语义有意义的领域，例如建筑，天空，地面，植被或汽车。 然后，他们将建筑分成独立的块，并采用正则化术语，通过在正交视图中利用建筑先验来推断。 这使得它们能够应对嘈杂和缺失的重建3D数据，并产生视觉上令人信服的结果。

- Mathias et al. (2016) propose a flexible 3-layered method for segmentation of building facades which avoids the need for explicitly specifying a grammar. First, the facade is segmented into semantic classes which are combined with the output of detectors for architectural elements such as windows and door. Finally, weak architectural priors such as alignment, symmetry, co-occurrence are proposed which encourage the reconstruction to be architecturally consistent. The complete pipeline is illustrated in Figure 16. In contrast to the majority of semantic facade modeling approaches that treat facades as planar surfaces, Martinovi´c et al. (2015) propose an approach for facade modeling which operates directly in 3D. As their approach avoids time-consuming conversions between 2D and 3D representations, they obtain substantially shorter runtime. Specifically, they reconstruct a semi-dense 3D point cloud using SfM and classify each point using a Random Forest classifier trained on 3D features. Afterwards, they separate individual facades based on their semantic structure and impose weak architectural priors.
- Mathias等人（2016）提出了一种灵活的3层分层建筑立面分割方法，避免了明确指定语法的需要。首先，门面被分割成语义类，它们与建筑元素如窗户和门的检测器的输出相结合。最后，提出了弱化的建筑先验，如对齐，对称，同现，鼓励重建在架构上保持一致。完整的流水线如图16所示。与将正面视为平面曲面的大多数语义外观建模方法相反，Martinovi'c et al。 （2015）提出了一种立体建模方法，可直接在3D中进行操作。由于他们的方法避免了2D和3D表示之间耗时的转换，所以它们获得了更短的运行时间。具体来说，他们使用SfM重建半密度3D点云，并使用经过3D特征训练的随机森林分类器对每个点进行分类。之后，他们根据自己的语义结构分开了各个立面，强化了体系结构先验。

- 6.6. Semantic Segmentation of Aerial Images 空中图像的语义分割
- The aim of aerial image parsing is the automated extraction of urban objects from data acquired by airborne sensors. The need for accurate and detailed information for urban objects such as roads is rapidly increasing because of its applications in navigation of autonomous driving systems. For example, aerial image parses can be used to automatically build road maps (even in remote areas) and keep them up-to-date. Furthermore, information from aerial images can be used for localization. However, the problem is challenging because of the heterogeneous appearance of objects like buildings, streets, trees and cars which results in high intra-class variance but low inter-class variance. Furthermore, the complex structure of the prior complicates inference. For instance, roads must form a connected network of thin segments with slowly changing curvatures which meet at junctions. This type of prior knowledge is more challenging to formalize and integrate into a structured prediction formulation than standard smoothness assumptions.
- 空中图像解析的目的是通过机载传感器获取的数据自动提取城市对象。由于其在自主驾驶系统的导航中的应用，对诸如道路等城市对象的准确详细信息的需求正在迅速增长。例如，航空图像解析可以用于自动构建路线图（甚至在偏远地区），并保持最新。此外，来自航空图像的信息可以用于定位。然而，这个问题是具有挑战性的，因为像建筑物，街道，树木和汽车这样的物体的外观异乎寻常，导致班内差异很大，班级间差异较小。此外，复杂的结构先前复杂化推理。例如，道路必须形成具有缓慢变化的曲率的薄段的连接网络，其在交叉点处相遇。这种类型的先验知识比标准平滑度假设更具挑战性，将其形式化并结合到结构化预测公式中。

- Wegner et al. (2013) propose a CRF formulation for road labeling in which the prior is represented by cliques that connect sets of superpixels along straight line segments. Specifically, they formulate the constraints as high-order cliques with asymmetric $P^N$-potentials which express a preference to assign all rather than just some of their constituent superpixels to the road class. This allows the road likelihood to be amplified for thin chains while still being amenable to efficient inference using graph cuts. Wegner et al. (2015) also model the road network using a CRF with long-range, higher-order cliques. However, unlike Wegner et al. (2013), they allow for arbitrarily shaped segments which adapt to more complex road shapes by searching for putative roads with minimum cost paths based on local features. Montoya et al. (2015) extend this formulation to multi-label classification of aerial images with class-specific priors for buildings and roads. In addition to the road network prior of Wegner et al. (2015), they introduce a second higher order potential for cliques specific to buildings.
- Wegner et al。 （2013）提出了一种用于道路标记的CRF公式，其中先前由通过沿直线段连接超像素组的分支来表示。具体来说，他们将约束作为具有不对称的PN-totentials的高阶组合，它们表示偏好将所有而不仅仅是他们的组成超像素的一些分配给道路类。这允许对于细链可以扩大道路的可能性，同时仍然可以使用图形切割进行有效的推断。 Wegner et al。 （2015年）还使用具有远程，高阶组合的CRF对道路网进行建模。然而，与Wegner等人不同（2013），它们允许通过基于局部特征搜索具有最小成本路径的推定道路来适应更复杂的道路形状的任意形状的段。 Montoya等人（2015年）将此方案扩展到具有类别特色的建筑物和道路先进的航空图像多标签分类。除了Wegner等人之前的道路网络之外（2015年），它们为特定建筑物的集团引入了第二高的潜力。

- In contrast to other methods, Verdie & Lafarge (2014) propose the application of Markov point processes for recovering specific structures from images, including road networks. Markov point processes are a generalization of traditional MRFs which can address object recognition problems by directly manipulating parametric entities such as line segments, whereas MRFs are restricted to labeling problems. Importantly, they implicitly solve the model-selection problem, i.e., they allow for an arbitrary number of variables in the MRF which can be associated with the parameters of the objects of interest. Specifically for road segmentation, the parametric representation of road segments is chosen as a point at the center of mass of the segment and two additional parameters modeling the length and orientation of the road segment.
- 与其他方法相反，Verdie＆Lafarge（2014）提出了马尔科夫点处理从图像恢复特定结构的应用，包括道路网络。 马可夫点过程是传统MRF的概括，可以通过直接操纵诸如线段的参数实体来解决对象识别问题，而MRF仅限于标签问题。 重要的是，它们隐含地解决了模型选择问题，即它们允许可以与感兴趣对象的参数相关联的MRF中的任意数量的变量。 特别针对道路分割，道路段的参数化表示被选择为段的质心点，以及两个额外的参数来建模道路段的长度和方位。

- **Aerial Image Parsing using Maps**: Instead of framing the problem of detecting topologically correct road network as a semantic segmentation problem, Mattyus et al. (2015) exploit map information from OpenStreetMap (OSM)27. OSM is a collection of roads, trails, caf´es, railway stations and much more all over the world contributed and maintained by a community of mappers. It provides freely available maps of the road topology in the form of piece-wise linear road segments. Given a road map from OSM, Mattyus et al. (2015) propose an MRF which reasons about the location of the road centerline and its width for each road segment in OSM. In addition, they incorporate smoothness between consecutive line segments by encouraging their widths to be similar. This formulation has the advantage that it enables efficient inference while restricting the road topology to the OSM map.
- 使用地图的空中图像分析：Mattyus等人，不是将拓扑正确的道路网络检测为语义分割问题的问题。 （2015）利用OpenStreetMap（OSM）27的地图信息。 OSM是一个道路，小径，咖啡馆，火车站的集合，还有世界各地由映射者社区贡献和维护的。 它以分段线性路段的形式提供道路拓扑的免费地图。 给出了OSM的路线图，Mattyus等人 （2015）提出一个MRF，说明道路中心线的位置及其在OSM中每个路段的宽度的原因。 此外，它们通过鼓励它们的宽度相似而在连续线段之间融合平滑度。 该公式的优点是能够有效地推理，同时将道路拓扑限制到OSM地图。

- **Fine-grained Image Parsing with Aerial-to-ground Reasoning**: While aerial images provide full coverage of a significant portion of the world, they are of much lower resolution than ground images. In aerial imagery the resolution relates to the ground area covered by one pixel. Whereas 1 meter resolution is already a high resolution for satellite imagery, the standard resolution for most image databases (e.g. Google Earth) is 12 inch. Resolutions of 6 to 1 inch are considered high resolutions for aerial imagery and are usually not publicly available. This makes fine grained segmentation from aerial images a challenging problem. On the other hand, ground images provide additional information which enables fine-grained semantic segmentation. Motivated by the complementary nature of these cues, several methods for fine grained segmentation have been recently proposed which jointly reason about co-located aerial and ground image pairs.
- 具有空中对地推理的细粒度图像解析：虽然空中图像提供了全世界相当大部分的全面覆盖，但它们的分辨率远低于地面图像。 在空中影像中，分辨率与一个像素覆盖的地面相关。 虽然1米分辨率已经是卫星图像的高分辨率，但大多数图像数据库（例如Google Earth）的标准分辨率为12英寸。 6至1英寸的分辨率被认为是高分辨率的航空影像，通常不是公开的。 这使得空中图像的细粒度分割成为一个具有挑战性的问题。 另一方面，地面图像提供了能够进行细粒度语义分割的附加信息。 由于这些线索的互补性质，最近提出了几种细粒度分割方法，这些方法共同地说明了共同定位的天线和地面图像对。

- Mattyus et al. (2016) extend the approach of Mattyus et al. (2015) by introducing a formulation that reasons about fine-grained road semantics such as lanes and sidewalks. To infer this information, they jointly consider monocular aerial images and high-resolution stereo images captured from ground vehicles. Specifically, they formulate the problem as energy minimization in an MRF, inferring the number and location of the lanes for each road segment, all parking spots and sidewalks along with the alignment between the ground and aerial images. Towards this goal, they exploit deep learning to estimate semantics from aerial and ground images and define potentials exploiting both cues. In addition, they define potentials which model road constraints like relationships between parallel roads and the smoothness along roads.
- In a related work, Wegner et al. (2016) build a map of trees for urban planning applications from aerial images, street view images and semantic map data. They train CNN based object detection algorithms on human-annotated data. Furthermore, they combine the CNN predictions from multiple street view images and aerial images with map data in a CRF formulation to achieve a geolocated fine-grained catalog.
- Mattyus等人 （2016）扩展了Mattyus等人的方法。 （2015）通过介绍道路和人行道等细粒度道路语义的原因。 为了推断这些信息，他们共同考虑从地面车辆捕获的单目空间图像和高分辨率立体图像。 具体来说，他们将MRF中的能量最小化问题制定出来，推断每个路段的车道数量和位置，所有停车点和人行道以及地面和航空图像之间的对齐。 为实现这一目标，他们利用深度学习来估计空中和地面图像的语义，并定义利用两种线索的潜力。 此外，他们界定了模拟道路约束的潜力，如平行道路之间的关系以及道路的平滑度。
- 在相关的工作中，Wegner et al。 （2016）从航空图像，街景图像和语义地图数据构建城市规划应用的树木地图。 他们在人工数据上训练基于CNN的对象检测算法。 此外，它们将来自多个街景图像和航空图像的CNN预测与CRF公式中的地图数据相结合，以实现地理位置的细粒度目录。

- 6.6.1. ISPRS Segmentation Challenge ISPRS分段挑战
- The focus of the ISPRS segmentation challenge (Rottensteiner et al. (2013, 2014)) is detailed 2D semantic segmentation of data acquired by airborne sensors as shown in Figure17. More specifically, the task is to assign labels to multiple urban object categories. The challenge comprises two airborne image datasets, Vaihingen and Potsdam, which have been manually annotated by the six most common land cover classes, namely impervious surfaces, building, vegetation, tree, car, clutter/background. Both areas cover urban scenes. The leaderboards of the datasets Potsdam and Vaihingen are provided in the Table 5. The performance of the approaches is assessed with the F1 scores for the six classes and overall
- ISPRS分段挑战的重点（Rottensteiner等（2013，2014））详细描述了机载传感器获取的数据的2D语义分割，如图17所示。 更具体地说，任务是将标签分配给多个城市对象类别。 挑战包括两个机载图像数据集，Vaihingen和波茨坦，这些数据已经被六个最常见的土地覆盖类别手工注释，即不透水表面，建筑物，植被，树木，汽车，杂波/背景。 这两个区域都是城市场景。 数据集Potsdam和Vaihingen的排行榜在表5中提供。该方法的表现用六个等级和总体的F1分数进行评估。

- Paisitkriangkrai et al. (2015) is one of the best-performing methods in the ISPRS segmentation challenge. They propose a semantic pixel labeling method which combines CNN features with hand-crafted features in a pixel-wise CRF formulation to infer a globally consistent labeling that is locally smooth except at edges. Sherrah (2016) propose to use fully-convolutional networks without any downsampling layers to preserve the resolution of the output. In order to make use of elevation data, they propose a hybrid network that combines the pre-trained image features with features based on available digital surface models (DSM) which capture the Earth’s surface. Sherrah (2016) achieve the best performance on the ISPRS Potsdam (Table 5a) and competitive results on Vaihingen in Table 5b.
- Paisitkriangkrai等 （2015）是ISPRS分段挑战中最好的方法之一。 他们提出了一种语义像素标记方法，其将CNN特征与以像素为单位的CRF公式中的手工特征相结合，以推断除了边缘之外局部平滑的全局一致的标记。 Sherrah（2016）建议使用完全卷积网络，而不需要任何下采样层来保持输出的分辨率。 为了利用高程数据，他们提出了一种混合网络，其将预先训练的图像特征与基于捕获地球表面的可用数字表面模型（DSM）的特征相结合。 Sherrah（2016）在表5b中的“Vaihingen”上获得了最佳表现（表5a）和竞争结果。

- Maggiori et al. (2016) introduce a model which extracts spatial features at multiple resolutions and learns how to combine them in order to integrate local and global information. Audebert et al. (2016) further improved the state-of-the-art for dense scene labeling of aerial images by exploiting the encoder-decoder architecture of SegNet (Badrinarayanan et al. (2015)). In addition, they introduce a multi-kernel convolutional layer for fast aggregation of predictions at multiple scales and perform data fusion from heterogeneous sensors using a residual correction network. Marmanis et al. (2016a) demonstrate the best performance on the ISPRS Vaihingen challenge in Table 5b. They use their previous work Marmanis et al. (2016b) which uses an ensemble of fully convolutional networks to obtain pixel-wise classification at full resolution of aerial images. Marmanis et al. (2016a) propose to compensate the loss of spatial resolution due to the pooling layers by combining semantic segmentation with edge detection.
- Maggiori等人（2016）介绍了一种以多种分辨率提取空间特征的模型，并学习如何组合它们，以便整合本地和全球信息。 Audebert等（2016年）通过利用SegNet编码器 - 解码器架构（Badrinarayanan等（2015））进一步改进了航空图像密集场景标记的最新技术。此外，它们引入了一个多内核卷积层，用于在多个尺度上快速聚合预测，并使用残差校正网络执行异构传感器的数据融合。 Marmanis等人（2016a）在表5b中证明了ISPRS Vaihingen挑战的最佳性能。他们使用他们以前的工作Marmanis et al。 （2016b），其使用完整卷积网络的集合来获得全分辨率的空间图像的像素分类。 Marmanis等人（2016a）提出通过将语义分割与边缘检测相结合来补偿由于汇集层造成的空间分辨率的损失。

- 6.7. Road Segmentation 道路分割
- Segmentation of road scenes is a crucial problem in computer vision for applications such as autonomous driving and pedestrian detection. For instance, in order to navigate, an autonomous vehicle needs to determine the drivable free space ahead and determine its own position on the road with respect to the lane markings. However, the problem is challenging due to the presence of a variety of differently shaped objects such as cars and people, different road types and varying illumination and weather conditions.
- Munoz et al. (2010) propose an alternative to standard inference in graphical models for semantic labeling of scenes. In particular, they train a sequence of inference models in a hierarchical procedure that captures the context over large regions. This allows them to bypass the difficulties of training structured prediction models when exact inference is intractable and leads to a very efficient and accurate scene labeling algorithm.
- Kuehnl et al. (2012) propose a method that aims to improve appearance-based classification by incorporating the spatial layout of the scene. Specifically, they propose a two-stage approach for road segmentation. First, they represent the road surface and delimiting elements such as curbstones and lane-markings using confidence maps based on local visual features. From these confidence maps, they extract SPatial RAY (SPRAY) features that incorporate global properties of the scene and train a classifier on those features. Their evaluation shows that spatial layout helps especially for the cases where there is a clear structural correspondence between properties at different spatial locations.
- 道路场景的分割是自动驾驶和行人检测等应用的计算机视觉中的关键问题。 例如，为了导航，自主车辆需要确定前方的可驾驶自由空间，并且相对于车道标记确定其在道路上的位置。 然而，由于存在各种不同形状的物体，例如汽车和人，不同的道路类型和不同的照明和天气条件，问题是具有挑战性的。
- Munoz等人 （2010）提出了场景语义标注图形模型中标准推理的替代方法。 特别地，他们在分层程序中训练一系列推理模型，以捕获大区域上下文。 这样就可以避免训练结构化预测模型的困难，当精确推理是棘手的，并导致非常有效和准确的场景标记算法
- Kuehnl et al。 （2012）提出了一种旨在通过结合场景的空间布局来改进基于外观的分类的方法。 具体来说，他们提出了一个两阶段的道路分割方法。 首先，它们使用基于局部视觉特征的置信图来代表路面和划线元素，例如路缘石和车道标记。 从这些置信图中，他们提取了包含场景全局属性的空间RAY（SPRAY）功能，并对这些特征进行了分类。 他们的评估表明，空间布局特别适用于在不同空间位置的属性之间存在明确的结构对应关系的情况

- Alvarez et al. (2010) propose a Bayesian framework to classify road sequences by combining low-level appearance cues with contextual 3D road cues such as horizon lines, vanishing points, 3D scene layout and 3D road stages. In addition, they extract temporal cues for temporal smoothing of the results. In a follow-up work, A´ lvarez & Lo´pez (2011) convert the image into an illuminant invariant feature space to make their method robust to shadows and then apply a classifier to assign a semantic label to each pixel. Mansinghka et al. (2013) propose an inverse-graphics inspired method employing generative probabilistic graphics programs (GPGP) to infer roads in images taken from vehicle-mounted cameras. GPGPs consist of a stochastic scene generator for generating random samples from a road scene prior, a graphics renderer for rendering the image segmentation for each sample and a stochastic likelihood model linking the renderer’s output and the data.
- CNN-based Methods: Almost all existing algorithms for labeling road scenes are based on machine learning where the parameters of the model are estimated from large annotated datasets. To alleviate the burden of annotating large datasets manually, A´ lvarez et al. (2012) propose a method for road segmentation where noisy training labels for road images are generated using a convolutional neural network trained on a general image database. They further propose a texture descriptor which is based on learning a linear combination of color planes to reduce variability in road texture.
- Alvarez et al（2010）提出了一个贝叶斯框架，通过结合低级别的外观线索与上下文3D路线线索（如地平线，消失点，3D场景布局和3D路段）来分类道路序列。 此外，它们提取时间线索以便对结果进行时间平滑。 在后续工作中，A'lvarez＆Lo'pez（2011）将图像转换为光源不变特征空间，使其方法对阴影有效，然后应用分类器为每个像素分配语义标签。 曼辛哈卡等人 （2013）提出了一种利用生成概率图形程序（GPGP）来推导从车载摄像机拍摄的图像中的道路的逆图形启发方法。 GPGP由随机场景发生器组成，用于从道路场景生成随机样本，用于渲染每个样本的图像分割的图形渲染器以及链接渲染器的输出和数据的随机似然模型。
- 基于CNN的方法：几乎所有现有的用于标记道路场景的算法都是基于机器学习，其中模型的参数是从大的注释数据集估计的。 为了减轻手动注释大数据集的负担，A'lvarez et al。 （2012）提出了一种用于道路分割的方法，其中使用在一般图像数据库上训练的卷积神经网络来生成道路图像的噪声训练标签。 他们进一步提出了一种纹理描述符，其基于学习颜色的线性组合以减少道路纹理的变异性。

- Mohan (2014) propose a scene parsing system using deconvolutional layers in combination with traditional CNNs. Deconvolutional layers learn features that capture mid-level cues such as edge intersections, parallelism and symmetry in image data and thus obtain a more robust representation than regular CNNs. Oliveira et al. (2016) investigate the trade-off between segmentation quality and runtime using U-Nets by Ronneberger et al. (2015). Specifically, they introduce a new mapping between classes and filters at the up-convolutional part of the network to reduce the runtime. They further segment the whole image with a single forward pass, which makes the approach more efficient than patch-based approaches.
- To mitigate the difficulties in acquiring human annotations, Laddha et al. (2016) propose a map-supervised deep learning pipeline which does not require human annotations for training a road segmentation algorithm. Instead, they obtain ground truth labels based on OpenStreetMap information projected into the image domain using the vehicle pose given by the GPS sensor.
- Mohan（2014）提出了一种使用去卷积层与传统CNN相结合的场景解析系统。 解卷积层学习捕获中级线索的特征，例如图像数据中的边缘交点，并行度和对称性，从而获得比常规CNN更强的表示。 Oliveira等人 （2016）研究了Ronneberger等人使用U-Nets进行分割质量和运行时间之间的权衡。（2015年）。 具体来说，它们在网络的上卷积部分引入类和过滤器之间的新映射，以减少运行时间。 他们进一步将整个图像分割成一个单一的向前传递，这使得该方法比基于补丁的方法更有效率。
- 为了缓解人类注解的缺陷，Laddha等 （2016）提出了一种地图监督的深层学习管道，不需要人为注释来训练道路分割算法。 相反，他们使用GPS传感器给出的车辆姿态，基于投影到图像域中的OpenStreetMap信息获得地面真实标签。

- 6.7.1. Free Space Estimation 自由空间估计
- Accurate and reliable estimation of free space and detection of obstacles are core problems that need to be solved to enable autonomous driving. Free space is defined by the available space on the ground surface where navigation of vehicle is guaranteed without collision. Obstacles refer to structures that block the path of the vehicle by sticking out of the ground surface. In contrast to road segmentation approaches, methods for estimating the free-space in front of a vehicle often rely on geometric features as derived from a depth map computed from stereo sensors. However, both approaches can be advantageously combined.
- Badino et al. (2007) propose a method for free space estimation by computing stochastic occupancy grids based on stereo information, where cells in a stochastic occupancy grid carry information about the likelihood of occupancy. Stereo information is integrated over time in order to reduce depth uncertainty. The boundary between free space and occupied space is robustly obtained using dynamic programming on the occupancy grid. This work laid the foundations for the Stixel representation, see Section 4 for an in-depth discussion. While the original method of Badino et al. (2007) makes the assumption of a planar road surface, this assumption is often violated in practice. To tackle more complicated road surfaces, Wedel et al. (2009) propose an algorithm which models non-planar road surfaces using B-splines. The surface parameters are estimated from stereo measurements and tracked over time using a Kalman filter.
- 准确可靠的自由空间估计和障碍物的检测是需要解决的核心问题，以实现自主驾驶。 自由空间由地面上的可用空间定义，车辆的导航保证没有碰撞。 障碍是指通过从地面伸出而阻挡车辆路径的结构。 与道路分割方法相反，用于估计车辆前方的自由空间的方法通常依赖于从立体声传感器计算的深度图导出的几何特征。 然而，可以有利地组合这两种方法。
- Badino等人 （2007）提出了一种基于立体声信息计算随机占用网格的自由空间估计方法，其中随机占用网格中的单元携带有关占用可能性的信息。 随着时间的推移，立体声信息被整合，以减少深度不确定性。 使用占用网格上的动态规划可以获得自由空间和占用空间之间的边界。 这项工作为Stixel代表奠定了基础，参见第4节进行了深入的讨论。 虽然Badino等人的原始方法 （2007）假设一个平面的路面，这个假设在实践中经常被违反。 为了解决更复杂的路面，Wedel等 （2009）提出了一种使用B样条模拟非平面路面的算法。 表面参数从立体测量估计，并使用卡尔曼滤波器随时间跟踪。

- Suleymanov et al. (2016) propose an online system to detect and drive on collision-free traversable paths, based on stereo estimation using a variational approach. In addition to free space detection, their approach also establishes a semantic segmentation of the scene, where labels include ground, sky, obstacles and vegetation. Fisheye cameras provide a wider field of view compared to regular cameras and allow for detection of obstacles closer to the car. H¨ane et al. (2015) propose a method for obstacle detection using monocular fisheye cameras. In order to reduce runtime, they avoid using visual odometry systems to provide accurate vehicle poses and instead rely on less accurate pose estimates from the wheel odometry.
- Long Range Obstacle Detection: The accuracy of obstacle detection methods at long range is a crucial factor for timely obstacle localization when the observer (i.e., the ego-vehicle) moves at high speed. Unfortunately, the error of stereo vision system increases quadratically with depth in contrast to laser range sensors or radar which do not suffer from this problem. To tackle this problem, Pinggera et al. (2015, 2016) propose long-range obstacle detection algorithms using stereo vision by exploiting geometric constraints on camera motion and planarity to formulate obstacle detection as a statistical hypothesis testing problem. Specifically, independent hypothesis tests are performed on small local patches distributed across the input images where free-space and obstacles are represented by the null and alternative hypothesis respectively. The detection results for an exemplary scene from their novel dataset is illustrated in Figure 18.
- Suleymanov等人 （2016）提出了一种基于使用变分方法的立体声估计的在无系统的无碰撞路径上进行检测和驱动的在线系统。 除了自由空间检测，他们的方法还建立了场景的语义分割，其中标签包括地面，天空，障碍物和植被。 与普通相机相比，鱼眼相机提供了更广阔的视野，并且能够检测靠近汽车的障碍物。 H¨ane等 （2015）提出了一种使用单眼鱼眼相机进行障碍物检测的方法。 为了减少运行时间，他们避免使用视觉测距系统来提供精确的车辆姿势，而是依靠轮距离测量的精确姿态估计
- 长距障碍检测：当观察者（即，自我 - 车辆）高速移动时，长距离障碍物检测方法的准确性是及时障碍物定位的关键因素。 不幸的是，立体视觉系统的误差随着深度的增加而增加，与激光距离传感器或雷达不同，该雷达不会受到此问题的影响。 为了解决这个问题，Pinggera等 （2015年，2016年）提出了使用立体视觉的远程障碍物检测算法，通过利用相机运动和平面度的几何约束来制定障碍物检测作为统计假设检验问题。 具体来说，对分布在输入图像上的小局部补丁执行独立假设检验，其中空间和障碍分别由空和替代假设表示。 图18中示出了来自其新颖数据集的示例场景的检测结果。

### 7. Reconstruction 再构建
- 7.1. Stereo 立体
- Stereo estimation is the process of extracting 3D information from 2D images captured by stereo cameras, without need for special range measurement devices. In particular, stereo algorithms estimate depth information by finding correspondences in two images taken at the same point in time, typically by two cameras mounted next to each other on a fixed rig. These correspondences are projections of the same physical surface in the 3D world. Depth information is crucial for applications in autonomous driving or driver assistance systems. Accurate estimation of dense depth maps is a necessary step for 3D reconstruction, and many other problems such as obstacle detection, free space analysis, and tracking benefit from the availability of depth estimates.
- 立体声估计是从立体相机拍摄的2D图像中提取3D信息的过程，而不需要特殊的距离测量装置。 特别地，立体声算法通过在同一时间点上拍摄的两个图像中的对应关系来估计深度信息，通常通过在固定的钻机上彼此相邻地安装两个相机。 这些对应是3D世界中相同物理表面的投影。 深度信息对于自主驾驶或驾驶员辅助系统中的应用至关重要。 精确估计密集深度图是3D重建的必要步骤，诸如障碍物检测，自由空间分析和跟踪等许多其他问题可从深度估计的可用性中获益。

- **Taxonomies**: Multiple taxonomies for stereo matching have been proposed in the literature. Guided by the computational restrictions, the earliest one is based on the density of the output (Franke & Joos (2000)). Feature-based methods provide only sparse depth maps based on edges while area-based methods, such as block matching, generate dense outputs at the expense of computation time. A more recent and commonly referred taxonomy of stereo algorithms is based on the optimization as local and global. Local methods compute the disparity by simply selecting the lowest matching cost which is known as the winner takes all (WTA) solution. Global methods formulate disparity computation as an energy-minimization framework based on the smoothness assumption between neighboring pixels or regions. There are various ways of finding the minimum of a global energy function, including variational approaches in continuous domain and discrete approaches using dynamic programming, Graph Cuts, and Belief Propagation.
- 分类：文献中已经提出了立体匹配的多重分类法。在计算限制的基础上，最早的是基于输出密度（Franke＆Joos（2000））。基于特征的方法仅提供基于边缘的稀疏深度图，而基于区域的方法（例如块匹配）以牺牲计算时间为代价产生密集输出。立体声算法的更新和普遍引用的分类法是基于本地和全局的优化。本地方法通过简单地选择被称为获胜者获得所有（WTA）解的最低匹配成本来计算差异。全局方法基于相邻像素或区域之间的平滑度假设，将视差计算作为能量最小化框架。有各种各样的方法可以找到全局能量函数的最小值，包括使用动态规划，图表切换和信念传播的连续域和离散方法的变分方法。

- Matching Cost Function: Stereo matching is a correspondence problem where the goal is to identify the matching points between left and right image based on a cost function. The algorithms usually assume images are rectified, and the search space is reduced to a horizontal line where the correspondence between a left and right point is encoded by the distance on this line, which is defined as disparity. The matching cost computation is the process of computing a cost function at each pixel for all possible disparities which takes its minimal value at the true disparity. However, it is hard to design such a cost function in practice, therefore stereo algorithms make the assumption of constant appearance between matching points. This assumption is often violated in real-world situations, such as cameras with slightly dierent settings causing exposure changes, vignetting, image noise, non-Lambertian surfaces, illumination changes, etc. Hirschm¨uller & Scharstein (2007) call these changes radiometric differences and systematically investigate their effect on commonly used matching cost functions, namely absolute differences, filter-based costs (LoG, Rank and Mean), hierarchical mutual information (HMI), and normalized crosscorrelation. They found that the performance of a cost function depends on the stereo method that uses it. On images with simulated and real radiometric differences, rank filter performed best for correlation-based methods. For global methods, in tests with global radiometric changes or noise, HMI performed best, while in the presence of local radiometric variations, Rank and LoG filters performed better than HMI. Qualitative results show that filter-based cost cause blurred object boundaries when used with global methods. None of the matching costs evaluated could succeed at handling strong lighting changes.
- 匹配成本函数：立体匹配是一个对应问题，其目的是基于成本函数来识别左右图像之间的匹配点。算法通常假设图像被整流，并且搜索空间减少到水平线，其中左和右点之间的对应被该行上的距离编码，其被定义为视差。匹配成本计算是对于所有可能的差异，在每个像素处计算成本函数的过程，其在真实差异下采用其最小值。然而，在实践中很难设计这样的成本函数，因此立体声算法使匹配点之间的外观不变。这种假设在现实世界的情况下经常遭到侵犯，例如具有轻微不同设置的摄影机，会引起曝光变化，渐晕，图像噪声，非朗伯表面，照明变化等。Hirschmüller＆Scharstein（2007）称这些变化辐射度差异，并系统地研究其对常用匹配成本函数的影响，即绝对差异，基于过滤器的成本（LoG，Rank和Mean），分层互信息（HMI）和归一化互相关。他们发现成本函数的性能取决于使用它的立体声方法。在具有模拟和实际辐射度差异的图像上，等级滤波器对于基于相关的方法表现最佳。对于全局方法，在具有全局辐射度变化或噪声的测试中，HMI性能最好，而在存在局部辐射度变化的情况下，Rank和LoG滤波器的性能优于HMI。定性结果表明，当与全局方法一起使用时，基于过滤器的成本导致对象边界模糊。评估的匹配成本中没有一个可以成功处理强烈的照明变化。

- **SGM**: Semi-Global Matching (SGM) (Hirschm¨uller (2008)) has become very influential due to its speed and high accuracy as evidenced in various benchmarks such as Middlebury (Scharstein & Szeliski (2002)) or KITTI (Geiger et al. (2012b)). SGM is also recently used on top of CNN features, since simply outputting the most likely configuration for every pixel is not competitive with modern stereo algorithms (Zbontar & LeCun (2016); Luo et al. (2016)). The energy function has two levels of penalization for small and large disparity differences with a weighting based on the local intensity gradient for the latter one. The energy is calculated by summing costs along 1D paths from multiple directions towards each pixel using dynamic programming and the result is determined by WTA. There are a couple of follow-up works investigating the practical and theoretical sides of SGM. Gehrig et al. (2009) propose a real-time, low-power implementation of the SGM with algorithmic extensions for automotive applications on a reconfigurable hardware platform. Drory et al. (2014) offer a principled explanation for the success of SGM by clarifying its relation to belief propagation and tree-reweighted message passing with an uncertainty measure as an outcome.
- The performance of SGMs can be further improved by incorporating confidences of the stereo estimation. Seki & Pollefeys (2016) leverage CNNs to predict the confidences for stereo estimations. Taking into account ideas from conventional confidences features, that neighboring pixel which are consistent are more likely to be correct and the disparity estimated from the other image should correspond, they design a two-channel disparity patch which is used as input for the CNN. In order to acquire dense disparity, the confidences are incorporated into SGM by weighting each pixel according to the estimated confidence.
- SGM：半全球配对（Hirschmuller（2008））由于其速度和准确性而变得非常有影响力，如米德伯勒（Scharstein＆Szeliski（2002））或KITTI（Geiger （2012b））。 SGM也最近被用于CNN功能之上，因为简单地输出每个像素的最可能的配置与现代立体声算法（Zbontar＆LeCun（2016）; Luo等（2016））不具有竞争力。能量函数对于小和大的差异差异具有两个级别的惩罚，基于对后者的局部强度梯度的加权。通过使用动态规划将来自多个方向的1D路径的成本与每个像素相加来计算能量，并且结果由WTA确定。有几项后续工作调查了上海证券交易所的实际和理论方面。 Gehrig等人（2009）提出了在可重配置硬件平台上实现汽车应用的算法扩展的SGM的实时低功耗实现。 Drory等（2014年）通过澄清其与信仰传播的关系和树重加权的消息传递以不确定性度量作为结果，为SGM的成功提供了原则性的解释。
- 通过引入立体声估计的信心，可以进一步提高SGM的性能。 Seki＆Pollefeys（2016）利用CNN来预测立体声估计的信心。考虑到常规信号特征的想法，一致的相邻像素更可能是正确的，并且从另一个图像估计的视差应该对应，它们设计一个双通道视差补丁，用作CNN的输入。为了获得密集的差异，通过根据估计的置信度对每个像素进行加权，将信息并入SGM。

- **Variable Baseline/Resolution**: Stereo estimates can be fused to yield a more complete reconstruction of the static parts of the three-dimensional scene. However, assuming fixed baseline, focal length, field of view might not always be the best strategy. Gallup et al. (2008) point out two problems with traditional stereo methods: dropping accuracy in the far range and unnecessary computation time spent in the near range. Given that choice of views for stereo is quite flexible in many applications such as structure from motion, Gallup et al. (2008) propose to dynamically select the best cameras with the appropriate baseline for accurate estimation in the far range from a set of possible cameras recording images at the same time. Further, they reduce the resolution to speed up the computation in the near range. In contrast to traditional fixed-baseline stereo, the proposed variable baseline/resolution stereo algorithm achieves constant accuracy over the reconstructed volume by evenly spreading the computation throughout the volume.
- **Planarity**: The inherent ambiguity in appearance based matching costs can be overcome by regularization, i.e., by introducing prior knowledge about the expected disparity map into the stereo estimation process. The simplest prior favors neighboring pixels to take on the same disparity value. However, such generic smoothness priors fail to reconstruct poorly-textured and slanted surfaces, as they favor fronto-parallel planes. A more generic approach to handle arbitrary smoothness priors is using higher-order connections beyond pairwise. Higherorder priors are able to express more realistic assumptions about depth images, but usually at additional computational cost. One very common way to deal with slanted surfaces in the literature is to assume piecewise planarity. Geiger et al. (2010) build a prior over the disparity space by forming a triangulation on a set of robustly matched correspondences, called support points. This reduces matching ambiguities and results in an efficient algorithm by restricting the search to plausible regions. Gallup et al. (2010) first train a classifier to segment an image into piecewise planar and non-planar regions and then enforce a piecewise planarity prior only for planar regions. Non-planar regions are modeled by the output of a standard multi-view stereo algorithm.
- 可变基准/分辨率：可以融合立体声估计，以产生三维场景的静态部分的更完整的重建。然而，假设固定基线，焦距，视野可能不总是最好的策略。盖洛普等人（2008）指出了传统立体声方法的两个问题：在远程范围内降低精度，在近距离内消耗不必要的计算时间。鉴于立体声的选择在许多应用中是非常灵活的，例如来自运动的结构，Gallup等人（2008）提出动态选择具有适当基准的最佳相机，以便在远距离范围内进行精确估计，从一组可能的摄像机同时记录图像。此外，它们降低了分辨率，以加速近似范围内的计算。与传统的固定基线立体声相比，所提出的可变基线/分辨率立体声算法通过在整个音量上均匀地展开计算，在重建体积上实现了恒定的精度。
- 平面：基于外观的匹配成本的固有歧义可以通过正则化来克服，即通过将关于预期视差图的先前知识引入到立体声估计过程中。最简单的先前有利于相邻像素具有相同的差异值。然而，这种通用平滑度先验不能重构不良纹理和倾斜的表面，因为它们有利于前平行平面。处理任意平滑先验的更通用的方法是使用成对以外的高阶连接。高阶先验能够对深度图像表达更真实的假设，但通常以额外的计算成本。在文献中处理倾斜表面的一个很常见的方法是假设分段平面。盖革等人（2010）通过在称为支持点的一组鲁棒匹配的对应上形成三角测量来构建差异空间的前一个。这减少了匹配模糊度，并通过将搜索限制到合理的区域来产生有效的算法。盖洛普等人（2010）首先训练分类器将图像分割成分段平面和非平面区域，然后仅在平面区域之前实施分段平面度。非平面区域由标准多视点立体声算法的输出建模。

- **Variational Approaches**: Similarly, in variational approaches, commonly used smoothness prior, Total Variation (TV) does not produce convincing results in the presence of weak and ambiguous observations, since it encourages piecewise constant regions leading to stair-casing artifacts. Haene et al. (2012) introduce patch-based priors into a TV framework in the form of small, piecewise planar dictionaries. Total Generalized Variation (TGV) (Bredies et al. (2010)) is argued to be a better prior than TV, since it does not penalize piecewise affine solutions. However, it is restricted to convex data terms in contrast to TV, where global solutions can be computed even in the presence of non-convex data terms. Coarse-to-fine approaches as an approximation to non-convex problem of stereo matching often end up with loss of details. To preserve fine details, Kuschk & Cremers (2013) integrate an adaptive regularization weight into the TGV framework by using edge detection and report improved results compared to a coarse-to-fine approach. Ranftl et al. (2013) obtain even better results by proposing a decomposition of the non-convex functional into two subproblems which can be solved globally where one is convex, and the other can be made convex by lifting the functional to a higher dimensional space.
- **State-of-the-art**: In Table 6 we show the ranking of stereo methods on the KITTI stereo 2015 benchmark. The KITTI benchmark reports the percentage of erroneous (bad) pixels over background regions (D1-bg), foreground regions (D1-fg) and over all regions (D1-all). The best performing method G¨uney & Geiger (2015) use object knowledge to compensate for the weak data term on the reflecting and textureless surfaces. Seki &Pollefeys (2016) achieve the best performance on background regions with the prediction of stereo correspondence confidences and integration into SGM. Recently, deep learning approaches (Zbontar&LeCun (2016); Luo et al. (2016); Mayer et al. (2016)) were proposed achieving state-of-the-art performance. The deep learning approach presented by Mayer et al. (2016) is one of the fastest approaches.
- 变异方法：类似地，在变分方法中，普遍使用的平滑度之前，总变异（TV）不会产生令人信服的结果存在弱和模糊的观察，因为它鼓励分段恒定区域导致楼梯套管伪影。 Haene等人（2012）将补丁为基础的先验引入电视框架，形式为小型，分段平面字典。总广义变异（TGV）（Bredies等人（2010））被认为比电视更好，因为它不惩罚分段仿射解决方案。然而，与TV相反，它仅限于凸数据项，即使在存在非凸数据项的情况下，也可以计算全局解。粗略到精细的方法作为立体匹配的非凸问题的近似通常最终导致细节的损失。为了保持细节，Kuschk＆Cremers（2013）通过使用边缘检测将自适应正则化权重整合到TGV框架中，并报告与粗略到粗略方法相比的改进结果。 Ranftl等人（2013）通过提出将非凸函数分解成两个子问题来获得更好的结果，这两个子问题可以在一个凸起的情况下被全局求解，另一个可以通过将功能提升到更高维度的空间来制作凸起。
- 最先进的：在表6中，我们展示了KITTI立体声2015基准的立体声方法的排名。 KITTI基准测试报告了背景区域（D1-bg），前景区域（D1-fg）和所有区域（D1-all）之间的错误（差）像素的百分比。最好的表现方法G¨uney＆Geiger（2015）使用对象知识来补偿反射和无纹理表面上的弱数据项。 Seki＆Pollefeys（2016）在背景区域实现了最佳性能，预测了立体声通信的信心并融入了SGM。最近，提出了深入学习的方法（Zbontar＆LeCun（2016）; Luo等（2016）; Mayer等（2016））提出了最先进的表现。 Mayer等人提出的深度学习方法（2016）是最快捷的方法之一。

- **Superpixels**: An alternative way of modeling piecewise planarity is to explicitly partition the image into superpixels and modeling the surface at each superpixel as a slanted plane (Yamaguchi et al. (2012); G¨uney & Geiger (2015)). However, care must be taken that the super-pixelization is indeed an over-segmentation of the image with respect to planarity, i.e., that no superpixel contains two surfaces which are not co-planar. Yamaguchi et al. (2012) jointly reason about occlusion boundaries and depth in a hybrid MRF composed of both continuous and discrete random variables. Guney & Geiger (2015) use a similar framework to incorporate object-category specific 3D shape proposals which regularize over larger distances. By leveraging semantic segmentation and 3D CAD models, they resolve ambiguities in reflective and textureless regions originating from highly specular surface of cars in the scene as shown in Figure 19.
- **Deep Learning**: In the last years, deep learning approaches (Mayer et al. (2016); Zbontar&LeCun (2016); Luo et al. (2016)) gained popularity in stereo estimation. Mayer et al. (2016) adapt the encoder-decoder architecture proposed by Dosovitskiy et al. (2015) that was used for optical flow estimation (see Section 8.1). The encoder computes abstract features while the decoder reestablishes the original resolution with additional crosslinks between the contracting and expanding network parts. In contrast to the encoder-decoder architecture, ˇ Zbontar & LeCun (2016); Luo et al. (2016) use Siamese network which consists of two sub-networks with shared weights and a final score computation layer. The idea is to train the network for computing the matching cost by learning a similarity measure on small image patches. Zbontar & LeCun (2016) define positive/negative examples as matching and non-matching patches and use a margin loss to train either a fast architecture with a simple dot-product layer in the end or a slow but more accurate architecture which learns score computation with a set of fully connected layers. Luo et al. (2016) use a similar architecture, but formulate the problem as multi-class classification over all possible disparities to capture correlations between different disparities implicitly as visualized in Figure 20.
- 超像素：分段平面建模的另一种方法是将图像明确地划分为超像素，并将每个超像素的表面作为倾斜平面进行建模（Yamaguchi et al。（2012）;G¨uney＆Geiger（2015）） 。然而，必须注意，超像素化确实是相对于平面度的图像的过度分割，即，没有超像素包含不共面的两个表面。山口等（2012）共同理解了由连续和离散随机变量组成的混合MRF中的闭塞边界和深度。 Guney＆Geiger（2015）使用类似的框架来整合在更大距离上规则化的对象​​类特定3D形状提案。通过利用语义分割和3D CAD模型，它们解决了源自现场汽车高度镜面的反射和无纹理区域的模糊性，如图19所示。
- 深度学习：在过去几年中，深度学习方法（Mayer等（2016）; Zbontar＆LeCun（2016）; Luo等（2016））在立体声估计中得到普及。 Mayer等人（2016）适应Dosovitskiy等人提出的编码器 - 解码器架构。 （2015年），用于光流估计（见第8.1节）。编码器计算抽象特征，而解码器重新建立原始分辨率，并在合同和扩展网络部分之间附加交叉链接。与编码器 - 解码器架构相比，Zbontar＆LeCun（2016）;罗等人（2016）使用由具有共享权重的两个子网络和最终分数计算层组成的暹罗网络。这个想法是通过学习小图像补丁的相似性度量来训练网络来计算匹配成本。 Zbontar＆LeCun（2016）将正/负的例子定义为匹配和非匹配补丁，并使用边际损失来训练最终的简单点阵产品层的快速架构，或者学习分数计算的慢但更准确的架构与一套完全连接的层。罗等人（2016）使用了类似的架构，但是将所有可能的差异作为多类别分类来形成问题，以便在图20中隐含地隐藏不同差异之间的相关性。


- Discussion: Stereo estimation has shown great progress in the last years both in terms of accuracy and efficiency. However, some inherent problems refrain it from being marked as solved. Stereo matching is ultimately searching for correspondences in two images based on the assumption of constant appearance. However, appearance frequently changes by cues different than geometry, furthermore occluded regions or pixels leaving the frame cannot be matched. Therefore, failure in those cases is inevitable for methods that solely rely on appearance matching without any other prior assumptions about the geometry. We show accumulated errors of top 15 methods on KITTI stereo benchmark Geiger et al. (2012b) in Figure 21. The most common example of failure case in the autonomous driving context are car surfaces due to shiny and reflective regions. G¨uney & Geiger (2015) specifically address this problem by integrating prior knowledge on possible car shapes. Similarly, windows that are reflective and transparent cannot be matched reliably. As concluded by Hirschm¨uller & Scharstein (2007), strong illumination changes constitute another common source of error such as inside a tunnel or over-exposure on road surfaces. Pixels leaving the frame and occlusions often cause errors for many methods and both require reasoning beyond matching and local interactions. Other specific examples of problematic regions include thin structures like traffic signs, or repetitive ones like fences.
- 讨论：立体声估计在过去几年中在精度和效率方面都取得了很大进展。然而，一些固有的问题避免被标记为解决。基于恒定外观的假设，立体匹配最终搜索两个图像中的对应关系。然而，外观通常由与几何不同的线索改变，而离开框架的遮挡区域或像素也不能匹配。因此，在这些情况下的失败对于仅依赖于外观匹配的方法来说是不可避免的，而对于几何形状没有任何其他先前的假设。我们在KITTI立体声基准Geiger等人显示了前15种方法的累积误差。 （2012b）。自主驾驶环境中故障案例的最常见例子是汽车表面，由于光线和反射区域。 G¨uney＆Geiger（2015）专门针对这个问题，整合了可能的汽车形状的先前的知识。类似地，反射和透明的窗口不能可靠地匹配。如Hirschmuller＆Scharstein（2007）所做的那样，强烈的照明变化构成了隧道内的另一个常见的错误来源，或者在路面上过度曝光。离开帧和遮挡的像素通常会导致许多方法的错误，并且都需要超出匹配和本地交互的推理。有问题的地区的其他具体例子包括像交通标志这样的薄结构，或像栅栏这样的重复的结构。

- 7.2. Multi-view 3D Reconstruction 多视角3D重构
- The goal of multi-view 3D reconstruction is to model the underlying 3D geometry by inverting the image formation process often under certain prior or smoothness assumptions. In contrast to two-view stereo, multi-view reconstruction algorithms in particular address the problems of varying viewpoints and the complete reconstruction of 3D scenes from more than two and potentially a very large number of images. If the camera parameters are known, solving for the 3D geometry of the scene is equivalent to solving the correspondence problem, based on a photo-consistency function which measures the agreement between different viewpoints.
- Taxonomies: Several categorizations of multi-view reconstruction algorithms have been proposed in the literature, typically considering the form of the photo-consistency function, the scene representation, visibility computation, priors, and initialization requirements as in Seitz et al. (2006). From an application perspective, the scene representation is a common way of classifying multi-view reconstruction approaches into depth map, point cloud, mesh, and volumetric.
- 多视图3D重建的目标是通过在某些先前或平滑假设下经常反转图像形成过程来对底层3D几何进行建模。与双视图立体声相比，多视图重建算法特别解决了来自两个以上且潜在的大量图像的变化视点和3D场景的完整重建的问题。如果相机参数是已知的，则解决场景的3D几何相当于基于测量不同视点之间的一致性的照片一致性函数来解决对应问题。
- 分类：文献中已经提出了多视图重建算法的几个分类，通常考虑到照片一致性函数的形式，场景表示，可视性计算，先验和初始化要求，如Seitz等人。 （2006年）。从应用的角度来看，场景表示是将多视图重建方法分为深度图，点云，网格和体积的常见方式。

- **Representations: Depth Map**: The depth map representation typically consists of a depth map for each input view estimated with a 3D modeling pipeline which starts with image matching followed by pose estimation and dense stereo. This representation is usually preferred in scene analysis due to its flexibility and scalability to large scenes. One strategy which is particularly effiective for urban scenes is Plane Sweeping Stereo algorithm (Collins (1996)). It sweeps a family of parallel planes in a scene, projects images onto a plane via planar homographies, then evaluates photo-consistency values on each plane. In large scenes, one of the challenges is to handle massive amount of data in real-time. Pollefeys (2008) propose a large scale, realtime 3D reconstruction system based on depth map representation. The real-time performance is achieved by incorporating a set of components which are particularly efficient on typical urban scenes such as a 2D feature tracker with automatic gain adaptation for handling large dynamic range in natural scenes, and parallel implementations of plane sweeping stereo and depth map fusion on GPU.
- **Representations: Point-cloud**: In contrast to a partial depth map for each view, point-cloud or patch based surface representations reconstruct a single 3D point-cloud model using all the input images. Under spatial consistency assumptions, the pointcloud on the surface of the scene can grow or expand which provides easy model manipulation such as merging and splitting. The representative work for these kind of approaches is Patch-based Multi-View Stereo (PMVS) by Furukawa & Ponce (2010). PMVS starts with a feature matching step to generate a sparse set of patches and then iterate between a greedy expansion step and a filtering step to make patches dense and remove erroneous matches.
- 表示：深度图：深度图表示通常由用3D建模流水线估算的每个输入视图的深度图组成，3D建模流水线以图像匹配开始，随后是姿态估计和密集立体。由于场景分析的灵活性和可扩展性，因此这种表示在场景分析中通常是首选的。一种对城市场景特别有效的策略是平面扫描立体声算法（Collins（1996））。它在场景中扫描一系列平行平面，通过平面同图将图像投影到平面上，然后评估每个平面上的照片一致性值。在大型场景中，挑战之一是实时处理大量数据。 Pollefeys（2008）提出了一种基于深度图表示的大规模实时三维重建系统。实时性能是通过结合一套在典型城市场景上特别有效的组件来实现的，例如具有自动增益适应的2D特征跟踪器，用于处理自然场景中的大动态范围，以及平面扫描立体声和深度图的并行实现融合在GPU上
- 表示：点云：与每个视图的部分深度图相反，点云或基于贴片的表面表示使用所有输入图像重建单个3D点云模型。在空间一致性假设下，场景表面上的点云可以增长或扩展，这提供了容易的模型操作，如合并和分割。这些方法的代表性工作是Furukawa＆Ponce（2010）的基于Patch的多视点立体声（PMVS）。 PMVS从特征匹配步骤开始，以生成一组稀疏的补丁，然后在贪心扩张步骤和过滤步骤之间进行迭代，以使补丁密集并删除错误的匹配项。

- **Representations: Volumetric**: Volumetric approaches represent geometry on a regularly sampled 3D grid, i.e. volume, either as a discrete occupancy function (Kutulakos&Seitz (2000)) or a function encoding distance to the closest surface (level-set) (Faugeras & Keriven (1998)). More recent approaches use a probability map defined at regular voxel locations to encode the probability of occupancy (Bhotika et al. (2002); Pollard & Mundy (2007); Ulusoy et al. (2015)). The amount of memory required is the main limitation for volumetric approaches. There is a variety of methods for dealing with this problem such as voxel hashing (Nießner et al. (2013)) or a data adaptive discretization of the space in the form of a Delaunay triangulation (Labatut et al. (2007)). One effective solution is an octree data structure which is essentially an adaptive voxel grid to allocate high resolution cells only near the surfaces.
- **Representations: Mesh or Surface**: The final representation in reconstruction is typically triangular mesh-based surfaces. Volumetric surface extraction fuses 3D information from an intermediate representation such as depth maps, point clouds, volumes or scans into a single, clean mesh model. Seminal work by Curless & Levoy (1996) proposes an algorithm to accumulate surface evidence into a voxel grid using signed distance functions. The surface is implicitly represented as the zero crossing of the aggregated signed distance functions. It can be extracted using the Marching Cube algorithm Lorensen & Cline (1987) or using volumetric graph cuts to label each voxel as interior or exterior. There are approaches which directly start from images and refine a mesh model using an energy function composed of a data term based on photo-consistency function and a regularization term for smoothness. In these approaches, the energy is usually optimized using gradient descent, where the movement of each vertex is determined by the gradient of the objective function.
- 表示：体积：体积方法表示定期采样的3D网格上的几何，即体积，作为离散占用函数（Kutulakos＆Seitz（2000））或函数编码距离最接近的表面（水平集）的距离（Faugeras ＆Keriven（1998））。最近的方法使用在常规体素位置定义的概率图来编码占用概率（Bhotika等（2002）; Pollard＆Mundy（2007）; Ulusoy等（2015））。所需的内存量是体积方法的主要限制。处理这个问题的方法有很多种，如体素散列（Nießneret al。（2013））或以Delaunay三角剖分形式对空间进行数据自适应离散化（Labatut et al。（2007））。一个有效的解决方案是八叉树数据结构，它本质上是一个适应性体素网格，仅在表面附近分配高分辨率单元。
- 表示：网格或曲面：重建中的最终表示通常是三角形网格的曲面。体积表面提取将3D信息从诸如深度图，点云，体积或扫描的中间表示融合到单个，干净的网格模型中。 Curless＆Levoy（1996）的精神工作提出了一种使用带符号距离函数将表面证据积累到体素网格中的算法。表面隐含地表示为聚合有符号距离函数的过零点。它可以使用Marching Cube算法Lorensen＆Cline（1987）提取，或使用体积图切割将每个体素标记为内部或外部。存在直接从图像开始的方法，并且使用由基于光一致性函数的数据项组成的能量函数和用于平滑度的正则化项来细化网格模型。在这些方法中，通常使用梯度下降优化能量，其中每个顶点的移动由目标函数的梯度确定。

- **Urban Reconstruction**: In this survey, we focus on multiview reconstruction from an autonomous driving perspective which mainly concerns the reconstruction of large urban areas, up to whole cities. The goal of urban reconstruction algorithms is to produce fully automatic, high-quality, dense reconstructions of urban areas by addressing inherent challenges such as lighting conditions, occlusions, appearance changes, high-resolution inputs, and large scale outputs. Musialski et al. (2013) provide a survey of urban reconstruction approaches by following an output-based ordering, namely buildings and semantics, facades and images, and finally blocks and cities.
- **Input Data**: Musialski et al. (2013) point out that ground, aerial and satellite imagery, as well as Light Detection and Ranging (LiDAR) scans are the most commonly used sensors for urban reconstruction. Ground-level imagery is the most prevalent one due to easy acquisition, storage and exchange. Aerial and satellite imagery have become more easily available due to the advances of Web-mapping projects. In contrast to aerial or multi-view imagery, satellite imagery provides a worldwide coverage at a high frequency with lower costs, but also with lower resolution. LiDAR delivers semi-dense 3D point-clouds which are fairly precise, both ground-level and aerial. Some approaches also incorporate several of these data types together in order to combine their complementary strengths. To deal with the challenging conditions of outdoor scenes, other methods leverage additional data sources, like Digital Surface Models (DSMs) which capture the Earth’s surface. DSMs are 2:5D representations of an urban scene that provide a height for each point on a regular grid. In the following, we provide recent examples of dierent input modalities.
- 城市重建：在这次调查中，我们从自主驾驶的角度重点关注多视角重建，主要涉及到大城市，直到整个城市的重建。城市重建算法的目标是通过解决诸如照明条件，遮挡，外观变化，高分辨率输入和大规模输出等内在挑战，来实现城市全自动，高质量，密集的重建。 Musialski等（2013年）通过遵循基于输出的排序，即建筑物和语义，外墙和图像，以及最终的街区和城市，提供城市重建方法的调查。
- 输入数据：Musialski et al。 （2013）指出，地面，空中和卫星图像以及光检测和测距（LiDAR）扫描是城市重建中最常用的传感器。地平面图像是最流行的，因为易于采集，存储和交换。由于Web-mapping项目的进步，空中和卫星图像变得更容易获得。与空中或多视点图像相比，卫星图像在高频率下提供了全球覆盖，成本更低，而且分辨率更低。 LiDAR提供半密度3D点云，这些云点相当精确，包括地面和天线。一些方法也将这些数据类型中的几种结合在一起，以便结合它们的互补优势。为了应对户外场景的挑战性条件，其他方法利用了捕获地球表面的数字表面模型（DSM）等附加数据源。帝斯曼是城市场景的2：5D表示，为常规网格上的每个点提供高度。在下文中，我们提供了不同输入模式的最近例子。

- **Stereo Sequences**: Cornelis et al. (2008) point out that the extraction of detailed 3D information from video streams incur high computational cost for reconstruction algorithms. By keeping the necessary level of detail low, they focus on creating compact, memory ecient 3D city models from a stereo pair, at high speed based on simplified geometry assumptions, namely ruled surfaces for facade and road surfaces. Since objects such as cars which are prevalent in urban scenes violate these assumptions, they integrate the detection and localization of cars into the reconstruction. By leveraging efficient stereo matching, Geiger et al. (2011) propose a system to generate accurate 3D reconstructions of static scenes from stereo sequences in realtime. For online reconstruction, they employ two threads: the first thread performs feature matching and ego-motion estimation, while the second thread performs dense stereo matching and 3D reconstruction.
- **Digital Surface Models (DSM)**: Digital Surface Models are either generated from aerial LiDAR point clouds or Multi-View Stereo (MVS) and adapted to geometric descriptions of urban scenes. MVS-based DSMs can be very noisy and therefore Lafarge et al. (2010) propose to generate DSMs from MVS imagery by reconstructing buildings with an assemble of simple urban structures extracted from a library of 3D parametric blocks. In contrast to MVS-based DSMs, laser scans have been also very popular to acquire 3D city models. Lafarge & Mallet (2012) provide a more complete description of urban scenes by simultaneously reconstructing trees and topologically complex ground surfaces in addition to the buildings from point clouds generated by aerial data. They model the original hybrid representation of buildings by combining two dierent types of 3D representations: primitives for regular parts of buildings as in Lafarge et al. (2010) and mesh patches for modeling atypical surfaces such as irregular roofs.
- 立体声序列：Cornelis et al。 （2008）指出，从视频流中提取详细的3D信息会导致重建算法的高计算成本。通过保持必要的细节水平，他们专注于从立体声对，基于简化的几何假设，即立面和路面的规则表面高速创建紧凑，记忆效率的3D城市模型。由于诸如城市场面普遍存在的汽车等物品违反了这些假设，将汽车的检测和本地化整合到重建中。通过利用高效的立体匹配，Geiger等（2011）提出了一种从立体声序列实时生成静态场景的精确3D重建的系统。对于在线重建，它们采用两个线程：第一个线程执行特征匹配和自主运动估计，而第二个线程执行密集的立体匹配和3D重建。
- 数字表面模型（DSM）：数字表面模型可以从空中LiDAR点云或多视点立体声（MVS）生成，并适用于城市场景的几何描述。基于MVS的DSM可能非常嘈杂，因此Lafarge等人（2010）提出从MVS图像生成DSM，通过从3D参数块库提取的简单城市结构的组合重建建筑物。与基于MVS的DSM相反，激光扫描也非常受欢迎，以获得3D城市模型。拉法基和马勒（2012）通过同时重建树木和拓扑复杂的地面以及由航空数据产生的点云的建筑物，提供了更完整的城市场景描述。他们通过组合两种不同类型的3D表示来模拟建筑物的原始混合表示：如Lafarge等人的常规建筑部分的原始图形。 （2010）和用于建模非典型表面（如不规则屋顶）的网格补丁。

- **Air and Street level**: Fruh et al. (2005) register a series of vertical 2D surface scans and camera images to airborne data (DSMs) to generate textured facade meshes of cities. They propose a class of data processing techniques to create visually appealing facade meshes by removing noisy foreground objects and filling holes in the geometry and texture of building facades. B´odis-Szomor´u et al. (2016) point out that airborne and mobile mapping data provide complementary information and need to be exploited together in order to produce complete and detailed large-scale city models. Airborne sensors can acquire roof structures, ground, and vegetation at large scale while on-road mobile mapping by multi-view stereo approaches or LiDAR provide the facade and street-side details. They propose
a solution to fuse a detailed on-road mobile mapping and a coarser but more complete point cloud from airborne acquisition in a joint surface mesh. Their evaluation shows that the quality of the model improves substantially by fusing streetside details into the airborne model.
- **Stereo Satellite**: Duan & Lafarge (2016) propose a method to produce compact 3D city models composed of ground and building objects from stereo pairs of satellite images. They represent the scene using convex polygons and perform joint classification and reconstruction of the semantic class (ground, roof, and facade) and the elevation of each polygon. Although their evaluation shows that the obtained results are not as accurate as LiDAR scans, the proposed method can produce fast, compact, and semantic-aware models robust to low resolution and occlusion problems.
- 空气和街道等级：Fruh et al。 （2005）将一系列垂直二维表面扫描和摄像机图像注册到机载数据（DSM），以生成城市的纹理外观网格。他们提出了一类数据处理技术，通过去除嘈杂的前景物体并填充建筑立面的几何和纹理中的孔，来创建视觉上吸引人的立面网格。 B'odis-Szomor'u等（2016）指出，机载和移动地图数据提供了补充信息，需要一起利用，以便制作完整和详细的大型城市模型。机载传感器可以大规模获得屋顶结构，地面和植被，而通过多视角立体声方式或LiDAR的道路移动地图可提供立面和街道细节。他们提出
一种解决方案，用于融合详细的道路上移动地图以及从联合表面网格中的空中采集获取更粗糙但更完整的点云。他们的评价表明，通过将街头细节融入机载模型，模型的质量大大提高。
- 立体声卫星：Duan＆Lafarge（2016）提出了一种制作紧凑型3D城市模型的方法，由地面和建筑物体组成的立体声对卫星图像。它们使用凸多边形代表场景，并进行语义类（地面，屋顶和立面）的联合分类和重建以及每个多边形的高程。虽然他们的评估表明所获得的结果不如LiDAR扫描的准确性，但是所提出的方法可以产生对低分辨率和闭塞问题坚固的快速，紧凑和语义感知模型。

- 7.3. Reconstruction and Recognition
- In autonomous driving, it is important to understand both the structural and semantic information of the surroundings. Traditionally, image segmentation methods employ priors entirely in the 2D image domain, i.e., spatial smoothness terms, and reconstruction methods usually encourage piecewise smooth surfaces. It has been long argued that semantics and 3D reconstruction carry valuable information to each other. Similarly to stereo, the motivation to incorporate semantics in reconstruction is photo-consistency failing in case of imperfect and ambiguous image information due to specularities, lack of texture, repetitive structures, or strong lighting changes. Semantic labels provide geometric cues about likely surface orientations at a certain location and help resolving inherent ambiguities. 3D reconstruction lifts the reasoning from 2D to 3D and acts as a strong regularizer by enforcing geometric consistency over multiple images for segmentation.
- Planarity and Primitives: Micusik & Kosecka (2009) present a method to overcome these difficulties by exploiting image segmentation cues as well as presence of dominant scene orientations and piecewise planar structures. In particular, they adopt a super-pixel based dense stereo reconstruction method by using the Manhattan world assumption with three orthogonal plane normals in the MRF formulation. Another way of exploiting piecewise planar structures and the shape repetition is to use primitives such as planes, spheres, cylinders, cones and tori (Lafarge et al. (2010); Lafarge & Mallet (2012); Lafarge et al. (2013)). Primitive arrangement-based approaches provide compactness and reduce complexity. However, they remain simplistic representations and fail to model fine details and irregular shapes. Therefore, Lafarge et al. (2013) propose a hybrid approach which is both compact and detailed. Starting from an initial mesh-based reconstruction, they use primitives for regular structures such as columns and walls, while irregular elements are still described by meshes for preserving details.
- 在自主驾驶中，了解周边环境的结构和语义信息很重要。传统上，图像分割方法在2D图像域中完全使用先验，即空间平滑度项，重建方法通常会促使分段平滑表面。长期以来，语义和3D重建相互传递有价值的信息。与立体声类似，在重建中纳入语义的动机是由于镜面反射，缺乏纹理，重复结构或强烈的照明变化而导致的图像信息不完整和模糊的情况下的照片一致性失败。语义标签提供关于某个位置处可能的表面取向的几何线索，并帮助解决固有的模糊性。 3D重建将推理从2D提升到3D，并通过对多个图像执行几何一致性进行分割，并作为强正则化器。
平面性和原始性：Micusik＆Kosecka（2009）提出了一种通过利用图像分割线索以及主要场景取向和分段平面结构的存在来克服这些困难的方法。特别地，它们采用基于超像素的密集立体重建方法，通过在MRF公式中使用具有三个正交平面法线的曼哈顿世界假设。使用分段平面结构和形状重复的另一种方法是使用诸如平面，球体，圆柱体，锥体和托里（Lafarge等人（2010）; Lafarge＆Mallet（2012）; Lafarge等人（2013））的原语。基于原始布置的方法提供了紧凑性并降低了复杂性。然而，它们仍然是简单的表示，并且不能对精细细节和不规则形状进行建模。因此，拉法基等（2013）提出一种既紧凑又详细的混合方法。从初始的基于网格的重建开始，它们使用基本原理用于常规结构，例如列和墙壁，而不规则元素仍由网格描述以保留细节。

- Volumetric: Volumetric scene reconstruction typically segments the volume into occupied and free-space regions. Haene et al. (2013) present the mathematical framework to extend it to a multi-label volumetric segmentation framework which assigns object classes or a free-space label to voxels as shown in Figure 22. They first learn appearance likelihoods and classspecific geometry priors for surface orientations from the training data. Then, these data-driven priors are used to define unary and pairwise potentials in a continuous formulation for volumetric segmentation. Joint reasoning benefits from typical class-specific geometry, such as the normals of the ground plane pointing upwards. In addition, it provides a class-specific smoothness prior in cases of weak cues for the scene geometry. Their evaluation shows the benefit of such a prior over standard smoothness assumptions such as Total Variation.
- Zhou et al. (2015) propose a method for 3D reconstruction of street scenes from a sequence of fisheye cameras by introducing semantic priors. Motivated by recurring objects of similar 3D shapes in outdoor scenes, they first localize buildings and vehicles using 3D object detectors and then jointly reconstruct them while learning a volumetric model of their shape. This allows to reduce noise while completing missing surfaces as objects of similar shape benefit from all observations of the respective category.
- 体积：体积场景重建通常将体积分成占用和自由空间区域。 Haene等人（2013）提出了数学框架，将其扩展到多标签体积分割框架，该框架将对象类或自由空间标签分配给体素，如图22所示。他们首先学习表面取向的外观似然性和类特定几何先验从训练数据。然后，这些数据驱动的先验被用来定义连续公式中的一元和成对的电位，用于体积分割。联合推理受益于典型的类别特定几何，如地平面向上的法线。此外，在场景几何的弱提示的情况下，它提供了一个类别的平滑度。他们的评估显示了这样一个先前过度的标准平滑假设（如总变异）的好处。
- Zhou et al。 （2015）提出了一种通过引入语义先验从一系列鱼眼摄影机的街道场景3D重建的方法。由于户外场景中类似3D形状的反复出现的对象，他们首先使用3D物体检测器对建筑物和车辆进行本地化，然后在学习其形状的体积模型的同时重建它们。这允许在完成缺失表面时减少噪声，因为具有类似形状的对象受益于相应类别的所有观察。

- **Monocular Video**: Failures in multi-view stereo cause problems for approaches like Haene et al. (2013) which require dense depth measurements. Using a monocular image stream as input, Kundu et al. (2014) propose another joint reasoning approach over a sparse point cloud from SfM and dense semantic labeling of the frames. This way, 3D semantic representation is temporally coherent without additional cost. They model the problem with a higher order CRF in 3D which allows realistic scene constraints and priors such as 3D object support. In addition, they explicitly model the free space which provides cues to reduce ambiguities, especially along weakly supported surfaces. Their evaluation on monocular datasets Camvid and Leuven shows improved 3D structure compared to traditional SfM and state-of-the-art multi-view stereo as well as better segmentation quality over video segmentation methods in terms of both per pixel accuracy and temporal consistency.
- 单目视频：多视点立体声故障导致Haene等人等方法出现问题。 （2013年），需要深度测量。 使用单目图像流作为输入，Kundu et al。 （2014）提出了从SfM的稀疏点云和框架的密集语义标注的另一个联合推理方法。 这样，3D语义表示在时间上相干而没有额外的成本。 他们用3D中的高阶CRF对问题进行建模，这样可以实现场景约束和3D对象支持等先修。 此外，他们明确地模拟了提供线索的自由空间，以减少模糊，特别是在弱支撑的表面上。 他们对单目数据集的评估Camvid和Leuven显示出与传统SfM和最先进的多视点立体声相比改进的3D结构以及在每像素精度和时间一致性方面对视频分割方法的更好的分割质量。

- **Volumetric**: Large-scale: Previous works on semantic reconstruction (Haene et al. (2013); Kundu et al. (2014)) are limited to small scenes and low resolution, because of their large memory footprint and computational cost. To scale them up to large scenes, Blaha et al. (2016) point out that high resolution is not required for large regions such as free space, parts under the ground, or inside the building. They propose an extension of Haene et al. (2013) by employing an adaptive octree data structure with coarse-to-fine optimization, in an application to generate 3D city models from terrestrial and aerial images. Starting from a coarse voxel grid, they solve a sequence of problems in which the solution is gradually refined only near the predicted surfaces. The adaptive refinement saves memory and runs much faster while still being as accurate as the fixed voxel discretization at the highest target resolution, both in geometric reconstruction and semantic labeling.
- Besides the spatial extent, the number of different semantic labels is also a problem for scalability due to increasing memory requirements. The complexity is quadratic in the number of labels due to indicator variables for the transitions between the different labels. Cherabier et al. (2016) propose to divide the scene into blocks in which only a set of relevant labels is active, since absence of many semantic classes from a specific block can be determined early on. Accordingly, they can deactivate a label right from the beginning of the optimization which leads to a more efficient processing. The set of active labels in each block is updated during the iterative optimization to recover from wrong initializations. Their evaluation shows that they can increase the number of labels from six to nine with a significant gain in memory compared to Haene et al. (2013).
- 体积：大规模：以前的语义重建（Haene et al。（2013）; Kundu et al。（2014））仅限于小场景和低分辨率，因为它们具有较大的内存占用和计算成本。为了将它们扩大到大型场景，Blaha等（2016年）指出，大型地区，如自由空间，地下部分，建筑物内部都不需要高分辨率。他们提出延长Haene等（2013）通过在从地面和航空图像生成3D城市模型的应用中采用具有粗略到优化的自适应八叉树数据结构。从粗体素网格开始，它们解决了一系列问题，其中解决方案仅在预测表面附近逐渐完善。自适应细化可以节省内存并且运行得更快，同时在几何重建和语义标注中仍然与最高目标分辨率下的固定体素离散度一样准确。
- 除空间范围外，由于内存需求的增加，不同语义标签的数量也是可扩展性的问题。由于不同标签之间的转换的指示符变量，标签数量的复杂度是二次方。 Cherabier等人（2016）提出将场景划分为只有一组相关标签是活动的块，因为可以在早期确定不存在来自特定块的许多语义类别。因此，它们可以从优化开始就停用标签，这导致更有效的处理。每个块中的活动标签集在迭代优化期间被更新，以从错误的初始化中恢复。他们的评估表明，与Haene等人相比，他们可以将标签的数量从6个增加到9个，记忆显着增加。 （2013年）。

- **Shape Priors**: Advances in sensors to acquire 3D shapes and the performance of object detection algorithms have encouraged the use of 3D shape priors in 3D reconstruction. Dimensionality reduction is an effective and popular way of representing shape knowledge. Early approaches use linear dimensionality reduction such as PCA to capture the shape variance in low dimensional latent shape spaces. More recent approaches use nonlinear dimensionality reduction such as Gaussian Process Latent Variable Models (GP-LVM) (Dame et al. (2013)). Dame et al. (2013) investigate the importance of shape priors in a monocular SLAM approach. In parallel with depth estimation, they refine an object’s pose, shape and scale to match an initial segmentation and depth cues. It is finally fused into the volumetric representation. Their experiments show improvement in transparent and specular surfaces, and even in unobserved parts of the scene. In addition to mean shape, Bao et al. (2013) propose to learn a set of anchor points as representative of object shape across several instances. They first perform an initial alignment using 2D object detectors. Next, they align the point cloud from SfM with the mean shape by matching anchor points, and then warp and refine it to approach the actual shape. Their evaluation demonstrates that the model is general enough to learn semantic priors for different object categories such as car, fruit, and keyboard by handling large shape variations across instances.
- While previous approaches (Dame et al. (2013); Bao et al. (2013)) try to fit a parametric shape model to input data, Haene et al. (2014) model the local distribution of normals for an object. They propose an object class specific shape prior in the form of spatially varying anisotropic smoothness terms. Similar to multi-label segmentation approach of Haene et al. (2013), they divide the reconstruction into object region and the supporting ground and apply the shape prior only on object to guide the optimization to the right shape.
- 先前的形状：获取3D形状的传感器的进步和对象检测算法的性能已经鼓励在3D重建中使用3D形状先验。尺寸减少是表示形状知识的有效和受欢迎的方式。早期方法使用线性维数降低（如PCA）来捕获低维隐形形状空间中的形状方差。更近期的方法使用非线性维数降低，如高斯过程潜在变量模型（GP-LVM）（Dame等（2013））。 Dame等人（2013）研究形状先验在单眼SLAM方法中的重要性。与深度估计并行，它们优化对象的姿态，形状和尺度以匹配初始分割和深度线索。它最终融合到体积表示中。他们的实验显示透明和镜面表面的改进，甚至在场景的未观察的部分。除了平均形状外，Bao et al。 （2013）提出在几个实例中学习一组锚点作为对象形状的代表。他们首先使用2D物体检测器进行初始对准。接下来，他们通过匹配锚点将SfM的点云与平均形状对齐，然后扭曲和细化以接近实际形状。他们的评估表明，该模型足以通过处理实例之间的大的形状变化来学习不同对象类别（如汽车，水果和键盘）的语义预测。
- 尽管以前的方法（Dame等人（2013）; Bao等（2013））试图将参数化形状模型拟合为输入数据，Haene等人（2014）模拟了一个对象的法线的局部分布。他们以空间变化各向异性平滑度项的形式提出了对象类特定形状。类似于Haene等人的多标签分割方法（2013），他们将重建划分为物体区域和支撑地面，然后仅在物体上应用形状，将优化引导到正确的形状。

- **Data-Driven**: Instead of modeling a semantic prior for each object explicitly, Wei et al. (2014) propose a data-driven regularization to transfer the shape information of the disparity or flow from semantically matched patches in the training database using the SIFT flow algorithm. They represent the shape information as the relative relationship of scene properties instead of absolute values. It is mainly for reusability of scene properties, such as modeling disparity of car independent of its position. They compare their data-driven prior against popular smoothness terms on Sintel and show improved performance while being comparable to state-of-the-art on KITTI.
- 数据驱动：而不是明确地为每个对象建模一个语义先验，Wei et al。 （2014）提出了一种数据驱动的正则化方法，使用SIFT流程算法从训练数据库中的语义匹配补丁中传输视差或流量的形状信息。 它们将形状信息表示为场景属性而不是绝对值的相对关系。 它主要用于场景属性的可重用性，如独立于其位置的汽车模型差异。 他们比较了他们的数据驱动先前与Sintel上的流行平滑度条款，并显示出改进的性能，同时与KITTI上的最先进技术相媲美。

### 8. Motion & Pose Estimation 运动与姿势估测
- 8.1. 2D Motion Estimation – Optical Flow  2D运动估测 - 光流
- Optical flow is defined as the two-dimensional motion of brightness patterns between two images. This definition only represents motion of intensities in the image plane but not the 3D motion of the objects in the scene. Recovering the 3D motion itself is the goal in Scene Flow discussed in Section 8.2.Figure 23 shows the synthetic Yosemite sequence with the optical flow ground truth generated by texture mapping aerial images of Yosemite valley on depth maps of the valley. Optical flow provides important information about the scene and serves as input for several tasks such as ego-motion estimation (Section 8.3), structure-from-motion and tracking (Section 9). The research on this problem started several decades ago with the variational formulation by Horn & Schunck (1981) assuming the brightness of a pixel to be constant over time. Optical flow is an inverse problem in which insufficient information is given to fully specify the solution. The brightness at a pixel provides only one constraint while the unknown motion vector has two components. This is known as the aperture problem and can only be solved by introducing an additional constraint which is usually a smoothness assumption encouraging similar motion vectors between neighboring pixel. Despite the long history of the optical flow problem, occlusions, large displacement and fine details are still challenging for modern methods. A fundamental problem with the optical flow definition is that besides the actual motion of interest, illumination changes, reflections and transparency can also cause intensity changes besides the motion.
- 光流被定义为两个图像之间的亮度图案的二维运动。该定义仅表示图像平面中的强度运动，而不表示场景中物体的3D运动。恢复3D运动本身是第8.2节中讨论的场景流程中的目标。图23显示了由优胜美地山谷的纹理映射生成的合成优胜美地序列，其深谷图的深度图。光流提供关于场景的重要信息，并且用作诸如自主运动估计（8.3节），运动结构和跟踪（9）等几项任务的输入。关于这个问题的研究在几十年前就开始，而Horn＆Schunck（1981）的变分公式假定像素的亮度随着时间的推移是恒定的。光流是一个反向问题，其中没有足够的信息来完全指定解决方案。像素处的亮度仅提供一个约束，而未知运动矢量具有两个分量。这被称为孔径问题，并且只能通过引入额外的约束来解决，附加约束通常是鼓励相邻像素之间的相似运动矢量的平滑假设。尽管光流问题的历史悠久，但现代方法仍然存在堵塞，大排量和细微细节。光流定义的一个根本问题是，除了感兴趣的实际运动之外，照明变化，反射和透明度也可能导致运动以外的强度变化。

- **Variational Formulation**: Traditionally, the optical flow problem has been approached with a variational formulation. Variational methods minimize an energy consisting of a data term, assuming little appearance change over time, and a smoothness term, encouraging similarity between spatial neighbors. Horn & Schunck (1981) introduced the brightness constancy assumption which models the intensity value of a pixel as constant over time. Considering one pixel this assumption yields one equation with two unknowns that cannot be solved as such (aperture problem). To estimate the optical flow an additional constraint is necessary. A common way of regularizing variational optical flow estimation is to encourage similarity of spatially neighboring flow vectors. This prior is motivated by the fact that flow fields are often smooth and discontinuities typically occur only at object boundaries. The original formulation by Horn & Schunck (1981) uses a quadratic penalty function in the data and smoothness term. This has the major limitation that violations of the brightness constancy assumption, like varying illumination conditions, can not be handled. One very popular way to alleviate this problem is using a robust penalty function as proposed by Black & Anandan (1993). In addition, several different data terms have been proposed that are less affected by illumination changes. Vogel et al. (2013) systematically evaluates pixel- and patch-based data costs in a unified testbed on the KITTI dataset (Geiger et al. (2012b)). On real data, they found patch-based terms to perform better than pixel-based terms. Another limitation of the original formulation by Horn & Schunck (1981) is that the homogeneous non-robust smoothness term does not allow flow discontinuities. However, in real world scenes different objects often cause optical flow discontinuities at their boundaries thus violating this assumption. Total Variation regularization used in Zach et al. (2007) replaces the quadratic penalization by the L1 norm to preserve discontinuities in the flow field. A remaining disadvantage of this model is that it favors fronto-parallel surfaces which is not a realistic assumption for real-world scenes. Thus, higher-order regularizations like the Total Generalized Variation (TGV) model have been proposed by Bredies et al. (2010). TGV priors can better represent real data as they leverage a piecewise affine motion model. The non-local Total Generalized Variation by Ranftl et al. (2014) is an extension of this model which enforces the piecewise affine assumption in a local neighborhood. They observed that considering only direct neighbors leads to a decrease of performance in regions where the data term is ambiguous. Zimmer et al. (2011) provide a detailed assessment of imageand flow-driven regularizers for the variational formulation and discuss the qualities of different data terms. Besides the model specifications, the choice of the optimization method and its implementation are additional factors which influence the performance of variational optical flow estimation algorithms.  detailed study of optical flow methods is provided by Sun et al. (2014). They uncover the reasons for the success of modern optical flow methods and propose an approach optimizing a classical formulation with modern techniques.
- 变异配方：：传统上，光流问题已经用变分公式进行了解。变数方法将由数据项组成的能量最小化，假定随着时间的推移几乎没有出现变化，并且平滑度项令人鼓励空间邻居之间的相似性。 Horn＆Schunck（1981）引入了亮度恒定性假设，其将像素的强度值随时间推定为恒定。考虑到一个像素，这个假设产生一个方程，其中有两个不能被解决的未知数（孔问题）。为了估计光流量，需要额外的约束。变分光学流量估计的常规方法是鼓励空间相邻流向量的相似性。这一事实是由于流场通常是平滑的，并且不连续通常仅在物体边界处发生。 Horn＆Schunck（1981）的原始方法在数据和平滑度项中使用二次惩罚函数。这样做的主要限制是不能处理亮度恒定假设的违规，如不同的照明条件。减轻这个问题的一个非常受欢迎的方法是使用Black＆Anandan（1993）提出的强大的惩罚函数。此外，已经提出了几个不同的数据项，其受照明变化的影响较小。 Vogel等（2013）系统评估KITTI数据集统一测试平台中的像素和补丁数据成本（Geiger等（2012b））。在实际数据中，他们发现基于片段的术语比基于像素的术语更好。 Horn＆Schunck（1981）的原始方法的另一个限制是，均匀非鲁棒平滑度项不允许流量不连续性。然而，在现实世界的场景中，不同的对象常常会在其边界引起光流不连续，从而违反了这一假设。 Zach等人使用的总变异正则化（2007）用L1范数取代二次惩罚，以保持流场中的不连续性。这种模式的一个缺点是，它有利于前平行表面，这对于现实世界的场景来说不是一个现实的假设。因此，Bredies等人已经提出了诸如总广义变化（TGV）模型的高阶正则化。 （2010年）。 TGV优先级可以更好地代表实际数据，因为它们利用分段仿射运动模型。 Ranftl等人的非局部总广义变异（2014）是这个模式的延伸，它强制在当地社区进行分段仿射假设。他们观察到，只考虑直接邻居导致数据项不明确的地区的业绩下降。 Zimmer等人（2011）提供了对变分公式的图像和流量驱动的正则化器的详细评估，并讨论了不同数据项的质量。除了模型规范，优化方法的选择及其实现还是影响变分光流估计算法性能的附加因素。 Sun等人提供了光学流动方法的详细研究。 （2014）。他们揭示了现代光流方法成功的原因，并提出了一种利用现代技术优化古典配方的方法。

- **Sparse Matches**: One major challenge, in particular for variational methods, is the estimation of large displacements since usually linear approximations are used that only hold in case of pixel motion. This problem is typically addressed with a coarse-to-fine strategy, estimating the flow on a coarser resolution to initialize the estimation on a finer resolution. While this strategy works for large structures of little complexity, fine geometric details are often lost in the process. Besides, textural details important for correspondence estimation are lost at coarse resolutions, hence leading the optimizer to a local minimum. One example for the loss of fine details is illustrated with a fast moving hand in Figure 24. These problems can be alleviated by integrating sparse features into the variational formulation as proposed by Brox & Malik (2011). The feature matches, obtained from nearest neighbor search on a coarse grid, are used as soft constraint in a coarse-to-fine optimization. While in Figure 24 the warping methods fail to recover the optical flow for the hand, the feature matches lead the optimization to the right solution. Another possibility to deal with large displacements is suggested by Revaud et al. (2015). They replace the coarse-tofine strategy with an interpolation of sparse matches to initialize a dense optimization at full resolution. Sparse matches are obtained using DeepMatching, a deep neural network matching approach introduced by Weinzaepfel et al. (2013). In contrast to DeepMatching, Menze et al. (2015a) use approximate nearest neighbor search to generate a set of proposals as candidates to be used in a discrete optimization framework. Inference is made feasible by restricting the number of matches to the most likely ones with non-maxima suppression and exploiting the truncated form of the pairwise potentials. Motivated by the success of Siamese networks in stereo (Zbontar & LeCun (2016)) (see Section 7.1), Guney & Geiger (2016) extend this work to learning features for 2D patch matching. They further investigate the importance of the receptive field size exploiting dilated convolutions as proposed by Yu & Koltun (2016) for semantic segmentation. Chen & Koltun (2016) argue that the heuristic pruning used to make inference feasible destroys the highly regular structure of the space of mappings and propose a discrete optimization over the full space. Min-convolutions are used to reduce the complexity and to effiectively optimize the large label space using a modified version of Tree-Reweighted Message Passing by Kolmogorov (2006). Wulff&Black (2015) present a different approach to obtain dense optical flow from sparse matches. In their approach, the optical flow field is represented as a weighted sum of basis flow fields learned from reference flow fields which have been estimated from Hollywood movies. They estimate the optical flow by finding the weights which minimize the error with respect to the detected sparse feature correspondences. While this results in overly smooth flow fields, the approach is very fast. Besides, a slower layered approach has been approached which better handles flow discontinuities.
- **稀疏匹配**：特别是变分方法的一个主要挑战是大位移的估计，因为通常使用仅在像素运动的情况下使用线性近似。这个问题通常用粗略到精细的策略来解决，估计更粗糙的分辨率上的流量来初始化更精细分辨率的估计。虽然这个策略适用于很小复杂性的大型结构，但是在这个过程中经常会丢失精细的几何细节。此外，对于通信估计重要的纹理细节在粗分辨率下丢失，因此导致优化器达到局部最小值。 Brox＆Malik（2011）提出的将稀疏特征整合到变分公式中可以缓解这些问题。在粗网格上从最近邻搜索获得的特征匹配，在粗到精优化中被用作软约束。而在图24中，翘曲方法无法恢复手的光流，特征匹配将优化导向正确的解决方案。 Revaud等人提出了处理大排量的另一种可能性。 （2015）。它们用粗略匹配的插值来代替粗略到精细策略，以在全分辨率下初始化密集优化。使用DeepMatching（Weinzaepfel等人介绍的深层神经网络匹配方法）获得稀疏匹配。 （2013年）。与DeepMatching相反，Menze等（2015a）使用近似最近邻搜索来生成一组提案作为在离散优化框架中使用的候选。通过将匹配次数限制为最有可能的匹配次数，通过非最大抑制和利用成对电位的截断形式，可以推断推理。由于暹罗网络在立体声（Zbontar＆LeCun（2016））中的成功（见第7.1节），Guney＆Geiger（2016）将此工作扩展到2D补丁匹配的学习功能。他们进一步研究了Yu＆Koltun（2016）提出的扩展卷积的接受场大小对语义分割的重要性。 Chen＆Koltun（2016）认为，用于推理可行性的启发式修剪破坏了映射空间的高度规则结构，并提出了在全部空间上的离散优化。 Min-convolutions用于降低复杂性，并使用Kolmogorov（2006）传递的Tree-Reweighted Message的修改版本来有效优化大标签空间。 Wulff＆Black（2015）提出了一种从稀疏匹配获得密集光流的不同方法。在他们的方法中，光流场被表示为从已经从好莱坞电影估计的参考流场获得的基本流场的加权和。它们通过找到相对于检测到的稀疏特征对应使误差最小化的权重来估计光流。虽然这导致流场过于流畅，但方法非常快。此外，已经采用较慢的分层方法，其更好地处理流动不连续性。

- **High Speed Flow**: With some exceptions (Wulff & Black(2015); Timofte&Gool (2015);Weinzaepfel et al. (2013); Farneback (2003); Zach et al. (2007)) most of the optical flow approaches are very inefficient and can not be applied in real-time which is necessary for applications in autonomous driving. The trade-off between accuracy and speed for different algorithms on KITTI 2012 benchmark Geiger et al. (2012b) is illustrated in Figure 25. The methods based on variational inference yield the best accuracy, however belong to the slowest set of methods for motion estimation. However, the duality based approach for total variation optical flow proposed by Zach et al. (2007) allows an efficient GPU implementation that performs in real-time (30 Hz) on a resolution of 320 x 240. Sparse matching approaches are usually more efficient than variational formulations but often need variational refinement as post processing step to achieve subpixel precision. The recent introduction of deep learning to the optical flow problem yielded several almost real-time approaches (Dosovitskiy et al. (2015); Ranjan & Black (2016)) including Ilg et al. (2016) which achieves state-of-the-art performance on popular datasets. These methods will be discussed below. The approach proposed by Kroeger et al. (2016) allows to trade-off accuracy and computational time. They realize fast patch correspondences with inverse search and obtain a dense flow field with the aggregation of patches along multiple scales. This allows them to estimate optical flow with up to 600 Hz at the cost of accuracy.
- 高速流：除了一些例外（Wulff＆Black（2015）; Timofte＆Gool（2015）; Weinzaepfel等（2013）; Farneback（2003）; Zach et al。（2007））大部分光流方法非常低效，不能实时应用，这对于自主驾驶中的应用是必需的。在KITTI 2012基准测试Geiger等人的不同算法的精度和速度之间的权衡（2012b）如图25所示。基于变分推理的方法产生最佳精度，但属于运动估计方法的最慢组。然而，Zach等人提出的基于双变量的全变量光流的方法（2007）允许在320 x 240的分辨率下实时（30 Hz）执行高效的GPU实现。稀疏匹配方法通常比变分公式更有效，但通常需要变分细化作为后处理步骤来实现子像素精度。最近对光流问题的深入学习引入了几种几乎实时的方法（Dosovitskiy等（2015）; Ranjan＆Black（2016）），其中包括Ilg等。 （2016），其在流行数据集上实现了最先进的性能。这些方法将在下面讨论。 Kroeger等人提出的方法（2016）允许权衡精度和计算时间。它们通过反向搜索实现快速补片对应，并通过多个尺度的斑块聚合获得稠密的流场。这允许他们以高达600Hz的精度估计光流。

- **State-of-the-art**: Currently, Sintel Butler et al. (2012) and KITTI Geiger et al. (2012b, 2013) discussed in Section 2 are the most popular datasets for the evaluation of optical flow algorithms. However, in this survey we focus on the autonomous driving application. Therefore, we will only refer to the KITTI leaderboard when we compare methods. Still, optical flow approaches not specifically designed for autonomous driving have a similar ranking on Sintel. In Table 7 we show the leaderboard for the KITTI 2015 benchmark. The performance of methods is assessed using the percentage of outliers, which are flow vectors with the absolute endpoint error (EPE) exceeding 3 pixel and 5% of its true values. The percentage of outliers is averaged over background (Fl-bg), foreground (Fl-fg) and all regions (Fl-all). In addition, the density of the output flow field and the runtime are provided. The best performing methods either learn optical flow end-to-end (Ilg et al. (2016)) or use semantic segmentation to split the scene into independently moving objects Bai et al. (2016); Sevilla-Lara et al. (2016). The best performing approach FlowNet2 (Ilg et al. (2016)) trains a deep neural network to solve the optical flow problem.
- 最先进的技术：目前，Sintel Butler等人（2012）和KITTI Geiger等人（2012b，2013）在第2节中讨论的是用于评估光流算法的最流行的数据集。然而，在本次调查中，我们专注于自主驾驶应用。因此，当我们比较方法时，我们只会参考KITTI排行榜。尽管如此，未专门为自主驾驶而设计的光流方法在Sintel上具有相似的排名。在表7中，我们显示了KITTI 2015基准测试的排行榜。使用异常值的百分比来评估方法的性能，其中绝对端点误差（EPE）超过3个像素的流向量和其真实值的5％。异常值的百分比在背景（Fl-bg），前景（Fl-fg）和所有区域（Fl-all）上平均。此外，还提供了输出流场的密度和运行时间。最佳表现的方法是学习光流端到端（Ilg et al。（2016）），或者使用语义分割来将场景分解成独立移动的对象Bai et al。 （2016）; Sevilla-Lara等人（2016）。 FlowNet2（Ilg et al。（2016））训练一个深层神经网络来解决光流问题。

- **Epipolar Flow**: In the context of autonomous driving, simplifying assumptions can be used to alleviate the optical flow problem. The assumption of a static scene or the decomposition of a scene into rigidly moving objects allow to treat optical flow as matching problem along epipolar lines radiated from the focus of expansion. Yamaguchi et al. (2013) propose a slanted-plane Markov random field that represents the epipolar flow of each segment with slanted planes. This formulation needs a time consuming optimization and can be avoided with the joint stereo and flow formulation of Yamaguchi et al. (2014). They assume the scene to be static and present a new semi global block matching algorithm using the joint evidence of stereo and video. This formulation allows them to rank third in KITTI 2012 while being 10 times faster than the best performing method. In contrast to these approaches, Bai et al.(2016) use the slanted plane model only for background flow estimation. An instance-segmentation allows them to formulate an independent epipolar flow estimation problem for each moving object. While for KITTI 2012 the advantage of this formulation is not evident because of the static scene, on KITTI 2015 which comprises dynamic scenes they achieve better results (Table 7).
- 对极流：在自主驾驶的背景下，可以使用简化假设来减轻光流问题。将静态场景或将场景分解为刚性移动物体的假设允许将光流作为从扩展焦点辐射的沿线的匹配问题来处理。山口等（2013）提出了一个斜面马尔可夫随机场，其表示具有倾斜平面的每个段的对极流。该配方需要耗时的优化，并且可以通过Yamaguchi等人的联合立体声和流量配方来避免。 （2014）。他们假设场景是静态的，并使用立体声和视频的联合证据呈现新的半全局块匹配算法。这种配方使他们在KITTI 2012中排名第三，而且比最佳性能方法快10倍。与这些方法相反，Bai等（2016）将倾斜平面模型仅用于背景流估计。实例分割允许它们为每个移动对象制定独立的对极流估计问题。而对于KITTI 2012来说，由于静态场景，KITTI 2015的优势并不明显，KITTI 2015包含动态场景，从而获得更好的效果（表7）。

- **Semantic Segmentation**: Scenes in the context of autonomous driving are usually composed of a static background and dynamic moving traffic participants. This observation can be exploited by splitting the scene into independently moving objects. As mentioned above, Bai et al. (2016) extract traffic participants using instance-level segmentation and estimate the optical flow independently for different instances. Sevilla-Lara et al. (2016) use semantic segmentation for optical flow estimation in several ways: on one hand, semantics provide information on object boundaries as well as spatial relationships between objects that are used to reason about depth ordering. On the other hand, the division of the scene allows Sevilla-Lara et al. (2016) to exploit different motion models according to the respective object type, similar to Bai et al. (2016). The motion of planar regions is modeled with homographies, whereas independently moving objects are modeled by affine motions allowing for deviations. Complex objects like vegetation are modeled with a classical spatially varying dense flow field. Finally, the constancy of object identities over time is used to encourage temporal consistency of the optical flow.
- 语义分割：在自主驾驶的上下文中的场景通常由静态背景和动态移动交通参与者组成。这种观察可以通过将场景分解为独立移动的对象来利用。如上所述，Bai et al。 （2016）使用实例级分割提取流量参与者，并针对不同的实例独立地估计光流。 Sevilla-Lara等人（2016）以多种方式使用语义分割进行光流估计：一方面，语义提供关于对象边界的信息以及用于深入排序的对象之间的空间关系。另一方面，场景的划分允许Sevilla-Lara等人（2016）根据各自的对象类型开发不同的运动模型，类似于Bai等。 （2016）。平面区域的运动是用同形图建模的，而独立运动的物体是通过允许偏差的仿射运动进行建模的。复杂的物体，如植被，建立在经典的空间变化密集流场。最后，随着时间的推移，对象身份的一致性被用来促进光流的时间一致性。

- **Confidences**: Considering the remaining challenges in optical flow, a confidence measure to assess the quality of the estimated flow is desirable. Several measures based on spatial and temporal gradients have been proposed (Uras et al. (1988); Anandan (1989); Simoncelli et al. (1991)) that quantify the difficulty to estimate flow for a specific image. In contrast, algorithm-specific measures (Bruhn & Weickert (2006); Kybic & Nieuwenhuis (2011)) have been proposed which give a confidence for an estimation only for a specific group of methods. Learning-based measures like Kondermann et al. (2007, 2008) learn a model that relates flow algorithm success to spatio-temporal image data or the computed flow field. A detailed evaluation of different confidence measures is given by Mac Aodha et al. (2013). In addition, they present another learning based approach which uses multiple feature types such as temporal, texture, distance from images edges, and others, to estimate confidences for the success of a given method.
- 置信度：考虑到光流中存在的其他挑战，需要一种评估估计流量质量的置信度量。已经提出了基于空间和时间梯度的几种措施（Uras等人（1988）; Anandan（1989）; Simoncelli等人（1991）），其量化了针对特定图像估计流动的难度。相比之下，已经提出了算法特异性测量（Bruhn＆Weickert（2006）; Kybic＆Nieuwenhuis（2011）），其对于仅针对特定组的方法进行估计而置信度。基于学习的措施，如Kondermann et al。 （2007,2008）学习了一种将流程算法成功与时空图像数据或计算流场相关联的模型。 Mac Aodha等人给出了不同信心度量的详细评估。 （2013年）。此外，他们提出另一种基于学习的方法，其使用多个特征类型，例如时间，纹理，与图像边缘的距离等，以估计给定方法的成功的置信度。

- **Deep Learning**: Most optical flow approaches do not incorporate any high-level information like semantics which makes it hard to resolve ambiguities. The knowledge about objects and their material property can be used to model reflectance and transparency which would allow to be unaffected by these phenomena. The recent success of convolutional neural networks to learn high-level information have led to the attempt of using them for the optical flow problem. Dosovitskiy et al. (2015) presented FlowNet to learn optical flow end-to-end using a CNN. FlowNet consists of a contracting part which extracts important features and an expanding part which produces the high resolution flow. They propose two different architectures: a simple network stacking the images and a complex network correlating features of the separately processed images. One problem in learning optical flow is the limited amount of training data. KITTI 2012 Geiger et al. (2012b) and KITTI 2015 Menze & Geiger (2015) only provide around 200 training examples each while Sintel Butler et al. (2012) has 1041 training image pairs. Since these datasets are too small to train large CNNs, Dosovitskiy et al. (2015) created the Flying Chairs dataset by rendering 3D chair models on top of images from Flickr. This first attempt to end-to-end optical flow learning demonstrated that it was possible to learn optical flow but could not reach state-of-the art performance on KITTI (Table 7) or Sintel. However, compared to methods performing at almost real-time they were the best performing. In contrast to the contracting and expanding networks of Dosovitskiy et al. (2015), Ranjan & Black (2016) present SpyNet, an architectur  inspired by the coarse-to-fine matching strategy leveraged in traditional optical flow estimation techniques. Each layer of the network represents a different scale and only estimates the residual flow with respect to the warped image. This formulation allowed them to achieve similar performance as FlowNet while being faster. Being 96 % smaller than FlowNet one major contribution was the memory efficiency which makes it attractive for embedded systems. Ilg et al. (2016) present FlowNet2, an improved version of FlowNet, by stacking the architectures and fusing the stacked network with a subnetwork specialized on small motions. Similar to SpyNet, they also input the warped image into the stacked networks. However, each stacked network estimates the flow between the original frames instead of the residual flow as in SpyNet. In contrast to FlowNet and SpyNet, they use the FlyingThings3D dataset (Mayer et al.(2016)) consisting of 22k renderings of static 3D scenes with moving 3D models from ShapeNet dataset (Savva et al. (2015)). FlowNet2 performs on par with state-of-the-art methods on Sintel and outperforms all others on KITTI 2015 (Table 7) while being one of the fastest. They provide different network variants for the spectrum between 8fps and 140fps allowing the trade-off between accuracy and computational resources.
- 深度学习：大多数光流方法不包含任何高级信息，如语义学，这使得难以解决歧义。关于物体及其材料性质的知识可用于建模反射率和透明度，这将不会受到这些现象的影响。卷积神经网络近来在学习高级信息方面的成功导致了将其用于光流问题的尝试。 Dosovitskiy等人（2015）介绍了FlowNet，使用CNN学习光端机端到端。 FlowNet由一个收缩部分组成，它提取重要特征和扩展部分产生高分辨率流。他们提出了两种不同的架构：一个简单的网络堆叠图像和一个复杂的网络相关的单独处理的图像的功能。学习光流的一个问题是训练数据量有限。 KITTI 2012 Geiger et al。 （2012b）和KITTI 2015 Menze＆Geiger（2015）仅提供约200个培训示例，而Sintel Butler等（2012）拥有1041个训练图像对。由于这些数据集太小，无法训练大型CNN，Dosovitskiy et al。 （2015）通过在Flickr的图像之上渲染3D椅子模型创建了飞行椅数据集。这种端到端光流学习的第一次尝试表明，有可能学习光流，但无法达到KITTI（表7）或Sintel的最先进的性能。然而，与几乎实时执行的方法相比，它们表现最好。与Dosovitskiy等人的承包和扩展网络相反。 （2015），Ranjan＆Black（2016）目前是SpyNet，这是一种灵感来自于传统光流估计技术中的粗匹配匹配策略的架构。网络的每个层表示不同的比例，并且仅估计相对于翘曲图像的残余流量。这种配方使得它们在获得与FlowNet相似的性能的同时更快。比FlowNet小96％的主要贡献是内存效率，使嵌入式系统具有吸引力。 Ilg等人（2016）提供FlowNet2，FlowNet的改进版本，通过堆叠架构并将堆叠网络与专门用于小型运动的子网进行融合。与SpyNet类似，他们还将翘曲的图像输入堆叠网络。然而，每个堆叠的网络估计在原始帧之间的流量，而不是如SpyNet中的剩余流量。与FlowNet和SpyNet相反，他们使用由ShapeNet数据集（Savva et al。（2015））的移动3D模型构成的22k渲染静态3D场景的FlyingThings3D数据集（Mayer等（2016））。 FlowNet2与Sintel的最先进的方法保持一致，并且在KITTI 2015（表7）上胜过其他所有方法，同时是最快的。它们为8fps和140fps之间的频谱提供不同的网络变体，允许在精度和计算资源之间进行权衡。

- **Discussion**: Robust optical flow methods need to handle intensity changes not caused by the actual motion of interest but by illumination changes, reflections and transparency. In real world scenes, repetitive patterns and occlusions are frequent sources of errors. While illumination changes have been addressed with novel data terms (Black & Anandan (1993); Vogel et al. (2013)) the problems caused by reflection, transparency, ambiguities and occlusions remain largely unsolved. In Figure 26 we show the accumulated error of the 15 best performing methods on KITTI 2015 (Menze & Geiger (2015)). The highest error can be observed for regions moving outside the image domain. Untextured, reflective and transparent regions also result in large errors in many cases. A better understanding of the world is necessary to tackle these problems. Semantics (Bai et al. (2016); Sevilla-Lara et al. (2016)) and learned highcapacity models (Dosovitskiy et al. (2015); Ranjan & Black (2016); Ilg et al. (2016)) have already proven to improve optical flow estimation by resolving ambiguities in the data. In addition, scene flow methods which jointly reason about flow and depth have demonstrated encouraging performance.
- 讨论：坚固的光流方法需要处理不是由实际运动造成的强度变化，而是照明变化，反射和透明度。在现实世界的场景中，重复的模式和遮挡是常见的错误来源。虽然用新的数据术语（Black＆Anandan（1993）; Vogel等（2013））解决了照明变化，但反射，透明度，模糊和遮挡所引起的问题仍然很大程度上尚未得到解决。在图26中，我们显示了KITTI 2015（Menze＆Geiger（2015））上15种表现最好的方法的累积误差。对于在图像域外移动的区域，可以观察到最高的误差。在很多情况下，非纹理，反光和透明的区域也会导致大的错误。更好地了解世界是解决这些问题的必要条件。语义学（Bai et al。（2016）; Sevilla-Lara et al。（2016）），并学习了大容量模型（Dosovitskiy等（2015）; Ranjan＆Black（2016）; Ilg等（2016））已经证明通过解决数据中的模糊度来改善光流估计。另外，共同推动流量和深度的场景流动方法表现出令人鼓舞的表现。

- 8.2. 3D Motion Estimation – Scene Flow  3D 运动估测-场景流
- Stereo matching does not reveal any motion information, and optical flow from a single camera is not well constrained and lacks the depth information lost by the projection. On the other hand, humans are able to effortlessly integrate depth and motion cues from observations over time. That kind of reasoning is essential for many tasks in autonomous driving such as segmentation of moving objects in the 3D world. Scene flow generalizes optical flow to 3D, or alternatively, dense stereo to dynamic scenes. Given stereo image sequences, the goal is to estimate the three dimensional motion field that is a 3D motion vector for every point on every visible surface in the scene. The minimal setup for image-based scene flow estimation is given by two consecutive stereo image pairs as visualized in Figure 27. Establishing correspondences between the four images results in the 3D location of the surface point in both frames and hence fully describes the 3D motion of that surface point. A dense output is preferred, although there are some early sparse approaches for real-time purposes (Franke et al. (2005)). Scene flow shares some challenges with stereo and optical flow such as matching ambiguities in weakly textured regions and the aperture problem.
- 立体声匹配不会显示任何运动信息，并且来自单个摄像机的光流量没有受到很好的约束，并且缺少投影损失的深度信息。另一方面，人类能够随着时间的推移将观察结果的深度和运动线索轻松整合。这种推理对于自主驾驶中的许多任务至关重要，例如在3D世界中移动物体的分割。场景流将光流广泛化为3D，或将密集立体声广泛化为动态场景。给定立体图像序列，目标是估计作为场景中每个可见表面上每个点的三维运动矢量的三维运动场。基于图像的场景流估计的最小设置由图27中可视化的两个连续的立体图像对给出。建立四个图像之间的对应度导致两个帧中的表面点的3D位置，并且因此完全描述了三维运动那个表面点。尽管有一些早期的稀疏方法用于实时目的（Franke等人（2005）），密集输出是首选的。场景流动与立体声和光学流动有一些挑战，例如弱纹理区域中的匹配模糊度和孔径问题。

- **Variational Approaches**: Following the seminal work by Vedula et al. (1999), the problem is traditionally formulated in a variational setting where optimization proceeds in a coarse-tofine manner and local regularizers are leveraged to encourage smoothness in depth and motion. Wedel et al. (2008, 2011) propose a variational framework by decoupling the motion estimation from the disparity estimation while maintaining the stereo constraints. Starting from a precomputed disparity map at each time step, optical flow for the reference frame and disparity for the other view are estimated. The motivation for decoupling is mainly computational efficiency by choosing the optimal technique for each task. In addition, Wedel et al. (2011) propose a solution for varying lighting conditions based on residual images and provide an uncertainty measure which is shown to be useful for object segmentation. Rabe et al. (2010) integrate a Kalman filter to the decoupling approach for temporal smoothness and robustness.
- 变异方法：遵循Vedula等人的开创性工作。 （1999）中，问题传统上是在变化设置中进行的，其中优化以粗略到精细的方式进行，并且利用局部正则化来促进深度和运动的平滑度。 Wedel等（2008年，2011年）提出了一种变分框架，通过将运动估计与视差估计相结合，同时保持立体约束。从每个时间步长的预先计算的视差图开始，估计参考帧的光流和其他视图的视差。通过选择每个任务的最优技术，解耦的动机主要是计算效率。此外，Wedel等（2011）提出了一种基于残差图像改变照明条件的解决方案，并提供一种显示对对象分割有用的不确定性度量。 Rabe等人（2010）将卡尔曼滤波器整合到解耦方法中，以实现时间平滑和鲁棒性。

- **Piecewise Rigidity**: Similar to stereo and optical flow, prior assumptions about the geometry and motion can be exploited to better handle the challenges of the scene flow problem. Vogel et al. (2015) and Lv et al. (2016) represent the dynamic scene as a collection of rigidly moving planar regions as shown in Figure 28. Vogel et al. (2015) jointly recover this segmentation while inferring the shape and motion parameters of each region. They use a discrete optimization framework and incorporate occlusion reasoning as well as other scene priors in the form of spatial regularization of geometry, motion and segmentation. In addition, they reason over multiple frames by constraining the segmentation to remain stable over a temporal window. Their experiments show that their view-consistent multi-frame approach significantly improves accuracy in challenging scenarios, and achieves state-of-the-art performance on the KITTI benchmark (see Table 8). Using the same representation, Lv et al. (2016) focus on an efficient solution to the problem. They assume a fixed superpixel segmentation and perform optimization in the continuous domain for faster inference. Starting from an initialization based on Deep Matching, they independently refine the geometry and motion of the scene, and finally perform a global non-linear refinement using the Levenberg-Marquardt algorithm.
- 分段刚度：与立体声和光流相似，可以利用关于几何和运动的先前假设来更好地处理场景流问题的挑战。 Vogel等（2015）和Lv et al。 （2016）表示动态场景作为刚性移动平面区域的集合，如图28所示。Vogel等人（2015）共同回顾了这一分段，同时推断了每个地区的形状和运动参数。他们使用离散优化框架，并以几何，运动和分割的空间正则化的形式结合闭塞推理以及其他场景先验。此外，它们通过限制分割在多个帧上导致在时间窗口上保持稳定。他们的实验表明，他们的视图一致的多框架方法显着提高了挑战性场景的准确性，并在KITTI基准测试中获得了最先进的性能（见表8）。使用相同的表示，Lv et al。 （2016）专注于有效解决问题。他们假设固定的超像素分割，并在连续域中进行优化，以便更快的推理。从基于深度匹配的初始化开始，它们独立地改进场景的几何和运动，并且最终使用全局非线性细化Levenberg-Marquardt算法。

- **Piecewise Rigidity at the Object Level**: Menze & Geiger (2015) also follow a slanted plane approach, but in addition to Vogel et al. (2015); Lv et al. (2016), they model the decomposition of the scene into a small number of independently moving objects and the background. By conditioning on a superpixelization, they jointly estimate this decomposition as well as the rigid motion of the objects and the plane parameters of each superpixel in a discrete-continuous CRF. Compared to Vogel et al. (2015); Lv et al. (2016), they leverage a more compact representation, implicitly regularizing over larger distances. They also present a new scene flow dataset by annotating dynamic scenes from the KITTI raw data collection using detailed 3D CAD models. They further present an extension of this model in Menze et al. (2015b) where the pose and 3D shape of the objects are inferred in addition to the rigid motion and the segmentation. In particular, they incorporate a deformable 3D active shape model of vehicles into the scene flow approach.
- 对象级别的分段刚度：Menze＆Geiger（2015）也遵循斜面方法，但除了Vogel等人（2015）; Lv et al。 （2016），他们将场景的分解模型化为少量的独立移动物体和背景。通过调整超像素化，它们共同估计了离散连续CRF中的这种分解以及物体的刚性运动和每个超级像素的平面参数。与Vogel等人相比（2015）; Lv et al。 （2016），他们利用更加紧凑的表现，在更大的距离上隐含地规范化。他们还通过使用详细的3D CAD模型从KITTI原始数据收集中注释动态场景，呈现新的场景流数据集。他们进一步提出了这一模式在Menze等人的扩展。 （2015b）除了刚性运动和分割之外，推断物体的姿态和3D形状。特别地，它们将车辆的可变形3D活动形状模型结合到场景流动方法中。
-

- **State-of-the-art**: In Table 8, we show the ranking of methods on the KITTI scene flow 2015 benchmark (Menze & Geiger (2015)). The methods are compared according to the percentage of erroneous pixels. In particular, the columns show the percentage of stereo disparity outliers in first frame (D1), the percentage of stereo disparity outliers in second frame (D2), the percentage of optical flow outliers (Fl), and the percentage of scene flow outliers (SF), i.e. outliers in either D0, D1 or Fl. The outlier ratio separately for foreground/background regions can be found on the website of the benchmark30, it is omitted here for space reasons. The top performing methods (Vogel et al. (2015); Menze & Geiger (2015); Lv et al. (2016)) use the assumption of rigidly moving segments. In addition, Menze & Geiger (2015) model the motion of independently moving objects and perform better in FI and SF on foreground regions, but it takes longer than the other two. Lv et al. (2016) achieve good results faster by focusing on efficient optimization in continuous domain. Derome et al. (2016) propose a two stage approach on GPU which runs several order of magnitude faster than the other methods. First they compute the static flow using stereo and visual odometry and correct the dynamic flow with a realtime optical approach Plyer et al. (2014).
- 最先进的：在表8中，我们显示了KITTI现场流程2015基准测试方法的排名（Menze＆Geiger（2015））。根据错误像素的百分比对这些方法进行比较。特别地，列显示了第一帧（D1）中的立体声视差异常值的百分比，第二帧（D2）中的立体声视差异常值的百分比，光流异常值的百分比（F1）和场景流异常值的百分比SF），即D0，D1或F1中的异常值。前景/背景区域的异常值可以在基准测试30的网站上找到，因为空间原因在此省略。表现最好的方法（Vogel等人（2015）; Menze＆Geiger（2015）; Lv等人（2016））使用了刚性移动段的假设。此外，Menze＆Geiger（2015）模拟了独立移动物体的运动，并在前景区域的FI和SF中表现更好，但是需要比其他两个时间更长的时间。 Lv et al。 （2016）通过关注连续域的有效优化，更快地获得了良好的效果。 Derome等（2016）提出了GPU的两阶段方法，其运行速度比其他方法快几个数量级。首先，它们使用立体声和视觉测距法计算静态流量，并用实时光学方法校正动态流量Plyer等。 （2014）。

- **Discussion**: Scene flow estimation shares most of the challenges with stereo and optical flow, while integrating more information leading to better results. Ideally, methods should exploit depth and motion cues together to reason about dynamic 3D scenes. We show the accumulated errors of top 5 methods on KITTI scene flow benchmark in Figure 29. Car surfaces are the most problematic regions due to matching problems and independent motion of cars. Pixels close to the image boundary are another typical source of error, especially on the road surfaces in front of the car where large scale changes occur. Although local planarity and rigidity assumptions alleviate the problem, they are often violated due to complex geometric objects like vegetation, pedestrians or bicycles. Wrong estimation of planes, for example superpixels extending to multiple surfaces cause additional problems, especially at the boundaries of objects. Semantic image understanding could help with these issues, especially at the object level by segmenting car instances. Another way to integrate more information is to consider long-term temporal interactions.
- 讨论：场景流量估计与立体声和光流量共享大部分挑战，同时集成更多信息，从而获得更好的效果。理想情况下，方法应该将深度和运动线索一起用于动态3D场景。我们显示图29中KITTI场景流程基准的前5种方法的累积误差。由于匹配问题和汽车的独立运动，汽车表面是最有问题的区域。靠近图像边界的像素是另一个典型的误差源，特别是在发生大规模变化的汽车前方的路面上。虽然局部平面度和刚性假设减轻了这个问题，但由于复杂的几何物体，如植被，行人或自行车，它们经常被侵犯。对飞机的错误估计，例如延伸到多个表面的超像素会引起额外的问题，特别是在对象的边界。语义图像理解可以帮助这些问题，特别是通过分割汽车实例在对象层面。整合更多信息的另一种方法是考虑长期的时间相互作用。

- 8.3. Ego-Motion Estimation  Ego-Motion 估计
- The estimation of the ego-motion, the position and orientation of the car, is another fundamental problem to realize autonomous driving. Traditionally, this problem was addressed in wheel odometry with wheel encoders, which measure the rotation of the wheel, by integrating the measurements over time. These methods suffer from wheel slip in uneven terrain or adverse conditions and can not recover from errors in the measurements. Visual odometry or LiDAR-based odometry techniques, which estimate ego-motion from images or laser range measurements, became popular because they are less affected by these conditions and can correct estimation errors by recognizing already visited places which is called loop closure (Section8.4.1). A detailed tutorial to this topic was presented by Scaramuzza & Fraundorfer (2011) and Fraundorfer & Scaramuzza (2011).
- 自主运动的估计，汽车的位置和方向是实现自主驾驶的另一个根本问题。 传统上，这个问题在车轮测距中得到了解决，车轮编码器通过随着时间的推移积分测量轮来旋转车轮。 这些方法在不平坦的地形或不利条件下遭受车轮滑移，并且不能从测量中的误差中恢复。 估计图像或激光范围测量中的自身运动的视觉测距或基于LiDAR的测距技术变得流行，因为它们受这些条件的影响较小，并且可以通过识别已被访问的位置（称为环闭合）来校正估计误差（Section 8.4.4）。 Scaramuzza＆Fraundorfer（2011）和Fraundorfer＆Scaramuzza（2011）提供了有关此主题的详细教程。

- **Formulation**: In visual odometry the goal is to recover the full trajectory of one camera or a camera system from images. This is incrementally done by estimating the relative transformation between the camera positions at two time steps and accumulating all transformations over time to recover the full trajectory. The incremental approach is illustrated in Figure 30. The different methods can be divided into two categories: feature-based methods, that extract an intermediate representation (features) from raw measurements, and direct formulations, that directly operates on raw measurements. Feature-based methods typically work only in environments conforming the used feature type. Especially in man-made environments, important information about straight and curved edges is discarded considering keypoints. In contrast, direct methods leverage the gradient information of the whole image. Therefore, these methods usually achieve higher accuracy and robustness in environments with little keypoints. The field was dominated by featurebased methods since they typically were more efficient but direct formulations have recently grown in popularity. In featurebased and direct formulations, the extracted representation or raw measurements are usually used as input in a probabilistic model to compute unknown hidden model parameters such as the camera motion or a world model. A Maximum Likelihood approach typically finds the model parameters that maximiz the probability of obtaining the measurements.
- 配方：在视觉测距中，目标是从图像中恢复一个摄像机或摄像机系统的完整轨迹。通过估计两个时间步长的摄像机位置之间的相对变换并随着时间的推移累积所有变换以恢复完整轨迹，这是递增的。增量方法如图30所示。不同的方法可以分为两类：基于特征的方法，从原始测量中提取中间表示（特征），以及直接对原始测量进行操作的直接配方。基于特征的方法通常仅在符合所使用的特征类型的环境中有效。特别是在人造环境中，考虑到关键点，丢弃关于直线和弯曲边缘的重要信息。相比之下，直接方法利用整个图像的渐变信息。因此，这些方法通常在关键点很小的环境中实现更高的精度和鲁棒性。该领域由基于特征的方法主导，因为它们通常更有效率，但直接配方近来越来越受欢迎。在基于特征和直接配方中，提取的表示或原始测量通常用作概率模型中的输入，以计算未知的隐藏模型参数，例如相机运动或世界模型。最大似然法通常找到最大化获得测量概率的模型参数。

- **Drift**: The incremental approach greatly suffers from drift caused by the accumulation of estimation errors of the individual transformations. It is usually addressed with an iterative refinement over the last x images. This is done by reprojecting image points into 3D by triangulation and minimizing the sum of squared reprojection errors (sliding window bundle adjustment or windowed bundle adjustment). Another method to reduce drift is simultaneous localization and mapping (SLAM) (Lee et al. (2013a); Engel et al. (2015); Pire et al. (2015); MurArtal et al. (2015)) which jointly estimates the location and a map of the environment to recognize places that have been visited before. The detection of already mapped places is known as loop closure and is used to reduce the drift in the trajectory as well as the map and achieve global consistency. Some work focus on the loop closure detection in specific (Cummins & Newman (2008); Paul & Newman (2010); Lee et al. (2013b)) which will be discussed in detail in Section 8.4.1. These approaches are computationally expensive and a careful selection of the extracted features can already reduce the estimation error and drift. Kitt et al. (2010) for example use bucketing to obtain well distributed corner-like feature matches whereas Deigmoeller & Eggert (2016) use different heuristics on flow and depth estimation to reject non stable features.
- 漂移：增量方法极大地受到由各个变换的估计误差积累引起的漂移的影响。通常通过对最后x个图像进行迭代细化来处理它。这可以通过三角测量将图像点重新投影到3D中，并将平方重新投影误差（滑动窗口束调整或窗口束调整）的总和最小化。减少漂移的另一种方法是同时进行本地化和映射（SLAM）（Lee等人（2013a）; Engel等人（2015）; Pire等人（2015）; MurArtal等（2015）），其共同估计位置和环境地图，以识别以前访问过的地方。已经映射的位置的检测被称为循环闭环，用于减少轨迹中的漂移以及地图，并实现全局一致性。 （Cummins＆Newman（2008）; Paul＆Newman（2010）; Lee et al。（2013b））中的一些工作侧重于循环闭合检测，将在8.4.1节中详细讨论。这些方法在计算上是昂贵的，并且提取的特征的仔细选择已经可以减少估计误差和漂移。 Kitt等人（2010）例如使用压缩来获得分布好的角状特征匹配，而Deigmoeller＆Eggert（2016）对流动和深度估计使用不同的启发式来拒绝非稳定特征。

- **2D-to-2D Matching**: Depending on how corresponding points between two time steps are represented (2D or 3D), different methods must be used to obtain the camera transformation. In case of 2D feature matches (2D-to-2D) the essential matrix can be estimated which represents the epipolar geometry between the two cameras. The translation and rotation can directly be extracted from the essential matrix. The eight-point algorithm (Longuet-Higgins (1981)) is a simple solution working with calibrated and uncalibrated cameras whereas the five-point algorithm (Nister (2004)) is a minimal case solution which only applies to the scenario of calibrated cameras. Scaramuzza et al. (2009) estimate the essential matrix from monocular images with only one 2D feature correspondence using non-holonomic constraints of wheeled vehicles imposing a restrictive motion model. Lee et al. (2013a) extend this idea to a novel two point minimal solution that is able to obtain the metric scale using a multi-camera system. In contrast to the non-holonomic constraints, Lee et al. (2014) assume the vertical directions to be known (from an Inertial Measurement Unit) and propose a minimal four-point and linear eight-point algorithm for a multicamera system. Kitt et al. (2010) estimate the ego-motion using trifocal tensor which relates features between three images. Using these algorithms within RANSAC all 6 degrees of freedom can be robustly obtained in these special scenarios. The number of iterations necessary to guarantee that a correct solution is found with RANSAC depends on the number of points from which the model can be instantiated. Therefore, a reduced number of correspondences will reduce the number of iterations and the runtime of the approach.
- 2D到2D匹配：根据两个时间步长之间的对应点（2D或3D）的不同，必须使用不同的方法来获得相机转换。在2D特征匹配（2D到2D）的情况下，可以估计必需矩阵，其表示两个相机之间的对极几何。可以直接从基本矩阵中提取平移和旋转。八点算法（Longuet-Higgins（1981））是一种使用校准和未校准的摄像机的简单解决方案，而五点算法（Nister（2004））是仅适用于校准摄像机场景的最小情况解决方案。 Scaramuzza等人（2009）从使用非限制性运动模型的轮式车辆的非完整约束来估计单眼图像中仅有一个2D特征对应关键矩阵。 Lee et al。 （2013a）将这一想法扩展到一种新颖的两点最小解决方案，该解决方案能够使用多摄像机系统获得度量标度。与非完整约束相反，Lee et al。 （2014）假设垂直方向是已知的（来自惯性测量单元），并提出了一种用于多摄像机系统的最小四点和线性八点算法。 Kitt等人（2010）估计使用三焦点张量的自我运动，其涉及三个图像之间的特征。在这些特殊情况下，在RANSAC中使用这些算法可以强有力地获得所有6个自由度。使用RANSAC找到确保正确解决方案所需的迭代次数取决于可以实例化模型的点数。因此，减少的对应数量将减少迭代次数和方法的运行时间。

- **3D-to-2D Matching**: In the case of 3D features at the previous time step and 2D image features at the current time step (3D-to-2D) the transformation is estimated from stereo data (or triangulation when using monocular images). Geiger et al. (2011) present a real-time 3D reconstruction approach using visual odometry. They detect sparse features using blob, corner detector and estimate the ego-motion by minimizing the reprojection error. The estimation is refined with a Kalman filter while the dense 3D reconstruction is obtained by triangulating the image points. In contrast, Engel et al. (2013) continuously estimate a semi-dense inverse depth map to do real-time visual odometry with a monocular camera. The depth is estimated using multi-view stereo for pixel with non-eligible gradients and is represented by a Gaussian probability distribution. The depth estimation is propagated from frame to frame and the transformation is estimated using whole-image alignment. With this semi-dense formulation they achieve comparable performance to fully dense methods while not requiring a depth sensor. Engel et al. (2016) present a direct sparse approach for monocular visual odometry. They use a fully directed probabilistic model and jointly optimize all model parameters (camera poses, camera intrinsics, inverse depth).
- 3D到2D匹配：在当前时间步长（3D到2D）的上一个时间步长的3D特征和2D图像特征的情况下，从立体声数据估计变换（或使用单目图像时的三角测量） ）。盖革等人（2011）提出了一种使用视觉测距法的实时三维重建方法。他们使用斑点，拐角检测器检测稀疏特征，并通过最小化重投影误差来估计自我运动。利用卡尔曼滤波器对该估计进行了改进，而通过对图像点进行三角测量得到了致密的3D重建。相比之下，恩格尔等（2013）连续估计一个半密集反相深度图，用单目相机进行实时视觉测距。使用具有非合格梯度的像素的多视角立体声来估计深度并且由高斯概率分布表示。深度估计从帧到帧传播，并且使用全图像对齐来估计变换。通过这种半密度配方，它们可以实现与完全致密的方法相当的性能，而不需要深度传感器。恩格尔等人（2016）提出了一种直观的单目视觉测距法。他们使用完全定向的概率模型，并共同优化所有模型参数（摄像机姿态，摄像机内在参数，反向深度）。

- **3D-to-3D Matching**: When dealing with 3D correspondences (3D-to-3D), the transformation can be obtained by aligning the two sets of 3D features. In case of visual odometry the extracted features from images are projected into 3D using depth whereas LiDAR-based approaches such as Zhang & Singh (2014, 2015) directly obtain the 3D points from the sensor. The triangulated 3D points from stereo will exhibit a large anisotropic uncertainty due to the small baseline and the quadratic increase of errors with respect to distance. Thus it is more natural to minimize reprojection errors in the images where error statistics can be approximated more easily while laser-based approaches do not suffer from this problem and thus can be optimized more easily in 3D space.
- 3D到3D匹配：当处理3D对比（3D到3D）时，可以通过对齐两组3D特征来获得变换。 在视觉测距的情况下，图像的提取特征使用深度投影为3D，而基于LiDAR的方法（如Zhang＆Singh（2014，2015））直接从传感器获取3D点。 由于基线小，相对于距离的误差的二次增加，来自立体声的三角形3D点将呈现大的各向异性不确定性。 因此，最大限度地减少图像中的重投影误差，使误差统计量可以更容易地近似，而基于激光的方法不会受到此问题的影响，因此可以在3D空间中更容易地进行优化。

- 8.3.1. State-of-the-art 先进技术
- Only few datasets exist for visual odometry and most are too short or consists of low quality imagery. The KITTI benchmark Geiger et al. (2012b) discussed in Section 2 provides a large dataset of challenging sequences and evaluation metrics. We provide the KITTI leaderboard of monocular approaches in Table 9, stereo approaches in Table 10 and LiDAR-based approaches is provided in Table 11. The performance is measured with the average translational and rotational error for all possible subsequences of length (100; : : : ; 800) meters.
- 只有少数数据集存在用于视觉测距，大多数数据太短或由低质量图像组成。 KITTI基准Geiger等 （2012b）提供了一个具有挑战性的序列和评估指标的大数据集。 我们在表9中提供了KITTI单眼方法的排行榜，表10中的立体声方法和基于LiDAR的方法在表11中提供。性能用所有可能的长度（100〜800）子序列的平均平移和旋转误差进行测量。

- **Monocular Visual Odometry**: Monocular visual odometry methods can recover the motion only up to a scale factor. The absolute scale can then be determined by computing the size of objects in the scene, from motion constraints, or integration with other sensors. The eight-point method proposed by Longuet-Higgins (1981) performs poorly in the presence of noise, in particular with uncalibrated cameras. Mirabdollah&Mertsching (2014) investigated the second order statistics of the essential matrix to reduce the estimation error with the eight-point method. They use the Taylor expansion up to the second order terms to obtain a covariance matrix that acts as regularization term along with the coplanarity equations. The drifting problem is particularly difficult in monocular visual odometry because of the lack of depth information. With a ground plane estimation in a real-time monocular SfM system Song & Chandraker (2014) deal with scale-drift and improve upon Mirabdollah & Mertsching (2014) results in Table 9. They combine multiple cues with learned models to adaptively weight per-frame observation covariances for ground plane estimation. Mirabdollah & Mertsching (2015) present a real-time and robust monocular visual odometry approach using the iterative five-point method. They obtain the location of landmarks with uncertainties using a probabilistic triangulation method and estimate the scale of the motion with low quality features on the ground plane. With this approach they outperform all monocular visual odometry methods in Table 9. Since the KITTI dataset require metric output the scale estimate has a strong impact on the performance of the approaches.
- 单目视觉测距：单目视觉测距方法可以恢复运动，只能达到比例因子。然后可以通过计算场景中的对象的大小，运动约束或与其他传感器的集成来确定绝对刻度。 Longuet-Higgins（1981）提出的八点法在噪声的存在下表现不佳，特别是未校准的摄像机。 Mirabdollah＆Mertsching（2014）调查了基本矩阵的二阶统计量，以八点法减少估计误差。他们使用泰勒扩展直到二阶项来获得协方差矩阵，其作为正则化项以及共平面方程。由于缺乏深度信息，单目视觉测距中的漂移问题尤其困难。在实时单目SfM系统中，通过对地面飞机的估计，Song＆Chandraker（2014）对表9中的Mirabdollah＆Mertsching（2014）的结果进行了大规模的漂移和改进。他们将多个线索与学习模型相结合，框架观测协方差估计。 Mirabdollah＆Mertsching（2015）提出了一种使用迭代五点法的实时稳健的单目视觉测距法。他们使用概率三角测量法获得具有不确定性的地标位置，并估计地面上具有低质量特征的运动规模。使用这种方法，它们优于表9中的所有单目视觉测距方法。由于KITTI数据集需要度量输出，所以尺度估计对方法的性能有很大的影响。

- **Stereo Visual Odometry**: Stereo visual odometry methods do not have the problem of estimating the scale because it is directly known from the baseline between the cameras. In addition, they allow to deal with the drifting problem with a joint formulation of ego-motion estimation and mapping. Therefore, stereo methods are typically outperforming monocular methods on the KITTI dataset (see Table 9 and Table 10). Engel et al (2015) propose a real-time large-scale direct SLAM algorithm that couples temporal multi-view stereo with static stereo from a camera setup (Figure 31). This allows them to estimate depth of pixels that are under-constrained in static stereo while avoiding scale-drift that occurs using multi-view stereo. The images are directly aligned based on photoconsistency of high contrast pixel. Pire et al. (2015) divide the problem into camera tracking and map optimization that can be run in parallel. While sharing the same map the tracking task matches features, creates new points and estimates the camera pose whereas the map optimization refines the map with bundle adjustment. This formulation allows them to achieve the same performance while being faster. Deigmoeller & Eggert (2016) follow a very different way by relying exclusively on pure measurements as mentioned before. With the estimation of scene flow on Harris corners and rejection of features with different heuristics they outperform the two SLAM approaches in terms of translational error but have the highest rotational error and runtime in Table 10.
- 立体视觉眼镜：立体视觉测距方法没有估计尺度的问题，因为它是从相机之间的基线直接知道的。此外，它们允许通过自我运动估计和映射的联合计算来处理漂移问题。因此，立体声方法通常在KITTI数据集上优于单目标方法（见表9和表10）。 Engel等人（2015）提出了一种实时大规模直接SLAM算法，将时间多视点立体声与静态立体声从相机设置相结合（图31）。这允许他们估计在静态立体声中受限制的像素的深度，同时避免使用多视角立体声发生的尺度漂移。基于高对比度像素的相依性，图像直接对齐。皮尔等人（2015）将问题分解为并行运行的摄像机跟踪和地图优化。在共享相同的地图时，跟踪任务与特征相匹配，创建新点并估计摄像机姿态，而地图优化通过束调整来优化地图。这种配方允许他们在更快的时候达到相同的性能。 Deigmoeller＆Eggert（2016）遵循一种完全不同的方式，仅依赖于前面提到的纯粹的测量。通过对哈里斯角的场景流的估计和具有不同启发式的特征的拒绝，它们在平移误差方面优于两种SLAM方法，但是在表10中具有最高的旋转误差和运行时间。

- Persson et al. (2015) propose a stereo visual odometry system for automotive applications based on techniques from monocular visual odometry. In particular, they use motion model predicted tracking by matching, similar to Song et al. (2013), and delayed outlier identification. They argue that stereo techniques outperform monocular techniques because the problem formulation is easier. Monocular techniques should be more refined and robust because they need to deal with intrinsically more difficult problem. This allows them to outperform the others in the translational error in Table 10. The two best performing methods decouple the estimation of the rotation and translation as there is a fundamental difference between their estimation. The translation is dependent on the depth in contrast to the rotation. Buczko & Willert (2016a) claim that errors from depth estimation affect the rotation estimation in a coupled formulation and can be avoided by decoupling them. Thus, they use an initial rotation estimation to decouple the rotational and translational optical flow. The resulting characteristics are then used to exclude outliers. Cvisic & Petrovic (2015) compute the motion with a separate estimation of the rotation using the five point and the translation using the three point method. They also present a modified IMU-aided version of the algorithm suitable for embedded systems.
- Persson等人（2015）提出了一种基于单目视觉测距技术的汽车应用立体视觉测距系统。特别地，他们使用运动模型预测跟踪通过匹配，类似于宋等人。 （2013年），并延迟异常情况鉴定。他们认为立体声技术优于单眼技术，因为问题的制定更容易。单眼技术应该更加精细和强大，因为它们需要处理本质上更加困难的问题。这使得它们在表10中的平移误差中优于其他。两种最佳表现方法将旋转和平移的估计解耦，因为它们的估计存在根本差异。翻译取决于与旋转相反的深度。 Buczko＆Willert（2016a）声称，深度估计误差影响耦合公式中的旋转估计，可以通过解耦来避免。因此，它们使用初始旋转估计来解耦旋转和平移光学流。所得到的特征然后用于排除异常值。 Cvisic＆Petrovic（2015）通过使用五点和使用三点法的平移来单独估计旋转来计算运动。它们还提出了适用于嵌入式系统的修改的IMU辅助版本的算法。

- Kreso & Segvic (2015) observed that the camera calibration is critical for visual odometry and that the remaining calibration errors in pre-calibrated systems like KITTI have adversarial effects on the estimation results. They therefore propose to correct the calibration of the camera by exploiting the ground truth motion. The deformation field is recovered by optimizing the reprojection error of point feature correspondences in neighboring stereo frames under the groundtruth motion. Using the deformation field they obtained state-of-the-art results at the time.
- Kreso＆Segvic（2015）观察到，相机校准对于视觉测距是至关重要的，并且预校准系统（如KITTI）中的剩余校准误差对估计结果具有对抗性影响。 因此，他们建议通过利用地面真相运动来校正摄像机的校准。 通过在地面真相运动下优化相邻立体声帧中的点特征对应的重新投影误差来恢复变形场。 使用变形场，他们获得了当时最先进的结果。

- **LiDAR-based Odometry**: The best performing methods on KITTI are using point clouds for ego-motion estimation (Table 11). Zhang & Singh (2014) split the SLAM problem into LiDAR-based odometry at high frequency with low fidelity and LiDAR-mapping at low frequency illustrated in Figure 32. The LiDAR-based odometry matches two consecutive LiDAR scans whereas the LiDAR-mapping matches and registers the new scan to a map. This results in low drift and computational complexity without the need for high accuracy range or inertial measurements. Zhang & Singh (2015) extend this work by combining isual odometry at high frequency with LiDAR-mapping at low frequency which allows them to further improve.
- 基于LiDAR的测距：KITTI上表现最好的方法是使用点云进行自我运动估计（表11）。 Zhang＆Singh（2014）将SLAM问题分解为高频率，低保真度和低频下的LiDAR映射，如图32所示。基于LiDAR的测距仪与两个连续的LiDAR扫描匹配，而LiDAR映射匹配和 将新扫描注册到地图。 这导致低漂移和计算复杂度，而不需要高精度范围或惯性测量。 Zhang＆Singh（2015）通过将高频视觉测距与低频LiDAR映射相结合来扩展这项工作，使他们进一步完善。

- **Discussion**: Though several approaches have addressed the scale-drift problem in monocular visual odometry they cannot compete with approaches using 3D information on the KITTI dataset yet. While LiDAR provides the richest source of information for ego motion estimation, stereo-based methods show competitive results. In Figure 33 we visualize the average translational and rotational errors of the best performing visual odometry methods on the KITTI benchmark. The second row shows the translational error, the third row shows the rotational error while the last row shows the speed. The highest translational and rotational error can usually be obverse in strong turns. Furthermore, the error is correlated with speed and the amount of independently moving objects in the scene which lower the number of features in the background. While large errors can be observed for crowded highway scenes (second from right), only moderate errors occur when the highway is empty (right and second from left). Larger errors can also be observed in very narrow environments (fourth from right) where feature displacements are large. Overall, the most accurate motion estimation is achieved using 3D information, so far. However, stereo cameras are very cheap sensors in comparison to LiDAR laser scanners and stereo-based methods achieve competitive results.
- 讨论：虽然几种方法已经解决了单目视觉测距中的尺度漂移问题，但是它们不能与使用三维信息的方法在KITTI数据集上竞争。虽然LiDAR为自我运动估计提供了最丰富的信息来源，但基于立体的方法显示出有竞争力的结果。在图33中，我们可以看出在KITTI基准测试中表现最佳的视觉测距方法的平均平移和旋转误差。第二行显示平移误差，第三行显示旋转错误，而最后一行显示速度。最高的平移和旋转误差通常在强力转弯时是正面的。此外，该误差与场景中的速度和独立移动物体的数量相关，这降低了背景中的特征数量。虽然拥挤的高速公路场景（右二）可以观察到较大的误差，但高速公路空闲时（左右两侧）只发生中度误差。在特征位移较大的非常狭窄的环境（右四）也可以观察到较大的误差。总的来说，迄今为止，使用3D信息实现了最准确的运动估计。然而，与LiDAR激光扫描仪和基于立体声的方法相比，立体相机是非常便宜的传感器，可以获得竞争优势。

- 8.4. Simultaneous Localization and Mapping (SLAM) 同步定位与构图 (SLAM)
- A detailed map of the environment is a commonly exploited prerequisite for path planning and navigation of an autonomous car. However, in places where a map is not provided or incomplete, the autonomous car needs to locate itself while generating the map. Further, the map needs to be updated continuously to reflect environmental changes over time. In this context, SLAM refers to the task of simultaneous estimation of the location of an agent while continuously building up a map of the environment. One particular challenge in autonomous driving, is that these systems need to handle large-scale environments in realtime.
- 详细的环境地图是自主车路径规划和导航的常见的先决条件。 然而，在没有提供或不完整的地图的地方，自动汽车需要在生成地图的同时进行定位。 此外，地图需要不断更新以反映随着时间的变化。 在这种情况下，SLAM是指同时估计代理的位置，同时不断地构建环境地图的任务。 自主驾驶中的一个特殊挑战是，这些系统需要实时处理大规模环境。

- **Formulation**: Traditionally, the map is represented by a set of landmarks, as for example image features. Early approaches to SLAM have addressed the problem using Bayesian formulations using extended Kalman filters (Smith et al. (1987)) or particle filters (Montemerlo et al. (2002)). Given the last state and current observations, the current state, represented by pose, velocity and the locations of the landmarks is recursively updated. However, this formulation is not applicable to large environments since the belief state and time complexity of the filter update grow quadratically in the number of landmarks in the map. The belief state represents all correlations between all pairs of variables which is O(n2) and whenever a landmark is observed the correlation to all other variables need to be updated with the same complexity. One solution for reducing complexity is a filtering technique based on a Graphical Model that maintains a tractable approximation of the belief state using a thin junction tree as proposed by Paskin (2003). However, it is known that filtering always produces an inconsistent map when applied on nonlinear SLAM problems (Julier & Uhlmann (2001)), which is usually the case when dealing with real data. In contrast, full SLAM approaches, such as graph-based or least-squares formulations, can provide exact solutions considering all poses at once. Kaess et al. (2008) propose an incremental smoothing and mapping approach based on fast incremental matrix factorization. They extend their work Dellaert & Kaess (2006) on factorizing the matrix of the nonlinear least-squares problem to an incremental approach that only recalculates entries which change in the matrix. Kaess et al. (2012) have introduced the Bayes tree, a novel data structure, to allow a better understanding of the connection between graphical model inference and sparse matrix factorization in SLAM. Factored probability densities are encoded in the Bayes tree which naturally maps to a sparse matrix.
- 制定：传统上，地图由一组地标表示，例如图像特征。早期的SLAM方法使用扩展卡尔曼滤波器（Smith et al。（1987））或粒子滤波器（Montemerlo et al。（2002））使用贝叶斯方程来解决问题。给定最后一个状态和当前的观测值，以姿态，速度和地标位置为代表的当前状态递归更新。然而，这个公式不适用于大型环境，因为过滤器更新的信念状态和时间复杂度在地图上的地标数量上二次增长。置信状态表示O（n2）的所有变量对之间的所有相关性，并且每当观察到地标时，与所有其他变量的相关性需要以相同的复杂度进行更新。降低复杂性的一个解决方案是基于图形模型的过滤技术，该方法使用Paskin（2003）提出的薄连接树来维护置信状态的易处理近似。然而，已知当应用于非线性SLAM问题时，滤波总是产生不一致的映射（Julier＆Uhlmann（2001）），这通常是在处理真实数据时的情况。相比之下，完整的SLAM方法，如基于图形或最小二乘法的公式，可以提供考虑到所有姿势的精确解决方案。 Kaess等人（2008）提出了一种基于快速增量矩阵分解的增量平滑和映射方法。他们扩展了他们的工作Dellaert＆Kaess（2006），将非线性最小二乘问题的矩阵分解为仅重新计算矩阵变化的条目的增量方法。 Kaess等人（2012）引入了贝叶斯树，一种新颖的数据结构，以便更好地了解SLAM中图形模型推理和稀疏矩阵分解的连接。因子密度在Bayes树中编码，它自然地映射到稀疏矩阵。

- **Environmental Changes**: A major challenge in SLAM are changes in the environment that might not be represented bya map. To alleviate this problem, Levinson et al. (2007) create a map only consisting of features that are very likely to be static. Using 3D LiDAR they retain only flat surfaces and obtain an infrared reflectivity map of overhead views of the road surface. The map is then used to locate a vehicle with a particle filter in real-time. Levinson & Thrun (2010) extend this work considering maps as probability distributions over environment properties instead of a fixed representation. Specifically, every cell of the probabilistic map is represented as its own Gaussian distribution over remittance values. This allows them to represent the world more accurately and localize with fewer errors. In addition, they can use offline SLAM to align multiple passes of the same environment at different time to build an increasingly robust understanding of the world.
- 环境变化：SLAM中的一个主要挑战是在可能无法表现的环境中发生变化一张地图。 为了缓解这个问题，Levinson等 （2007）创建一个仅由很可能是静态的功能组成的地图。 使用3D LiDAR，它们只保留平面，并获得路面俯视图的红外反射率图。 然后，该地图用于实时地定位具有粒子滤波器的车辆。 Levinson＆Thrun（2010）扩展了这项工作，将地图视为环境属性的概率分布而不是固定表示。 具体来说，概率图的每个单元都被表示为其自身的高斯分布在汇款值上。 这允许他们更准确地代表世界，并以更少的错误进行本地化。 此外，他们可以使用离线SLAM来在不同时间对齐相同环境的多个通道，以建立对世界的越来越强的理解。

- 8.4.1. Loop Closure Detection 环路闭合检测
- The relocalization in already mapped areas is an important subproblem of SLAM, referred to as loop closure detection. Relocalization is used to correct drifts in the trajectory and inaccuracies in the map caused by drift. Cummins & Newman (2008) present a probabilistic approach for the recognition of places based on their appearance. They learn a generative model of place appearances using bag-of-words because distinctive combinations of visual words will often arise from common objects. The generative model is robust and works even in visually repetitive environments. The performance of the approach is demonstrated on a self recorded dataset and visualized in Figure 34. Paul & Newman (2010) extend this idea by incorporating distance between words coupled to the observation of pairs of visual words with a random graph. The random graph models the pairwise distance between words besides their distribution of occurrences. In contrast, Lee et al. (2013b) show that the relative pose with metric scale between two loopclosing pose-graph vertices can directly be obtained from the epipolar geometry of a multi-camera system with overlapping views. They simplify the problem using a planar constraint on the motion of a car and estimate the loop-constraint using least squares optimization.
- 已映射区域的重新定位是SLAM的一个重要的子问题，称为环路闭包检测。重新定位用于校正由漂移引起的轨迹和地图误差的漂移。康明斯＆纽曼（2008）提出了一种基于外观识别地位的概率方法。他们通过使用单词来学习一个生成模式的地方出现，因为常见的对象常常会产生视觉词汇的特殊组合。生成模型是健壮的，即使在视觉上重复的环境中也是如此。该方法的性能在自我记录的数据集上得到证明，并在图34中可视化。Paul＆Newman（2010）通过将与观察对话视觉词之间的距离与随机图相结合来扩展这一想法。随机图模型除了发生分布之外的词之间的成对距离。相比之下，Lee et al。 （2013b）显示，可以直接从具有重叠视图的多摄像机系统的对极几何获得两个闭环姿态图顶点之间的度量标度的相对姿态。他们使用对汽车运动的平面约束来简化问题，并使用最小二乘法优化来估计循环约束。

- **LiDAR-based**: Image-based loop closure detection can become unreliable in case of strong illumination changes or strong viewpoints changes. In contrast, LiDAR-based localization is not affected by changes in illumination and does not suffer as much from changes in viewpoint due to the captured 3D geometry. Dube et al. (2016) propose a loop closure detection algorithm based on matching 3D segments. Segments from the point cloud are extracted and described using a combination of descriptors. Matching of segments is performed by obtaining candidates with kd-tree search in feature space and estimating the matching score of the candidates with a random forest.
- 基于LiDAR：基于图像的闭环检测在强烈的照明变化或强烈的观点变化的情况下可能变得不可靠。 相比之下，基于LiDAR的定位不受照明变化的影响，并且由于所捕获的3D几何形状而不会受到视点变化的影响。 Dube等人 （2016）提出了一种基于匹配3D片段的闭环检测算法。 使用描述符的组合来提取和描述来自点云的分段。 通过在特征空间中获得具有kd-tree搜索的候选者并且利用随机森林来估计候选者的匹配分数来执行段的匹配。

- 8.4.2. Visual SLAM  视觉SLAM
- Lategahn et al. (2011) propose a dense stereo visual SLAM method that estimates a dense 3D map. Using a sparse visual SLAM system, they obtain the pose and a sparse map. For the dense 3D map, they compute a dense representation from stereo in a local coordinate system and continuously update the map by tracking the local coordinate systems with the sparse SLAM system. Engel et al. (2014) extend their semi-dense method for visual odometry (Engel et al. (2013)) by performing image alignment and loop closure detection using a formulation based on optimizing the similarity transformation. Semi-dense depth is estimated using multi-view stereo from small baselines to create and refine a semi-dense map using pose graph optimization. The fusion of visual and inertial cues proposed by Leutenegger et al. (2013) takes advantage of their complementary nature. Instead of filtering, they use a non-linear optimization approach and integrate the IMU error with the reprojection error of landmarks into a joint cost function. Mur-Artal et al. (2015) use the ORB features proposed by Rublee et al. (2011) for tracking, mapping, relocalization and loop closure. They combine methods from loop detection (Galvez-Lopez & Tardos (2012)), loop closing (Strasdat et al. (2010, 2011)) and pose graph optimization (Kummerle et al. (2011)) into one system.
- Lategahn等（2011）提出了一种密集的立体视觉SLAM方法来估计密集的3D地图。使用稀疏的视觉SLAM系统，它们获得姿态和稀疏映射。对于密集的3D地图，它们从局部坐标系中的立体声计算密集表示，并通过使用稀疏SLAM系统跟踪局部坐标系来连续更新地图。恩格尔等人（2014）通过使用基于优化相似变换的公式执行图像对准和闭环检测来扩展其用于视觉测距的半密集方法（Engel等人（2013））。使用来自小基线的多视角立体声估计半密度深度，以使用姿势图优化来创建和细化半密集地图。 Leutenegger等人提出的视觉和惯性线索的融合（2013）利用其互补性。他们不是过滤，而是使用非线性优化方法，将IMU错误与地标的重新投射错误整合到联合成本函数中。 Mur-Artal等（2015）使用Rublee等人提出的ORB特征（2011年），用于跟踪，绘制，重新定位和循环关闭。他们将来自循环检测的方法（Galvez-Lopez＆Tardos（2012）），循环关闭（Strasdat等（2010,2011））和姿态图优化（Kummerle等（2011））组合成一个系统。

- 8.4.3. Mapping  绘图
- For autonomous driving applications, metric and semantic maps at different level of details are required to solve different tasks. Metric maps allow accurate localization whereas semantic maps can provide problem specific information such as parking areas for automated parking. Those maps can also be generated offline with computationally expensive methods and later incorporated into an autonomous driving system.
- 对于自主驾驶应用，需要不同级别细节的度量和语义地图来解决不同的任务。 公制地图允许精确定位，而语义地图可以提供特定于问题的信息，例如用于自动停车的停车区域。 这些地图也可以用计算上昂贵的方法离线生成，然后再并入自主驾驶系统。
- **Metric Maps**: The Google Street View project (Anguelov et al. (2010)) is a prominent example for a large collection of panoramic imagery in cities around the world. For collecting the dataset, they estimate the pose in a Kalman-filter-based approach fusing data from GPS, wheel encoder and inertial navigation. Estimation at 100 Hz allows to accurately match image pixels from 15 small cameras to 3D rays from a laserscanner. The pose estimates are refined with a probabilistic graphical model of the network that represents all known roads and intersections in the world. From the image and laserscan data, they reconstruct the scene and obtain photorealistic 3D models by robustly fitting coarse meshes. Frahm et al. (2010) propose a dense 3D reconstruction approach from Internet-scale photo collections. Geometric relationships between the images are estimated using a combination of 2D appearance, color and 3D multi-view geometry constraints. They obtain the dense geometry of the scene via fast plane sweeping stereo and a depth map fusion approach. Exploiting the appearance and geometry constraints, they present a highly parallel approach which allows to process 3 million images within a day on a single computer. Figure 35 shows two example models reconstructed from Flickr images of Rome and Berlin. For autonomous driving applications, it is often suffcient to map the road surface in 2D (i.e., in bird’s eye view) which allows for localization with respect to features on the road such as road markings or imperfections in the road surface. Geiger (2009) present an approach for road mosaicing in dynamic environments to create obstacle-free bird eye views. The road surface is extracted using optical flow on Harris corners and approximated by a plane. This allows to describe the mapping between the images with homographies. The road images are finally combined using multi-band blending.
- 公制地图：Google街景视图项目（Anguelov et al（2010））是世界各地城市大型全景图像的一个突出例子。为了收集数据集，他们用基于卡尔曼滤波器的方法估计姿态，融合来自GPS，轮编码器和惯性导航的数据。在100Hz的估计允许从15个小型相机到激光扫描仪的3D光线的图像像素精确匹配。使用代表世界上所有已知道路和交叉点的网络的概率图形模型来改进姿态估计。从图像和激光扫描数据，他们通过鲁棒地拟合粗网格重建现场并获得逼真的3D模型。 Frahm et al。 （2010）提出了一个从互联网规模的照片收藏的密集3D重建方法。使用2D外观，颜色和3D多视图几何约束的组合来估计图像之间的几何关系。他们通过快速平面扫描立体声和深度图融合方法获得场景的密集几何。利用外观和几何约束，它们呈现出高度并行的方法，允许在单个计算机上在一天内处理300万张图像。图35显示了从罗马和柏林的Flickr图像重建的两个示例模型。对于自主驾驶应用，通常足以在2D（即鸟瞰图）中映射路面，其允许相对于道路上的特征（例如道路标记或道路表面的缺陷）进行定位。 Geiger（2009）在动态环境中提出了道路拼接方法，以创造无障碍鸟瞰图。路面使用Harris角上的光流提取，并用平面近似。这允许描述具有同形异义的图像之间的映射。道路图像最终使用多频带混合组合。

- **Semantic Maps**: All methods discussed so far focus on creating metric maps ignoring semantic information. However, for tasks like automated parking, a semantic map that is updated jointly with the metric map is necessary. Grimmett et al. (2015) fuse semantic and metric maps for vision-only automated parking. They update the map with static and dynamic labels and use active learning for lane, parking space and pedestrian crossings detection.
- 语义地图：迄今为止所讨论的所有方法都集中在创建忽略语义信息的度量标图。 然而，对于诸如自动停车的任务，需要与度量图一起更新的语义地图。 Grimmett等人 （2015）视觉专用自动停车的保险丝语义和度量地图。 他们用静态和动态标签更新地图，并使用主动学习车道，停车位和行人过路检测。

- 8.5. Localization 本地化
- Localization is a well-studied problem in both robotics and vision, covering a broad range of techniques from indoor localization of a robot using noisy sensory measurements to locating where a picture was taken in the entire world. From an autonomous driving perspective, the main task is to precisely localize the ego-vehicle on a map. Localization is also an important subroutine of SLAM approaches, where it is used for detecting loop-closures and correcting drift when mapping the environment, see Section 8.4.1.
- 本地化是机器人和视觉两个方面的一个很好的研究问题，涵盖了使用嘈杂的感官测量的机器人的室内定位到定位在整个世界拍摄照片的广泛的技术。 从自主驾驶的角度来看，主要任务是精确地将自己的车辆定位在地图上。 本地化也是SLAM方法的重要子程序，用于在映射环境时检测环路闭合和校正漂移，参见第8.4.1节。

- Localization can be performed using either sensors like a GPS system or visual information based on images. Using GPS alone typically provides an accuracy around 5 m. Although centimeter-level precision is possible in open spaces using combinations of sensors as in KITTI car (Geiger et al. (2012b)), it is often rendered infeasible in traffic scenes with several disturbing effects such as occlusions by vegetation and buildings or multi-path effects due to reflections. Therefore, image-based localization independent of satellite systems is highly relevant. Early image-based techniques (Li et al. (2009); Zheng et al. (2009)) approach the problem as classification into one of a predefined set of places which are referred as “landmarks”. Others (e.g. Hays & Efros (2008)) create a database of images with known locations and formulate the localization as an image retrieval problem. These methods require a similarity measure to compare images based on local or global appearance cues. The larger the database, the more diffcult the localization task becomes. Challenges include appearance changes, similar looking places, and the changes due to viewpoint or position.
- 可以使用诸如GPS系统的传感器或基于图像的视觉信息来执行本地化。单独使用GPS通常提供大约5米的精度。虽然在KITTI车（Geiger等人（2012b））中使用传感器组合的开放空​​间中厘米级精度是可能的，但是在具有若干扰动效果的交通场景中，通常会使其变得不可行，例如植被和建筑物或多由于反射导致的路径效应。因此，独立于卫星系统的图像定位是非常重要的。早期的基于图像的技术（Li et al。（2009）; Zheng et al。（2009））将问题作为一种被称为“地标”的预定义的一组地点的分类。其他（例如Hays＆Efros（2008））创建具有已知位置的图像数据库，并将其定位为图像检索问题。这些方法需要相似性度量来比较基于局部或全局外观线索的图像。数据库越大，本地化任务就越难。挑战包括外观变化，看似类似的地方，以及由观点或位置引起的变化。

- **Survey**: Lowry et al. (2016) provide a comprehensive review of the current state of place recognition research. They first define what qualifies as a place in the context of robotic navigation by referencing to studies in psychology and neuroscience. Then, they review ways of describing a place using local or global descriptors and/or metric range information. They also provide a taxonomy based on the level of physical abstraction in the map and whether or not metric information is included in the place description. They further discuss how place recognition solutions can implicitly or explicitly account for appearance change within the environment and finally provide some future directions with respect to advances in deep learning, semantic scene understanding, and video description.
- 调查：Lowry et al（2016）全面审查了目前状态识别研究的情况。 他们首先通过参考心理学和神经科学研究来定义在机器人导航的上下文中是什么。 然后，他们审查使用本地或全局描述符和/或度量范围信息描述场所的方法。 它们还提供基于地图中物理抽象级别的分类法，以及度量信息是否包含在地点描述中。 他们进一步讨论了地方识别解决方案如何隐含或明确地解释环境中的外观变化，并最终提供了深度学习，语义场景理解和视频描述方面的未来发展方向。

- **Monte Carlo Methods**: The problem of map localization has been traditionally approached using Monte Carlo methods which recover the probability distribution over the agents pose by drawing a set of samples. Dellaert et al. (1999) define indoor localization in two steps, global position estimation and local position tracking over time. Instead of modeling the probability density function itself, they represent uncertainty by maintaining a set of samples and update the representation over time using Monte Carlo methods. This allows them to model arbitrary multimodal distributions in a memory efficient way. Outdoor localization is in general more challenging compared to the indoor localization task due to its scale and often unreliable sensor information such as GPS failures. Oh et al. (2004) use semantic information available in maps to compensate for the failure cases of GPS sensors. By exploiting knowledge about the environment, they assign probabilities to the target zones on the map, such as zero probability to the buildings. They incorporate these map-based priors in the particle filter formulation to bias the motion model toward areas of higher probability.
- 蒙特卡罗方法：传统上使用蒙特卡罗方法处理地图定位的问题，通过绘制一组样本来恢复代理姿势的概率分布。 Dellaert等（1999）定义了两个步骤的室内定位，全局位置估计和局部位置跟踪随着时间的推移。不是对概率密度函数本身进行建模，而是通过维护一组样本来表示不确定性，并使用蒙特卡罗方法随时间更新表示。这允许他们以有效的方式对任意多模态分布进行建模。室内定位通常比室内定位任务更具挑战性，因为它的尺度和常常不可靠的传感器信息，如GPS故障。哦等（2004）使用地图中可用的语义信息来补偿GPS传感器的故障情况。通过利用有关环境的知识，他们将概率分配给地图上的目标区域，例如建筑物的零概率。他们将这些基于地图的先验结合在粒子滤波器公式中，以将运动模型偏向较高概率的区域。

- **Metric, Topological, Topometric**: Visual localization techniques are commonly classified into metric and topological methods. Metric localization is achieved by computing the 3D pose with respect to a map. Topological localization approaches provide a coarse estimate from a finite set of possible locations which are represented as nodes in a graph which are connected by edges that link them according to some distance or appearance criteria. Metric localization can be very accurate, but is usually not suitable for long sequences, while topological localization may be more reliable, but only provides rough estimates. Badino et al. (2012) propose a topometric approach as a combination of topological and metric localization to provide geometrically accurate localization using graph-based methods. In contrast to topological methods, the graph is more fine-grained and each node corresponds to a metric location without a semantic meaning. During mapping phase, the graph is constructed using the vehicle position from GPS at fixed distance intervals and associating visual or 3D features to the corresponding graph node. At runtime, real-time localization is performed using a Bayes filter to estimate the probability distribution of the vehicle position along the route by matching features extracted from the sensor data to the map’s feature database. Brubaker et al. (2016) also leverage a graph-based representation. In contrast to traditional localization approaches, however, they do not require a visual feature database of the environment, but instead directly build this graph from road networks extracted from OpenStreetMap. They further propose a probabilistic model which allows to infer a distribution over the vehicle location using visual odometry measurements. For tractability in very large environments, they leverage several analytic approximations for efficient inference yielding higher stability compared to particle-based filtering techniques which suffer from particle depletion when ambiguities persist over long periods.
- 公制，拓扑，拓扑学：视觉定位技术通常分为度量和拓扑方法。通过计算相对于地图的3D姿态来实现度量定位。拓扑定位方法从可能位置的有限集合提供粗略的估计，这些可能位置被表示为图中的节点，其通过根据一些距离或外观标准将它们连接的边缘连接。度量定位可以非常准确，但通常不适用于长序列，而拓扑定位可能更可靠，但仅提供粗略估计。 Badino等人（2012）提出了拓扑和度量定位的组合的拓扑方法，以使用基于图的方法提供几何精确的定位。与拓扑方法相反，图形更细粒度，每个节点对应于没有语义意义的度量位置。在映射阶段，使用GPS定位距离间隔的车辆位置构建图形，并将视觉或3D特征与相应的图形节点相关联。在运行时，使用贝叶斯滤波器执行实时定位，以通过从传感器数据提取的特征匹配到地图的特征数据库来估计沿着路线的车辆位置的概率分布。 Brubaker等人（2016）也利用了基于图形的表示。然而，与传统的本地化方法相比，它们不需要环境的视觉特征数据库，而是从OpenStreetMap提取的道路网络直接构建该图。他们进一步提出一个概率模型，其允许使用视觉测距测量来推断车辆位置上的分布。对于非常大的环境中的易处理性，与基于粒子的过滤技术相比，它们利用多个分析近似来进行有效推断，从而在长时间持续存在模糊性时，会遇到粒子耗尽。

- **Scale and Accuracy**: For the problem of localization, the scale of the target area is a distinctive property to compare different approaches and is related to the accuracy achieved. Both scale and accuracy depend on the methodology used, such as map-based approaches (Brubaker et al.(2016)) which might suffer from the errors on the map and descriptor-based approaches (Badino et al. (2012); Schreiber et al. (2013)) using global or local descriptors. While the descriptor based method of Badino et al. (2012) achieves an average localization accuracy of 1 m over an 8 km route, the road network based localization approach of Brubaker et al. (2016) attains an accuracy of 4 m on a 18 km2 map containing 2,150 km of drivable roads. Schreiber et al. (2013) point out that the required precision for autonomous driving and future driver assistance systems is in the range of a few centimeters and present a feature-based localization algorithm which can achieve this on approximately 50 km of rural roads. They approach the problem from the perspective of lane recognition. In a separate drive, they create a highly accurate map that contains road markings and curbs. While driving, they detect and match them to the map in order to determine the position of the vehicle relative to the markings.
- 规模和精度：对于本地化的问题，目标区域的规模是比较不同方法的独特属性，与实现的准确性有关。尺度和准确性取决于所使用的方法，例如基于地图的方法（Brubaker等人（2016）），其可能遭受地图上的错误和基于描述符的方法（Badino等（2012）; Schreiber et （2013）），使用全局或局部描述符。虽然Badino等人的基于描述符的方法（2012）在8公里路线上实现了1米的平均定位精度，Brubaker等人的基于道路网络的本地化方法。 （2016年）在18公里的地图上达到4米的精度，包含2,150公里的可驱动道路。 Schreiber et al。 （2013）指出，自主驾驶和未来驾驶员辅助系统所需的精度在几厘米的范围内，并提出了一种基于特征的定位算法，可以在约50公里的农村道路上实现这一点。他们从车道识别的角度来解决问题。在一个单独的驱动器，他们创建一个高度精确的地图，包含路标和路缘。驾驶时，他们检测并匹配他们到地图，以确定车辆相对于标记的位置。

- **Structure-based Localization**: While the output of traditional localization approaches is either a rough camera position or a distribution over positions, a more recent line of work which is known as “structure-based localization” aims to estimate all camera matrix parameters, including position, orientation, and camera intrinsics. Localization is realized as a 2D-to-3D matching problem where the 2D points on the images are matched to a large, geo-registered 3D point cloud and the pose is estimated with respect to correspondences as shown in Figure 36.
- 基于结构的本地化：虽然传统的本地化方法的输出是粗略的相机位置或位置分布，但是称为“基于结构的定位”的更新的工作线旨在估计所有相机矩阵参数，包括位置 ，方向和相机内在性。 本地化实现为2D到3D匹配问题，其中图像上的2D点与大的地理注册的3D点云匹配，并且相对于对应度估计姿势，如图36所示。

- In structure-based approaches, the pose estimate provides a powerful geometric constraint for validating the location estimate. However, a straightforward solution, for example direct matching by approximate nearest neighbor search using SIFT features, would result in many incorrect matches. With growing model size, the discriminative power of the descriptors decreases and matching becomes more ambiguous. Consequently, RANSAC techniques have diffculty finding the correct pose. To address this issue, Li et al. (2012) find statistical cooccurrences of 3D model points in images, and then use them as a sampling prior for RANSAC to exploit co-visibility relations. In addition, they employ a bidirectional matching scheme, forward from features in the image to points in the database and inverse from points to image features. They show that the bidirectional approach performs better than forward or inverse matching alone. Besides ambiguities, the amount of memory required for storing the large number of descriptors contained in the model is another problem related to large scale. Model compression by reducing the number of points produces fewer matches and increases the number of images which cannot be localized. Instead, more recent methods (Sattler et al. (2015, 2016)) use quantization into a fine vocabulary where each descriptor is represented by its word ID. Sattler et al. (2015) separate the diffcult problem of finding a unique 2D-3D matching into two simpler ones. They first establish locally unique 2D-3D matches using a fine visual vocabulary and a visibility graph which encodes the visibility relation between 3D points and cameras. Then, they disambiguate these matches by using a simple voting scheme to enforce the co-visibility of the selected 3D points. Their experiments show that matching based on a visual vocabulary can achieve state-of-the-art. Sattler et al. (2016) propose a prioritized matching scheme based on quantization, focusing on efficiency. They significantly accelerate 2D-to-3D matching by considering more likely features first and terminating the correspondence search as soon as enough matches are found.
- 在基于结构的方法中，姿态估计为验证位置估计提供了强大的几何约束。然而，直接的解决方案，例如通过使用SIFT特征的近似最近邻搜索的直接匹配将导致许多不正确的匹配。随着模型大小的增加，描述符的辨别力下降，匹配变得越来越模糊。因此，RANSAC技术难以找到正确的姿势。为了解决这个问题，Li et al。 （2012）在图像中发现3D模型点的统计同时性，然后将其用作RANSAC之前的抽样以利用共同可见性关系。此外，它们采用双向匹配方案，从图像中的特征向前转到数据库中的点，并从点到图像特征反向。他们表明双向方法比单向前向或反向匹配更好。除了模糊之外，存储模型中包含的大量描述符所需的内存量是与大规模相关的另一个问题。模型压缩通过减少点数产生较少的匹配，并增加不能本地化的图像数量。相反，更新的方法（Sattler等人（2015年，2016年））将量化用于精细词汇，其中每个描述符由其词ID表示。 Sattler等人（2015年）将找到独特的2D-3D匹配的困难问题分成两个简单的。他们首先使用精细的视觉词汇和可视化图来建立本地独特的2D-3D匹配，该图可以对3D点和相机之间的可见性关系进行编码。然后，他们通过使用简单的投票方案来强制所选3D点的共同可见性来消除这些比赛的歧义。他们的实验表明，基于视觉词汇的匹配可以实现最先进的。 Sattler等人（2016）提出了基于量化的优先匹配方案，重点关注效率。通过首先考虑更多可能的功能，并且一旦找到足够的匹配就终止对应搜索，从而显着加速了2D到3D的匹配。

- **Structure-based Localization using Deep Learning**: Kendall et al. (2015) and Walch et al. (2016) use a convolutional neural network to regress the camera pose from a single RGB image in an end-to-end manner. The motivation for using CNNs for this task is to eliminate the problems caused by large textureless areas, repetitive structures, motion blur, and illumination changes which can be challenging for feature based methods. In contrast to classical localization approaches whose runtime depends on several factors such as the number of features found in a query image or the number of 3D points in the model, the runtime of CNN-based approaches only depends on the size of the network. Kendall et al. (2015) modify GoogLeNet (Szegedy et al.(2015)) by replacing softmax classifiers with affine regressors and inserting another fully connected layer before the final regressor which can be used as a localization feature vector for further analysis. The final architecture, dubbed PoseNet is initialized by using the weights of classification networks trained on giant datasets such as ImageNet (Deng et al. (2009)) and Places (Zhou et al. (2014)). Further, it is fine-tuned on a new
pose dataset which was automatically created by using SfM to generate camera poses from a video of the scene. Walch et al. (2016) use a similar approach, but in addition they spatially correlate each element of the output of the CNN with Long Short-Term Memory (LTSM) units by exploiting their memorization capabilities. This way, the network is able to capture more contextual information and outperform PoseNet in different localization tasks including large-scale outdoor, small-scale indoor, and a newly proposed large-scale indoor localization benchmark. Although CNN-based approaches cannot match the precision of state-of the-art SIFT-based methods (Sattler et al. (2016)), their importance becomes more apparent in indoor environments with large textureless surfaces and repetitive scene elements where SIFT-based method cannot produce enough matches to obtain correct SfM reconstructions.
- 使用深度学习的基于结构的本地化：Kendall et al（2015）和Walch et al（2016）使用卷积神经网络以端对端的方式从单个RGB图像中回归相机姿态。使用CNN进行这项任务的动机是消除由无纹理区域，重复结构，运动模糊和照明变化引起的问题，这对基于特征的方法来说可能具有挑战性。与经典的本地化方法相比，其运行时间取决于几个因素，例如在查询图像中发现的特征数量或模型中的3D点的数量，基于CNN的方法的运行时间仅取决于网络的大小。 Kendall等人（2015）修改GoogLeNet（Szegedy等（2015）），通过用仿射回归器替换softmax分类器，并在最后的回归算子之前插入另一个完全连接的层，可以用作定位特征向量进行进一步分析。通过使用在诸如ImageNet（Deng等人（2009））和Places（Zhou等人（2014））等巨型数据集上训练的分类网络的权重，将最终的架构称为PoseNet。此外，它是一个新的微调
姿态数据集，通过使用SfM自动创建，以从场景的视频生成相机姿势。 Walch等人（2016）使用了类似的方法，但是通过利用其记忆功能，还可以将CNN的输出的每个元素与长时间内存（LTSM）单元空间相关。这样，网络能够捕获更多的语境信息，并且在不同的本地化任务（包括大型户外，小型室内）和新提出的大型室内本地化基准测试中胜过PoseNet。虽然基于CNN的方法不符合最先进的基于SIFT的方法的精度（Sattler等（2016）），但它们的重要性在具有大的无纹理表面和重复场景元素的室内环境中变得更加明显，其中SIFT-基于方法不能产生足够的匹配以获得正确的SfM重建。

- **Cross-view Localization**: It is diffcult to keep ground imagery around the world up to date, while it is much easier to establish live maps from aerial images and satellites. This gives rise to a new approach, geo-localization which tries to register ground-level images to aerial imagery. The underlying idea is to learn a mapping between ground-level and aerial image viewpoints to localize a ground-level query in an aerial image reference database. Lin et al. (2013) match ground-level queries to other ground-level reference photos as in traditional geolocalization, but then use the overhead appearance and land cover attributes of those ground-level matches to build slidingwindow classifiers in the aerial and land cover domain. In contrast to previous methods, they can often localize a query even if it has no corresponding ground-level images in the database by learning the co-occurrence of features in different views. Lin et al. (2015) collect a cross-view patch dataset using range data and camera parameters from Google street views to warp the dominant building surface plane to appear approximately like a 45% aerial view. Inspired by the success of face verification algorithms using deep learning, they train a Siamese network to match cross-view pairs of the same location. Workman et al. (2015) introduce another massive cross-view dataset. They first use CNNs for extracting ground-level image features and then, they learn to predict these features from aerial images of the same location. This way, the CNN is able to extract semantically meaningful features from aerial images without manually specifying semantic labels. They conclude that the crossview localization approach can obtain a precise estimate of the geographic locations which are distinctive from above. Otherwise, it can be used as a pre-processing step to a more expensive matching process.
- 横向本地化：将世界各地的地面图像保持原状，同时从航空图像和卫星建立实时地图更为容易。这产生了一种新的方法，地理定位尝试将地面图像注册到航空图像。其基本思想是学习地面和航空图像视点之间的映射，以便在空间图像参考数据库中本地化地面级查询。林等人（2013年）将传统地理定位中的地面级别查询与其他地面级参考照片进行匹配，然后利用这些地面级别匹配的架空外观和土地覆盖属性在空中和陆地覆盖域中构建滑动窗口分类器。与之前的方法相反，即使通过学习不同视图中的特征的同现，数据库中也没有相应的地面图像，所以它们通常可以定位查询。林等人（2015）将使用Google街景视图的范围数据和摄像机参数收集交叉视图修补程序数据集，以扭曲主要建筑物的平面，大致如45％的鸟瞰图。灵感来自使用深度学习的面部验证算法的成功，他们训练一个暹罗网络来匹配相同位置的交叉视图对。工人等（2015）引入了另一个巨大的跨视图数据集。他们首先使用CNN提取地面图像特征，然后学习从相同位置的航空图像预测这些特征。这样，CNN能够从空中图像中提取语义上有意义的特征，而无需手动指定语义标签。他们得出结论，跨视图本地化方法可以获得与上述不同的地理位置的精确估计。否则，它可以用作更加昂贵的匹配过程的预处理步骤。

- **Cross-view Localization: Buildings**: There are methods specialized to building facades in cross-view matching. The repeating patterns can yield a valuable matching indicator for regularity-driven approaches (Figure 37). By combining satellite and oblique bird’s eye-view, Bansal et al. (2011) first extract building outlines and facades and then match the ground image to oblique aerial images based on a statistical description of the facade pattern. Wolff et al. (2016) define a matching cost function to compare a street view motif to an aerial view motif based on similarity of color, texture and edge-based context features.
- 跨界本地化：建筑物：专门用于建立立体视角匹配的方法。 重复模式可以为规则性驱动的方法产生有价值的匹配指标（图37）。 通过结合卫星和倾斜的鸟瞰图，Bansal et al。 （2011）首先提取建筑物轮廓和立面，然后根据立面图案的统计描述将地面图像与倾斜航空图像进行匹配。 Wolff等人 （2016）定义了匹配成本函数，以基于颜色，纹理和基于边缘的上下文特征的相似性将街景图案与鸟瞰图图案进行比较。

- **Cross-view Localization**: Reconstructions: Another line of work addresses the problem of geo-referencing a reconstruction by automatic alignment with a satellite image, floor plan, map, or other overhead view. Kaminsky et al. (2009) compute the optimal alignment between SfM reconstructions and overhead images using an objective function that matches 3D points to image edges and imposes free space constraints based on the visibility of points in each camera. Matching ground and aerial images directly is a diffcult endeavor due to the large differences in their camera viewpoints, occlusions, and imaging conditions. Instead of seeking invariant feature detections, Shan et al. (2014) propose a viewpoint-dependent matching technique by exploiting approximate alignment information and underlying 3D geometry.
- 跨视图本地化：重建：另一行工作通过与卫星图像，平面图，地图或其他俯瞰视图进行自动对齐来解决地理参考重建的问题。 Kaminsky等人 （2009）使用将3D点与图像边缘匹配的目标函数计算SfM重建和开销图像之间的最佳对齐，并且基于每个相机中的点的可见性来施加自由空间约束。 由于摄像机视点，遮挡和成像条件的差异很大，直接匹配地面和航空图像是一个难题。 Shan等人，而不是寻求不变特征检测 （2014）通过利用近似对齐信息和底层3D几何提出了一种与视点相关的匹配技术。

- **Semantic Alignment from LiDAR**: Several companies acquire LiDAR data from scanners mounted on cars driving through cities to acquire 3D models of real-world urban environments. However, the accuracy of the 3D point positions acquired by the 3D scanners depends on the scanner poses predicted by GPS, inertial sensors, and SfM, which often fail in urban environments. These misalignments cause problems for point cloud registration methods. Yu et al. (2015) propose to align semantic features that can be matched robustly at different scales. By following a coarse-to-fine approach, they first successively align roads, facades, and poles, which can be matched robustly. In the following, they match cars and other small objects, which require better initial alignments to find correct correspondences. The use of semantic features provides a globally consistent alignment of LiDAR scans and their evaluation shows improvement over the initial alignments.
- 来自LiDAR的语义对齐：几家公司从驾驶城市车辆的扫描仪获取LiDAR数据，以获取现实城市环境的3D模型。 然而，由3D扫描仪获取的3D点位置的准确度取决于GPS，惯性传感器和SfM预测的扫描仪姿势，这些都在城市环境中经常失败。 这些不对齐会导致点云登记方法的问题。 Yu et al。 （2015）提出了可以在不同规模下强化匹配的语义特征。 通过遵循粗略的方法，他们首先连续对齐可以鲁棒匹配的道路，立面和极点。 在下文中，它们匹配汽车和其他小物体，这需要更好的初始对准以找到正确的对应关系。 使用语义特征提供了全局一致的LiDAR扫描对齐方式，并且它们的评估显示出比初始比对的改进。



  9. 追踪
  追踪的目标是给定传感器测量数据的情况下实时评估一个或多个目标的状态。典型来说，目标的状态由它在一定时间的位置、速度和加速度来表达。追踪其他车辆对自动驾驶任务而言非常重要。举个例子，汽车刹车距离随速度变化会有次方级的变化。为了防止相撞，系统需要足够提前做出反应。其他车辆的轨迹足以预测停车的位置和可能相撞的情况。
  在自行车和行人的案例中，比较难以预测未来的行为，因为他们可能会突然改变方向。然而，结合其他车辆的分类进行追踪，能够调整汽车在这种情况下的速度。此外，追踪其他汽车可被用来进行自动距离控制，提前预估其他车辆可能做的变动。
  9.1 立体追踪
  9.2 行人追踪
  9.3 顶级成果
  9.4 讨论
  10. 场景理解
  自动驾驶的基本需求之一是充分理解其周遭环境，比如复杂的交通场景。户外场景理解的复杂任务包括若干个子任务，比如深度估计、场景分类、目标探测与追踪、事件分类以及更多，其中每一个子任务描述场景的一个特定方面。联合建模这些特定方面以利用场景不同元素之间的关系并获得一个整体理解，这样做是有益的。大多数场景理解模型的目标是获得一个丰富但紧凑的场景表征，这个场景包含所有的元素，比如布局元素、交通参与者以及彼此之间的关系。相比于 2D 图像域中的推理，3D 推理在解决几何场景理解的问题上起着重要作用，并以 3D 目标模型、布局元素、闭塞关系等形式促使场景产生了更多的信息表征。场景理解的一个特殊挑战是城市市区与郊区交通情景的阐释。相较于高速公路和农村公路，市区场景包含了很多独立移动的交通参与者，道路与十字路口几何布局中的更多变化性，以及由于模糊的视觉特征和光照变化所带来的难度升级。
  从单一图像到视频
  结合目标探测与跟踪
  其他表征
  11. 传感器运动控制的端到端学习
  当前最先进的自动驾驶方法包含大量的模型，例如（交通信号、灯、汽车、行人的）探测、（车道、门面的）分割、运动估计、交通参与者的跟踪，重建。然后，这些组件的结果按照控制系统的规则组合起来。但是，为了解决操控汽车方向和速度的问题，这需要稳健地解决场景理解中的诸多开放性难题。最近的文献提出了作为替代性方案的若干个端到端自动驾驶方法。端到端驾驶使用的是从一个感觉输入（比如，正面摄像头图像）直接映射到驾驶操作（比如，转向角）的独立系统。
  结论
    本文中，我们就自动驾驶计算机视觉的难题、数据集和方法提供了一个综合性调查。为了完成这一目标，我们的调查同时涵盖了最为相关的历史资料，以及识别、重建、运动估测、追踪、场景理解、端到端学习等当前最先进的专门主题。通过使用 KITTI 基准的全新深入质量分析并考虑其他数据集，我们还讨论了开放问题和当前这些主题下的研究挑战。我们的交互式在线工具平台运用图形可视化了分类方法，从而可使你轻松浏览被调查的文献。将来，我们计划在这一交互式平台上不断更新相关文献，为这一领域提供一个实时的概观。我们希望该项调查和该工具平台可进一步激发新研究，并且通过这一详尽的概述，使得初学者更容易进入该领域





    
